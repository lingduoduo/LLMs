{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02068558",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai rdflib spacy pyvis datasets scikit-learn matplotlib tqdm pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eb326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# NLP and KG libraries\n",
    "import spacy\n",
    "from rdflib import Graph, Literal, Namespace, URIRef\n",
    "from rdflib.namespace import RDF, RDFS, XSD, SKOS # 添加SKOS用于altLabel\n",
    "\n",
    "# OpenAI client for LLM\n",
    "from openai import OpenAI\n",
    "\n",
    "# Visualization\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Hugging Face datasets library\n",
    "from datasets import load_dataset\n",
    "\n",
    "# For embedding similarity\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1141f1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用特定版本可以帮助保持一致性\n",
    "cnn_dm_dataset=load_dataset(\"cnn_dailymail\", \"3.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a432c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算记录总数  \n",
    "total_records=len(cnn_dm_dataset[\"train\"]) +len(cnn_dm_dataset[\"validation\"]) +len(cnn_dm_dataset[\"test\"])  # 打印总数和样本记录  \n",
    "print(f\"Total number of records in the dataset: {total_records}\\n\")\n",
    "print(\"Sample record from the training dataset:\")\n",
    "print(cnn_dm_dataset[\"train\"][0])\n",
    "\n",
    "\n",
    "# #### OUTPUT ####\n",
    "# Total number of records in the dataset: 311971\n",
    "\n",
    "# Sample record from the training dataset:\n",
    "# {'article': 'LONDON, England (Reuters) -- Harry Potter star Daniel ...'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71589658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义与技术公司收购相关的关键词   \n",
    "ACQUISITION_KEYWORDS= [\"acquire\", \"acquisition\", \"merger\", \"buyout\", \"purchased by\", \"acquired by\", \"takeover\"]   \n",
    "TECH_KEYWORDS= [\"technology\", \"software\", \"startup\", \"app\", \"platform\", \"digital\", \"AI\", \"cloud\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e291e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 仅取训练集  \n",
    "cnn_dm_dataset_train=cnn_dm_dataset['train']  \n",
    "# 初始化一个空列表来存储过滤后的文章  \n",
    "filtered_articles= []  \n",
    "# 遍历数据集并基于关键词过滤文章  \n",
    "forrecordincnn_dm_dataset_train:      \n",
    "# 检查任何关键词是否出现在文章文本中      \n",
    " found_keyword = False\n",
    "    for keyword in ACQUISITION_KEYWORDS:\n",
    "        if keyword.lower() in record['article'].lower():\n",
    "            found_keyword = True\n",
    "            break# 一旦找到关键词就停止            \n",
    "    # 如果找到关键词，将文章添加到过滤列表中      \n",
    "    if found_keyword:\n",
    "      filtered_articles.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994c45e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印过滤后文章的总数  \n",
    "print(f\"Total number of filtered articles: {len(filtered_articles)}\")  \n",
    "# 打印一个过滤后文章的样本  \n",
    "print(\"\\nSample of a filtered article:\")  \n",
    "print(filtered_articles[0]['article'])  \n",
    "### OUTPUT #### \n",
    "\n",
    "# Sample of a filtered article:\n",
    "# SAN DIEGO, California (CNN) -- You must know whats really driving the \n",
    "# immigration debate ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dfbdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_articles = []\n",
    "\n",
    "for record in filtered_articles：\n",
    "    text = record['article']\n",
    "\n",
    "    # 使用正则表达式进行基本清理\n",
    "    text = re.sub(r'^\\(CNN\\)\\s*(--)?\\s*', '', text)  # 删除（CNN）前缀\n",
    "    text = re.sub(r'By .*? for Dailymail\\.com.*?Updated:.*', '', text, flags=re.I | re.S) # 删除副标题\n",
    "    text = re.sub(r'PUBLISHED:.*?UPDATED:.*', '', text, flags=re.I | re.S) # 删除已发表/已更新的内容\n",
    "    text = re.sub(r'Last updated at.*on.*', '', text, flags=re.I) # 移除最后更新的内容\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '[URL]', text) # 替换网址\n",
    "    text = re.sub(r'<.*?>', '', text) # 删除 HTML 标记\n",
    "    text = re.sub(r'b[\\w.-]+@[\\w.-]+\\.\\w+\\b', '[EMAIL]', text) # 替换邮件地址\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # 对空白进行规范化处理\n",
    "\n",
    "    # 存储清理后的结果\n",
    "    cleaned_articles.append({\n",
    "        “id\": record[‘id’]、\n",
    "        “cleaned_text\": text、\n",
    "        “summary\": record.get(‘highlights’, ‘’)\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7369562",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c6e5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载并加载 spaCy 的英语模型\n",
    "# 只需运行一次）\n",
    "spacy.cli.download(“en_core_web_sm”)\n",
    "nlp = spacy.load(“en_core_web_sm”)\n",
    "\n",
    "# 初始化一个计数器，用于保存实体标签计数（例如，PERSON、ORG、DATE）\n",
    "entity_counts = Counter()\n",
    "\n",
    "# 循环处理每篇文章，并应用 spaCy 的命名实体识别技术\n",
    "for article in cleaned_articles：\n",
    "    text = article['cleaned_text'] # 获取清理后的文本\n",
    "    doc = nlp(text) # 使用 spaCy 处理文本\n",
    "\n",
    "    # 计算在文本中找到的每个实体标签\n",
    "    for ent in doc.ents：\n",
    "        entity_counts[ent.label_] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084a3d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取标签和计数\n",
    "labels, counts = zip(*entity_counts)\n",
    "\n",
    "# 绘制条形图\n",
    "plt.figure(figsize=(12, 7))  # 设置图表尺寸\n",
    "plt.bar(labels, counts, color='skyblue') # 创建条形图\n",
    "plt.title(“Top Entity Type Distribution (via spaCy)”)  # 图表标题\n",
    "plt.ylabel(“Frequency”) # Y 轴标签\n",
    "plt.xlabel(“Entity Label”) # X 轴标签\n",
    "plt.xticks(rotation=45, ha=“right”) # 旋转 X 轴标签以提高可视性\n",
    "plt.tight_layout() # 调整布局以确保所有内容都能匹配\n",
    "plt.show() # 显示曲线图"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93daee9",
   "metadata": {},
   "source": [
    "### Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645975f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用提供的配置初始化OpenAI客户端  \n",
    "client = OpenAI(  \n",
    "    base_url=\"YOUR LLM API Provider link\",  \n",
    "    api_key=\"LLM API KEY\"  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9c289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(system_prompt, user_prompt, model_name):  \n",
    "    \"\"\"  \n",
    "    向语言模型（LLM）发送请求，根据提供的提示提取实体。\n",
    "\n",
    "    Args:  \n",
    "        system_prompt (str): 给LLM的指示或上下文（例如，如何行为）。 \n",
    "        user_prompt (str): 包含要提取实体的文本的用户输入。\n",
    "        model_name (str): 要使用的LLM模型的标识符（例如，\"gpt-4\"）。 \n",
    "\n",
    "    Returns:  \n",
    "        str: 来自LLM的JSON格式字符串响应，如果客户端不可用则为None。\n",
    "    \"\"\"\n",
    "\n",
    "    # 构建并发送聊天完成请求到LLM  \n",
    "    response = client.chat.completions.create(  \n",
    "        model=model_name,  \n",
    "        messages=[  \n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # 系统级指令  \n",
    "            {\"role\": \"user\", \"content\": user_prompt}       # 用户提供的输入  \n",
    "        ],  \n",
    "    )  \n",
    "\n",
    "    # 提取并返回响应内容（JSON字符串）  \n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0225e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按频率获取前N个实体类型  \n",
    "relevant_entity_labels_for_llm = [label for label, count in entity_counts.most_common(TOP_N_ENTITY_TYPES)]  \n",
    "entity_types_string_for_prompt = \", \".join(relevant_entity_labels_for_llm)  \n",
    "\n",
    "\n",
    "\n",
    "# LLM的系统提示  \n",
    "# 我们指示它返回一个带有\"entities\"键的JSON对象  \n",
    "# 其值是实体对象的列表。  \n",
    "llm_ner_system_prompt = (  \n",
    "    f\"You are an expert Named Entity Recognition system. \"\n",
    "    f\"From the provided news article text, identify and extract entities. \"\n",
    "    f\"The entity types to focus on are: {entity_types_string_for_prompt}. \"\n",
    "    f\"For each identified entity, provide its exact text span from the article and its type (use one of the provided types). \"\n",
    "    f\"Output ONLY a valid JSON object with a single key 'entities'. The value of 'entities' MUST be a list of JSON objects, \"\n",
    "    f\"where each object has 'text' and 'type' keys. \"\n",
    "    f\"Example: {{\\\"entities\\\": [{{\\\"text\\\": \\\"United Nations\\\", \\\"type\\\": \\\"ORG\\\"}}, {{\\\"text\\\": \\\"Barack Obama\\\", \\\"type\\\": \\\"PERSON\\\"}}]}} \"\n",
    "    f\"If no entities of the specified types are found, the 'entities' list should be empty: {{\\\"entities\\\": []}}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e6160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_llm_entity_json_output(llm_output_str):  \n",
    "    \"\"\"  \n",
    "    解析LLM的JSON字符串并返回实体列表。\n",
    "    假设格式为：{\"entities\": [{\"text\": \"...\", \"type\": \"...\"}]}  \n",
    "\n",
    "    Args:  \n",
    "        llm_output_str (str): 来自LLM的JSON字符串。\n",
    "\n",
    "    Returns:  \n",
    "        list: 提取的实体或如果解析失败则返回空列表。\n",
    "    \"\"\"\n",
    "    ifnot llm_output_str:  \n",
    "        return []  # 如果没有输出则返回空列表  \n",
    "\n",
    "    # 如果存在markdown代码块则移除  \n",
    "    if llm_output_str.startswith(\"```json\"):  \n",
    "        llm_output_str = llm_output_str[7:].rstrip(\"```\").strip()  \n",
    "\n",
    "    try:  \n",
    "        data = json.loads(llm_output_str)  \n",
    "        return data.get(\"entities\", [])  # 返回实体列表，如果未找到则为空  \n",
    "    except json.JSONDecodeError:  \n",
    "        return []  # JSON错误时返回空列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d216ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义我们的实体提取LLM  \n",
    "TEXT_GEN_MODEL_NAME = \"microsoft/phi-4\"\n",
    "\n",
    "# 遍历有限数量的清理后文章以   \n",
    "# 使用LLM提取实体  \n",
    "for i, article_data in enumerate(cleaned_articles):  \n",
    "    article_id = article_data['id']  \n",
    "    article_text = article_data['cleaned_text']  \n",
    "\n",
    "    # 调用LLM提取实体  \n",
    "    llm_response_content = call_llm(  \n",
    "        llm_ner_system_prompt,  \n",
    "        article_text,  \n",
    "        TEXT_GEN_MODEL_NAME  \n",
    "    )  \n",
    "\n",
    "    # 将LLM的响应解析为实体列表  \n",
    "    extracted_llm_entities = []  \n",
    "    if llm_response_content:  \n",
    "        extracted_llm_entities = parse_llm_entity_json_output(llm_response_content)  \n",
    "\n",
    "    # 将结果与文章一起存储  \n",
    "    articles_with_llm_entities.append({  \n",
    "        \"id\": article_id,  \n",
    "        \"cleaned_text\": article_text,  \n",
    "        \"summary\": article_data['summary'],  \n",
    "        \"llm_extracted_entities\": extracted_llm_entities  \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bd218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(articles_with_llm_entities[4212]['llm_extracted_entities'])  \n",
    "\n",
    "\n",
    "\n",
    "# ### OUTPUT ###  \n",
    "# Extracted 20 entities for article ID 4cf51ce937a.  \n",
    "#   Sample entities: [  \n",
    "#   {  \n",
    "#     \"text\": \"United Nations\",  \n",
    "#     \"type\": \"ORG\"\n",
    "#   },  \n",
    "#   {  \n",
    "#     \"text\": \"Algiers\",  \n",
    "#     \"type\": \"GPE\"\n",
    "#   },  \n",
    "#   {  \n",
    "#     \"text\": \"CNN\",  \n",
    "#     \"type\": \"ORG\"\n",
    "#   }  \n",
    "\n",
    "#    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57cfe74",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73d5b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 关系提取的系统提示  # 我们要求一个带有\"relationships\"键的JSON对象。  \n",
    "llm_re_system_prompt = (\n",
    "    \"You are an expert system for extracting relationships between entities from text, \"\n",
    "    \"specifically focusing on **technology company acquisitions**. \"\n",
    "    \"Given an article text and a list of pre-extracted named entities (each with 'text' and 'type'), \"\n",
    "    \"your task is to identify and extract relationships. \"\n",
    "    \"The 'subject_text' and 'object_text' in your output MUST be exact text spans of entities found in the provided 'Extracted Entities' list. \"\n",
    "    \"The 'subject_type' and 'object_type' MUST correspond to the types of those entities from the provided list. \"\n",
    "    \"Output ONLY a valid JSON object with a single key 'relationships'. The value of 'relationships' MUST be a list of JSON objects. \"\n",
    "    \"Each relationship object must have these keys: 'subject_text', 'subject_type', 'predicate' (one of the types listed above), 'object_text', 'object_type'. \"\n",
    "    \"Example: {\\\"relationships\\\": [{\\\"subject_text\\\": \\\"Innovatech Ltd.\\\", \\\"subject_type\\\": \\\"ORG\\\", \\\"predicate\\\": \\\"ACQUIRED\\\", \\\"object_text\\\": \\\"Global Solutions Inc.\\\", \\\"object_type\\\": \\\"ORG\\\"}]} \"\n",
    "    \"If no relevant relationships of the specified types are found between the provided entities, the 'relationships' list should be empty: {\\\"relationships\\\": []}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e602ce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# {\n",
    "#   \"relationships\": [\n",
    "#     {\n",
    "#       \"subject_text\": \"Innovatech Ltd.\",\n",
    "#       \"subject_type\": \"ORG\",\n",
    "#       \"predicate\": \"ACQUIRED\",\n",
    "#       \"object_text\": \"Global Solutions Inc.\",\n",
    "#       \"object_type\": \"ORG\"\n",
    "#     }\n",
    "#   ]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881140d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_llm_relationship_json_output(llm_output_str_rels):\n",
    "  \"\"\"\n",
    "  解析LLM的JSON字符串以提取关系。\n",
    "  预期格式：\n",
    "  {\"relationships\": [{\"subject_text\": ..., \"predicate\": ..., \"object_text\": ...}]}\n",
    "  Args:          \n",
    "  llm_output_str_rels (str): 来自LLM的JSON字符串。\n",
    "  Returns:          \n",
    "  list: 提取的关系或如果解析失败则返回空列表。\n",
    "  \"\"\"\n",
    "ifnot llm_output_str_rels:\n",
    "    return []  # 如果没有输出则返回空列表      \n",
    "# 如果存在markdown代码块则移除\n",
    "if llm_output_str_rels.startswith(\"```json\"):\n",
    "        llm_output_str_rels = llm_output_str_rels[7:].rstrip(\"```\").strip()\n",
    "\n",
    "try:\n",
    "    data = json.loads(llm_output_str_rels)\n",
    "    return data.get(\"relationships\", [])  # 返回关系列表，如果未找到则为空\n",
    "except json.JSONDecodeError:\n",
    "    return []  # JSON错误时返回空列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a058cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历每篇文章的实体数据\n",
    "for i, article_entity_data in enumerate(articles_with_llm_entities):\n",
    "    # 从文章数据中提取文章 ID、已清理文本和已提取实体\n",
    "    article_id_rels = article_entity_data['id']\n",
    "    article_text_rels = article_entity_data['cleaned_text']\n",
    "    current_entities = article_entity_data['llm_extracted_entities']\n",
    "    \n",
    "    # 将实体列表序列化为 JSON 字符串，以便包含在提示中\n",
    "    entities_json_for_prompt = json.dumps(current_entities)\n",
    "\n",
    "    # 构建用户提示，请求从 LLM 中提取关系\n",
    "    user_prompt_for_re = (\n",
    "        f\"Article Text:\\n```\\n{article_text_for_llm_re}\\n```\\n\\n\"\n",
    "        f\"Extracted Entities (use these exact texts for subjects/objects of relationships):\\n```json\\n{entities_json_for_prompt}\\n```\\n\\n\"\n",
    "        \"Identify and extract relationships between these entities based on the system instructions.\"\n",
    "    )\n",
    "    \n",
    "    # 调用 LLM 以根据提示提取关系\n",
    "    llm_response_rels_content = call_llm_for_relationships(llm_re_system_prompt, user_prompt_for_re, TEXT_GEN_MODEL_NAME)\n",
    "    \n",
    "    # 初始化一个空列表来存储提取的关系\n",
    "    extracted_llm_rels = []\n",
    "    \n",
    "    # 如果 LLM 响应不是空的，则解析 JSON 响应中提取的关系\n",
    "    if llm_response_rels_content:\n",
    "        extracted_llm_rels = parse_llm_relationship_json_output(llm_response_rels_content)\n",
    "\n",
    "    \n",
    "    # 将原始文章数据和提取的关系附加到结果列表中\n",
    "    articles_with_llm_relations.append({ \n",
    "      **article_entity_data, # 保留原始文章数据(id, text, entities, etc.)\n",
    "      \"llm_extracted_relationships\": extracted_llm_rels  # 添加提取的关系\n",
    "    })\n",
    "处理完成后，我们可以查看一篇文章中提取的关系样本：\n",
    "\n",
    "# 打印一篇样本文章的关系\n",
    "print(articles_with_llm_entities[1234]['llm_extracted_relationships'])\n",
    "\n",
    "# ### OUTPUT ###\n",
    "# Extracted 3 relationships using LLM.\n",
    "#   Sample LLM relationships: [\n",
    "#   {\n",
    "#     \"subject_text\": \"Microsoft Corp.\",\n",
    "#     \"subject_type\": \"ORG\",\n",
    "#     \"predicate\": \"ACQUIRED\",\n",
    "#     \"object_text\": \"Nuance Communications Inc.\",\n",
    "#     \"object_type\": \"ORG\"\n",
    "#   },\n",
    "#   {\n",
    "#     \"subject_text\": \"Nuance Communications Inc.\",\n",
    "#     \"subject_type\": \"ORG\",\n",
    "#     \"predicate\": \"HAS_PRICE\",\n",
    "#     \"object_text\": \"$19.7 billion\",\n",
    "#     \"object_type\": \"MONEY\"\n",
    "#   }\n",
    "# ]\n",
    "# ... (similar output for other articles) ...\n",
    "# 至此，我们已成功从文章数据集中提取了实体（节点）和关系（边），完成了构建知识图谱所需的基本元素。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab76570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Bulk-import keywords into RediSearch autocomplete, then query them.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from redis import Redis\n",
    "from redis.commands.search.suggestion import Suggestion\n",
    "\n",
    "##############################################################################\n",
    "# 1 ▸ connection\n",
    "##############################################################################\n",
    "\n",
    "r = Redis(host=\"localhost\", port=6379, decode_responses=True)\n",
    "ac  = r.ft()                     # we only need the helper object, no schema\n",
    "\n",
    "##############################################################################\n",
    "# 2 ▸ importer\n",
    "##############################################################################\n",
    "\n",
    "def add_keywords_from_file(\n",
    "    filepath: str | Path,\n",
    "    key: str,\n",
    "    batch: int = 10_000,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Read KEYWORD <TAB|SPACE|COMMA> COUNT from *filepath* and insert into\n",
    "    the RediSearch autocomplete dictionary stored under *key*.\n",
    "    \"\"\"\n",
    "    filepath = Path(filepath)\n",
    "\n",
    "    with filepath.open(encoding=\"utf-8\") as fh:\n",
    "        pipe   = r.pipeline(transaction=False)\n",
    "        total  = 0\n",
    "\n",
    "        for i, line in enumerate(fh, 1):\n",
    "            # -------------------- parse \"keyword  count\"\n",
    "            parts = [p.strip() for p in line.strip().replace(\",\", \" \").split()]\n",
    "            if len(parts) < 2 or not parts[-1].isdigit():\n",
    "                continue                          # skip malformed lines\n",
    "            *token_parts, count_str = parts\n",
    "            token  = \" \".join(token_parts)\n",
    "            score  = float(count_str)             # RediSearch wants a float\n",
    "\n",
    "            # -------------------- stage FT.SUGADD\n",
    "            pipe.ft().sugadd(\n",
    "                key,\n",
    "                Suggestion(token, score=score),\n",
    "                # INCR=True makes repeated terms accumulate counts\n",
    "                increment=True,\n",
    "            )\n",
    "            total += 1\n",
    "\n",
    "            if i % batch == 0:\n",
    "                pipe.execute()\n",
    "                pipe = r.pipeline(transaction=False)\n",
    "\n",
    "        pipe.execute()\n",
    "\n",
    "    print(f\"Inserted/updated {total:,} keywords into '{key}'\")\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# 3 ▸ query helper\n",
    "##############################################################################\n",
    "\n",
    "def suggest(\n",
    "    key: str,\n",
    "    prefix: str,\n",
    "    *,\n",
    "    max_results: int = 10,\n",
    "    fuzzy: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Return up to *max_results* suggestions for *prefix*.\n",
    "    \"\"\"\n",
    "    results = ac.sugget(\n",
    "        key,\n",
    "        prefix,\n",
    "        max=max_results,\n",
    "        fuzzy=fuzzy,\n",
    "        with_scores=True,\n",
    "        with_payloads=False,\n",
    "    )\n",
    "    # results is a list of Suggestion objects\n",
    "    return [(s.string, s.score) for s in results]\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# 4 ▸ demo\n",
    "##############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DICT_KEY = \"keywords_ac\"\n",
    "\n",
    "    add_keywords_from_file(\"keywords_counts.txt\", DICT_KEY)\n",
    "\n",
    "    for p in (\"stra\", \"strawb\", \"jam\"):\n",
    "        print(f\"\\nSuggestions for '{p}':\")\n",
    "        for term, score in suggest(DICT_KEY, p):\n",
    "            print(f\"  {term:<30s} {score:,.0f}\")\n",
    "\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.data import find\n",
    "\n",
    "# List all corpora you may need across your projects\n",
    "NLTK_RESOURCES = [\n",
    "    \"corpora/stopwords\",\n",
    "    \"corpora/wordnet\",\n",
    "    \"tokenizers/punkt\",\n",
    "    \"taggers/averaged_perceptron_tagger\",\n",
    "    # Add more if needed\n",
    "]\n",
    "\n",
    "def ensure_nltk_resources():\n",
    "    \"\"\"Download required NLTK resources safely (only if missing).\"\"\"\n",
    "    for resource in NLTK_RESOURCES:\n",
    "        try:\n",
    "            find(resource)\n",
    "        except LookupError:\n",
    "            nltk.download(resource.split(\"/\", 1)[1], quiet=True)\n",
    "\n",
    "# Call once during module load or app startup\n",
    "ensure_nltk_resources()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
