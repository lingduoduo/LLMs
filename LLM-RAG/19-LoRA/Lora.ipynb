{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OH3hOy6ay4Cw",
        "outputId": "59c1a26d-1ea8-4bf9-d111-28b6f3206676"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Jan 26 00:58:21 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   28C    P0             44W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZ7tHPWqzwQR",
        "outputId": "c21e32ea-562c-4a13-901f-ab5c3bbb54c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.6 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -q install -U \"transformers>=4.41\" \"datasets>=2.20\" \"accelerate>=0.33\" peft deepspeed sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZyMZo96Yz5Gr"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > ds_zero2_bf16.json <<'JSON'\n",
        "{\n",
        "  \"bf16\": { \"enabled\": true },\n",
        "  \"zero_optimization\": {\n",
        "    \"stage\": 2,\n",
        "    \"allgather_partitions\": true,\n",
        "    \"allgather_bucket_size\": 200000000,\n",
        "    \"reduce_scatter\": true,\n",
        "    \"reduce_bucket_size\": 200000000,\n",
        "    \"overlap_comm\": true,\n",
        "    \"contiguous_gradients\": true\n",
        "  },\n",
        "  \"gradient_accumulation_steps\": 16,\n",
        "  \"train_micro_batch_size_per_gpu\": 1,\n",
        "  \"wall_clock_breakdown\": false\n",
        "}\n",
        "JSON\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Rg4S6tJI0FHr"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cat > sft_lora_ds_hf.py <<'PY'\n",
        "import argparse\n",
        "from typing import Dict, Any, List, Optional\n",
        "from inspect import signature\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset, concatenate_datasets, Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    p = argparse.ArgumentParser()\n",
        "    # DeepSpeed injects this\n",
        "    p.add_argument(\"--local_rank\", type=int, default=-1)\n",
        "\n",
        "    p.add_argument(\"--model_name_or_path\", type=str, required=True)\n",
        "    p.add_argument(\"--output_dir\", type=str, required=True)\n",
        "\n",
        "    p.add_argument(\"--dataset_1\", type=str, required=True)\n",
        "    p.add_argument(\"--dataset_1_split\", type=str, default=\"train\")\n",
        "    p.add_argument(\"--dataset_1_n\", type=int, default=2000)\n",
        "\n",
        "    p.add_argument(\"--dataset_2\", type=str, default=None)\n",
        "    p.add_argument(\"--dataset_2_split\", type=str, default=\"train\")\n",
        "    p.add_argument(\"--dataset_2_n\", type=int, default=600)\n",
        "\n",
        "    p.add_argument(\"--text_field\", type=str, default=None)\n",
        "\n",
        "    p.add_argument(\"--num_train_epochs\", type=float, default=1)\n",
        "    p.add_argument(\"--per_device_train_batch_size\", type=int, default=1)\n",
        "    p.add_argument(\"--per_device_eval_batch_size\", type=int, default=1)\n",
        "    p.add_argument(\"--gradient_accumulation_steps\", type=int, default=16)\n",
        "    p.add_argument(\"--learning_rate\", type=float, default=1e-4)\n",
        "    p.add_argument(\"--warmup_ratio\", type=float, default=0.05)\n",
        "    p.add_argument(\"--logging_steps\", type=int, default=5)\n",
        "    p.add_argument(\"--eval_steps\", type=int, default=50)\n",
        "    p.add_argument(\"--save_steps\", type=int, default=50)\n",
        "    p.add_argument(\"--save_total_limit\", type=int, default=2)\n",
        "    p.add_argument(\"--max_seq_length\", type=int, default=2048)\n",
        "    p.add_argument(\"--dataloader_num_workers\", type=int, default=4)\n",
        "\n",
        "    p.add_argument(\"--deepspeed\", type=str, default=None)\n",
        "    p.add_argument(\"--bf16\", type=str, default=\"True\")\n",
        "\n",
        "    p.add_argument(\"--lora_r\", type=int, default=8)\n",
        "    p.add_argument(\"--lora_alpha\", type=int, default=32)\n",
        "    p.add_argument(\"--lora_target_modules\", type=str, default=\"all-linear\")\n",
        "    return p.parse_args()\n",
        "\n",
        "\n",
        "def pick_all_linear_target_modules(model) -> List[str]:\n",
        "    names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if module.__class__.__name__ in (\"Linear\", \"Linear4bit\", \"Linear8bitLt\"):\n",
        "            names.add(name.split(\".\")[-1])\n",
        "    return sorted(list(names))\n",
        "\n",
        "\n",
        "def format_example(ex: Dict[str, Any], tokenizer, text_field: Optional[str]) -> str:\n",
        "    if text_field and text_field in ex and ex[text_field]:\n",
        "        return ex[text_field]\n",
        "\n",
        "    if \"instruction\" in ex and \"output\" in ex:\n",
        "        inp = ex.get(\"input\", \"\")\n",
        "        if inp:\n",
        "            return f\"### Instruction:\\n{ex['instruction']}\\n\\n### Input:\\n{inp}\\n\\n### Response:\\n{ex['output']}\"\n",
        "        return f\"### Instruction:\\n{ex['instruction']}\\n\\n### Response:\\n{ex['output']}\"\n",
        "\n",
        "    if \"prompt\" in ex and (\"completion\" in ex or \"response\" in ex):\n",
        "        comp = ex.get(\"completion\", ex.get(\"response\", \"\"))\n",
        "        return f\"{ex['prompt']}{comp}\"\n",
        "\n",
        "    if \"question\" in ex and \"answer\" in ex:\n",
        "        return f\"### Question:\\n{ex['question']}\\n\\n### Answer:\\n{ex['answer']}\"\n",
        "\n",
        "    if \"messages\" in ex and ex[\"messages\"]:\n",
        "        try:\n",
        "            return tokenizer.apply_chat_template(ex[\"messages\"], tokenize=False, add_generation_prompt=False)\n",
        "        except Exception:\n",
        "            return str(ex[\"messages\"])\n",
        "\n",
        "    if \"conversations\" in ex and ex[\"conversations\"]:\n",
        "        try:\n",
        "            msgs = []\n",
        "            for m in ex[\"conversations\"]:\n",
        "                role = m.get(\"from\", \"\")\n",
        "                content = m.get(\"value\", \"\")\n",
        "                if role in (\"human\", \"user\"):\n",
        "                    msgs.append({\"role\": \"user\", \"content\": content})\n",
        "                else:\n",
        "                    msgs.append({\"role\": \"assistant\", \"content\": content})\n",
        "            return tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=False)\n",
        "        except Exception:\n",
        "            return str(ex[\"conversations\"])\n",
        "\n",
        "    return str(ex)\n",
        "\n",
        "\n",
        "def load_n(name: str, split: str, n: int) -> Dataset:\n",
        "    ds = load_dataset(name, split=split)\n",
        "    if n and n < len(ds):\n",
        "        ds = ds.select(range(n))\n",
        "    return ds\n",
        "\n",
        "\n",
        "def build_training_args(args, bf16: bool) -> TrainingArguments:\n",
        "    # Build kwargs compatible with the transformers version installed\n",
        "    base = dict(\n",
        "        output_dir=args.output_dir,\n",
        "        num_train_epochs=args.num_train_epochs,\n",
        "        per_device_train_batch_size=args.per_device_train_batch_size,\n",
        "        per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
        "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
        "        learning_rate=args.learning_rate,\n",
        "        warmup_ratio=args.warmup_ratio,\n",
        "        logging_steps=args.logging_steps,\n",
        "        eval_steps=args.eval_steps,\n",
        "        save_steps=args.save_steps,\n",
        "        save_total_limit=args.save_total_limit,\n",
        "        bf16=bf16,\n",
        "        dataloader_num_workers=args.dataloader_num_workers,\n",
        "        deepspeed=args.deepspeed,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    params = signature(TrainingArguments.__init__).parameters\n",
        "    if \"evaluation_strategy\" in params:\n",
        "        base[\"evaluation_strategy\"] = \"steps\"\n",
        "    elif \"eval_strategy\" in params:\n",
        "        base[\"eval_strategy\"] = \"steps\"\n",
        "\n",
        "    if \"save_strategy\" in params:\n",
        "        base[\"save_strategy\"] = \"steps\"\n",
        "\n",
        "    return TrainingArguments(**base)\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    bf16 = args.bf16.lower() == \"true\"\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, trust_remote_code=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        torch_dtype=torch.bfloat16 if bf16 else torch.float16,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "\n",
        "    if args.lora_target_modules == \"all-linear\":\n",
        "        target_modules = pick_all_linear_target_modules(model)\n",
        "    else:\n",
        "        target_modules = [x.strip() for x in args.lora_target_modules.split(\",\") if x.strip()]\n",
        "\n",
        "    lora_cfg = LoraConfig(\n",
        "        r=args.lora_r,\n",
        "        lora_alpha=args.lora_alpha,\n",
        "        target_modules=target_modules,\n",
        "        lora_dropout=0.0,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "    model = get_peft_model(model, lora_cfg)\n",
        "\n",
        "    ds1 = load_n(args.dataset_1, args.dataset_1_split, args.dataset_1_n)\n",
        "    if args.dataset_2:\n",
        "        ds2 = load_n(args.dataset_2, args.dataset_2_split, args.dataset_2_n)\n",
        "        train_ds = concatenate_datasets([ds1, ds2])\n",
        "    else:\n",
        "        train_ds = ds1\n",
        "\n",
        "    def tok_fn(ex):\n",
        "        text = format_example(ex, tokenizer, args.text_field)\n",
        "        return tokenizer(text, truncation=True, max_length=args.max_seq_length, padding=False)\n",
        "\n",
        "    train_ds = train_ds.map(tok_fn, remove_columns=train_ds.column_names)\n",
        "    eval_ds = train_ds.select(range(min(64, len(train_ds))))\n",
        "\n",
        "    collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "    targs = build_training_args(args, bf16)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=targs,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=eval_ds,\n",
        "        data_collator=collator,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model(args.output_dir)\n",
        "    tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "PY\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run training with DeepSpeed (single GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee8RziiD0gwJ",
        "outputId": "bf85dcb2-73f4-416d-bb0b-aeb5dc96522d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2026-01-26 01:13:36.566552: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2026-01-26 01:13:36.583988: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769390016.605237    6559 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769390016.611837    6559 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769390016.628101    6559 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769390016.628127    6559 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769390016.628131    6559 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769390016.628135    6559 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-26 01:13:36.633029: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[2026-01-26 01:13:41,479] [WARNING] [runner.py:232:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
            "[2026-01-26 01:13:41,479] [INFO] [runner.py:630:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None --log_level=info sft_lora_ds_hf.py --model_name_or_path Qwen/Qwen3-0.6B --output_dir output --num_train_epochs 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 16 --learning_rate 1e-4 --warmup_ratio 0.05 --logging_steps 5 --eval_steps 50 --save_steps 50 --save_total_limit 2 --max_seq_length 2048 --dataloader_num_workers 4 --bf16 True --deepspeed ds_zero2_bf16.json --lora_r 8 --lora_alpha 32 --lora_target_modules all-linear --dataset_1 tatsu-lab/alpaca --dataset_1_split train --dataset_1_n 2000\n",
            "2026-01-26 01:13:49.363338: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769390029.383909    6682 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769390029.390203    6682 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769390029.405657    6682 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769390029.405681    6682 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769390029.405684    6682 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769390029.405687    6682 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[2026-01-26 01:13:54,264] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.22.3-1+cuda12.5\n",
            "[2026-01-26 01:13:54,264] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.22.3-1\n",
            "[2026-01-26 01:13:54,264] [INFO] [launch.py:155:main] 0 NCCL_VERSION=2.22.3-1\n",
            "[2026-01-26 01:13:54,264] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
            "[2026-01-26 01:13:54,264] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.22.3-1+cuda12.5\n",
            "[2026-01-26 01:13:54,264] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
            "[2026-01-26 01:13:54,264] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.22.3-1\n",
            "[2026-01-26 01:13:54,264] [INFO] [launch.py:162:main] WORLD INFO DICT: {'localhost': [0]}\n",
            "[2026-01-26 01:13:54,264] [INFO] [launch.py:168:main] nnodes=1, num_local_procs=1, node_rank=0\n",
            "[2026-01-26 01:13:54,264] [INFO] [launch.py:179:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
            "[2026-01-26 01:13:54,264] [INFO] [launch.py:180:main] dist_world_size=1\n",
            "[2026-01-26 01:13:54,264] [INFO] [launch.py:184:main] Setting CUDA_VISIBLE_DEVICES=0\n",
            "[2026-01-26 01:13:54,265] [INFO] [launch.py:272:main] process 6805 spawned with command: ['/usr/bin/python3', '-u', 'sft_lora_ds_hf.py', '--local_rank=0', '--model_name_or_path', 'Qwen/Qwen3-0.6B', '--output_dir', 'output', '--num_train_epochs', '1', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '16', '--learning_rate', '1e-4', '--warmup_ratio', '0.05', '--logging_steps', '5', '--eval_steps', '50', '--save_steps', '50', '--save_total_limit', '2', '--max_seq_length', '2048', '--dataloader_num_workers', '4', '--bf16', 'True', '--deepspeed', 'ds_zero2_bf16.json', '--lora_r', '8', '--lora_alpha', '32', '--lora_target_modules', 'all-linear', '--dataset_1', 'tatsu-lab/alpaca', '--dataset_1_split', 'train', '--dataset_1_n', '2000']\n",
            "2026-01-26 01:14:00.126218: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769390040.146818    6805 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769390040.153099    6805 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769390040.168536    6805 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769390040.168563    6805 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769390040.168566    6805 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769390040.168569    6805 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:919: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
            "  warnings.warn(\n",
            "Map: 100% 2000/2000 [00:00<00:00, 2255.43 examples/s]\n",
            "WARNING:accelerate.accelerator:Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 16. Using DeepSpeed's value.\n",
            "Before initializing optimizer states\n",
            "MA 1.15 GB         Max_MA 1.16 GB         CA 1.18 GB         Max_CA 1 GB \n",
            "CPU Virtual Memory:  used = 9.68 GB, percent = 11.6%\n",
            "After initializing optimizer states\n",
            "MA 1.15 GB         Max_MA 1.17 GB         CA 1.2 GB         Max_CA 1 GB \n",
            "CPU Virtual Memory:  used = 9.68 GB, percent = 11.6%\n",
            "After initializing ZeRO optimizer\n",
            "MA 1.15 GB         Max_MA 1.15 GB         CA 1.2 GB         Max_CA 1 GB \n",
            "CPU Virtual Memory:  used = 9.68 GB, percent = 11.6%\n",
            "{'loss': 2.4157, 'grad_norm': 3.667959451675415, 'learning_rate': 5.714285714285714e-05, 'epoch': 0.04}\n",
            "{'loss': 1.9845, 'grad_norm': 3.5611462593078613, 'learning_rate': 9.830508474576272e-05, 'epoch': 0.08}\n",
            "{'loss': 1.9027, 'grad_norm': 2.8231592178344727, 'learning_rate': 9.40677966101695e-05, 'epoch': 0.12}\n",
            "{'loss': 1.7343, 'grad_norm': 1.901330590248108, 'learning_rate': 8.983050847457629e-05, 'epoch': 0.16}\n",
            "{'loss': 1.5963, 'grad_norm': 1.9185261726379395, 'learning_rate': 8.559322033898305e-05, 'epoch': 0.2}\n",
            "{'loss': 1.7147, 'grad_norm': 1.9640188217163086, 'learning_rate': 8.135593220338983e-05, 'epoch': 0.24}\n",
            "{'loss': 1.5735, 'grad_norm': 1.958548665046692, 'learning_rate': 7.711864406779662e-05, 'epoch': 0.28}\n",
            "{'loss': 1.7112, 'grad_norm': 1.7882945537567139, 'learning_rate': 7.288135593220338e-05, 'epoch': 0.32}\n",
            "{'loss': 1.6451, 'grad_norm': 1.973265290260315, 'learning_rate': 6.864406779661017e-05, 'epoch': 0.36}\n",
            "{'loss': 1.6723, 'grad_norm': 2.045638084411621, 'learning_rate': 6.440677966101695e-05, 'epoch': 0.4}\n",
            " 40% 50/125 [03:26<05:06,  4.09s/it]\n",
            "  0% 0/64 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/64 [00:00<00:02, 20.96it/s]\u001b[A\n",
            "  9% 6/64 [00:00<00:03, 16.40it/s]\u001b[A\n",
            " 12% 8/64 [00:00<00:03, 15.53it/s]\u001b[A\n",
            " 16% 10/64 [00:00<00:03, 15.08it/s]\u001b[A\n",
            " 19% 12/64 [00:00<00:03, 14.82it/s]\u001b[A\n",
            " 22% 14/64 [00:00<00:03, 14.66it/s]\u001b[A\n",
            " 25% 16/64 [00:01<00:03, 14.54it/s]\u001b[A\n",
            " 28% 18/64 [00:01<00:03, 14.50it/s]\u001b[A\n",
            " 31% 20/64 [00:01<00:03, 14.40it/s]\u001b[A\n",
            " 34% 22/64 [00:01<00:02, 14.39it/s]\u001b[A\n",
            " 38% 24/64 [00:01<00:02, 14.29it/s]\u001b[A\n",
            " 41% 26/64 [00:01<00:02, 14.24it/s]\u001b[A\n",
            " 44% 28/64 [00:01<00:02, 14.25it/s]\u001b[A\n",
            " 47% 30/64 [00:02<00:02, 14.20it/s]\u001b[A\n",
            " 50% 32/64 [00:02<00:02, 14.20it/s]\u001b[A\n",
            " 53% 34/64 [00:02<00:02, 14.18it/s]\u001b[A\n",
            " 56% 36/64 [00:02<00:01, 14.21it/s]\u001b[A\n",
            " 59% 38/64 [00:02<00:01, 14.22it/s]\u001b[A\n",
            " 62% 40/64 [00:02<00:01, 14.15it/s]\u001b[A\n",
            " 66% 42/64 [00:02<00:01, 14.18it/s]\u001b[A\n",
            " 69% 44/64 [00:03<00:01, 14.19it/s]\u001b[A\n",
            " 72% 46/64 [00:03<00:01, 14.21it/s]\u001b[A\n",
            " 75% 48/64 [00:03<00:01, 14.20it/s]\u001b[A\n",
            " 78% 50/64 [00:03<00:00, 14.13it/s]\u001b[A\n",
            " 81% 52/64 [00:03<00:00, 14.07it/s]\u001b[A\n",
            " 84% 54/64 [00:03<00:00, 14.07it/s]\u001b[A\n",
            " 88% 56/64 [00:03<00:00, 14.14it/s]\u001b[A\n",
            " 91% 58/64 [00:04<00:00, 14.27it/s]\u001b[A\n",
            " 94% 60/64 [00:04<00:00, 14.32it/s]\u001b[A\n",
            " 97% 62/64 [00:04<00:00, 14.38it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.4582611322402954, 'eval_runtime': 4.7092, 'eval_samples_per_second': 13.591, 'eval_steps_per_second': 13.591, 'epoch': 0.4}\n",
            " 40% 50/125 [03:30<05:06,  4.09s/it]\n",
            "100% 64/64 [00:04<00:00, 13.10it/s]\u001b[A\n",
            "                                   \u001b[A/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:279: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "{'loss': 1.662, 'grad_norm': 1.6228251457214355, 'learning_rate': 6.016949152542373e-05, 'epoch': 0.44}\n",
            "{'loss': 1.6362, 'grad_norm': 2.034334897994995, 'learning_rate': 5.593220338983051e-05, 'epoch': 0.48}\n",
            "{'loss': 1.7499, 'grad_norm': 1.7148997783660889, 'learning_rate': 5.1694915254237284e-05, 'epoch': 0.52}\n",
            "{'loss': 1.6708, 'grad_norm': 2.0146281719207764, 'learning_rate': 4.745762711864407e-05, 'epoch': 0.56}\n",
            "{'loss': 1.6405, 'grad_norm': 1.7765523195266724, 'learning_rate': 4.3220338983050854e-05, 'epoch': 0.6}\n",
            "{'loss': 1.617, 'grad_norm': 1.7769523859024048, 'learning_rate': 3.898305084745763e-05, 'epoch': 0.64}\n",
            "{'loss': 1.6251, 'grad_norm': 1.8087109327316284, 'learning_rate': 3.474576271186441e-05, 'epoch': 0.68}\n",
            "{'loss': 1.6846, 'grad_norm': 2.085437059402466, 'learning_rate': 3.050847457627119e-05, 'epoch': 0.72}\n",
            "{'loss': 1.5775, 'grad_norm': 1.6831166744232178, 'learning_rate': 2.627118644067797e-05, 'epoch': 0.76}\n",
            "{'loss': 1.6682, 'grad_norm': 1.9316449165344238, 'learning_rate': 2.2033898305084748e-05, 'epoch': 0.8}\n",
            " 80% 100/125 [06:57<01:41,  4.08s/it]\n",
            "  0% 0/64 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/64 [00:00<00:03, 20.23it/s]\u001b[A\n",
            "  9% 6/64 [00:00<00:03, 15.81it/s]\u001b[A\n",
            " 12% 8/64 [00:00<00:03, 15.07it/s]\u001b[A\n",
            " 16% 10/64 [00:00<00:03, 14.63it/s]\u001b[A\n",
            " 19% 12/64 [00:00<00:03, 14.42it/s]\u001b[A\n",
            " 22% 14/64 [00:00<00:03, 14.32it/s]\u001b[A\n",
            " 25% 16/64 [00:01<00:03, 14.21it/s]\u001b[A\n",
            " 28% 18/64 [00:01<00:03, 14.08it/s]\u001b[A\n",
            " 31% 20/64 [00:01<00:03, 14.08it/s]\u001b[A\n",
            " 34% 22/64 [00:01<00:03, 13.97it/s]\u001b[A\n",
            " 38% 24/64 [00:01<00:02, 13.75it/s]\u001b[A\n",
            " 41% 26/64 [00:01<00:02, 13.65it/s]\u001b[A\n",
            " 44% 28/64 [00:01<00:02, 13.49it/s]\u001b[A\n",
            " 47% 30/64 [00:02<00:02, 13.50it/s]\u001b[A\n",
            " 50% 32/64 [00:02<00:02, 13.43it/s]\u001b[A\n",
            " 53% 34/64 [00:02<00:02, 13.45it/s]\u001b[A\n",
            " 56% 36/64 [00:02<00:02, 13.44it/s]\u001b[A\n",
            " 59% 38/64 [00:02<00:01, 13.43it/s]\u001b[A\n",
            " 62% 40/64 [00:02<00:01, 13.44it/s]\u001b[A\n",
            " 66% 42/64 [00:03<00:01, 13.40it/s]\u001b[A\n",
            " 69% 44/64 [00:03<00:01, 13.37it/s]\u001b[A\n",
            " 72% 46/64 [00:03<00:01, 13.55it/s]\u001b[A\n",
            " 75% 48/64 [00:03<00:01, 13.70it/s]\u001b[A\n",
            " 78% 50/64 [00:03<00:01, 13.76it/s]\u001b[A\n",
            " 81% 52/64 [00:03<00:00, 13.82it/s]\u001b[A\n",
            " 84% 54/64 [00:03<00:00, 13.89it/s]\u001b[A\n",
            " 88% 56/64 [00:04<00:00, 13.95it/s]\u001b[A\n",
            " 91% 58/64 [00:04<00:00, 14.05it/s]\u001b[A\n",
            " 94% 60/64 [00:04<00:00, 14.17it/s]\u001b[A\n",
            " 97% 62/64 [00:04<00:00, 14.20it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.4189071655273438, 'eval_runtime': 4.8952, 'eval_samples_per_second': 13.074, 'eval_steps_per_second': 13.074, 'epoch': 0.8}\n",
            " 80% 100/125 [07:02<01:41,  4.08s/it]\n",
            "100% 64/64 [00:04<00:00, 12.71it/s]\u001b[A\n",
            "                                   \u001b[A/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:279: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "{'loss': 1.6037, 'grad_norm': 1.9144203662872314, 'learning_rate': 1.7796610169491526e-05, 'epoch': 0.84}\n",
            "{'loss': 1.453, 'grad_norm': 1.7844524383544922, 'learning_rate': 1.3559322033898305e-05, 'epoch': 0.88}\n",
            "{'loss': 1.5214, 'grad_norm': 1.9207191467285156, 'learning_rate': 9.322033898305085e-06, 'epoch': 0.92}\n",
            "{'loss': 1.6002, 'grad_norm': 1.5134059190750122, 'learning_rate': 5.084745762711865e-06, 'epoch': 0.96}\n",
            "{'loss': 1.6086, 'grad_norm': 1.6848305463790894, 'learning_rate': 8.474576271186441e-07, 'epoch': 1.0}\n",
            "100% 125/125 [08:47<00:00,  4.10s/it]/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:279: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "{'train_runtime': 528.6866, 'train_samples_per_second': 3.783, 'train_steps_per_second': 0.236, 'train_loss': 1.6907665405273438, 'epoch': 1.0}\n",
            "100% 125/125 [08:48<00:00,  4.23s/it]\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:279: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "[2026-01-26 01:23:06,326] [INFO] [launch.py:367:main] Process 6805 exits successfully.\n"
          ]
        }
      ],
      "source": [
        "!deepspeed --num_gpus=1 sft_lora_ds_hf.py \\\n",
        "  --model_name_or_path Qwen/Qwen3-0.6B \\\n",
        "  --output_dir output \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --per_device_train_batch_size 1 \\\n",
        "  --per_device_eval_batch_size 1 \\\n",
        "  --gradient_accumulation_steps 16 \\\n",
        "  --learning_rate 1e-4 \\\n",
        "  --warmup_ratio 0.05 \\\n",
        "  --logging_steps 5 \\\n",
        "  --eval_steps 50 \\\n",
        "  --save_steps 50 \\\n",
        "  --save_total_limit 2 \\\n",
        "  --max_seq_length 2048 \\\n",
        "  --dataloader_num_workers 4 \\\n",
        "  --bf16 True \\\n",
        "  --deepspeed ds_zero2_bf16.json \\\n",
        "  --lora_r 8 \\\n",
        "  --lora_alpha 32 \\\n",
        "  --lora_target_modules all-linear \\\n",
        "  --dataset_1 tatsu-lab/alpaca --dataset_1_split train --dataset_1_n 2000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-whhvCL0keT",
        "outputId": "bda3fad5-dd8b-4320-ecd4-660aa81f30a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 324M\n",
            "drwxr-xr-x 4 root root 4.0K Jan 26 01:23 .\n",
            "drwxr-xr-x 1 root root 4.0K Jan 26 01:14 ..\n",
            "-rw-r--r-- 1 root root 1.1K Jan 26 01:23 adapter_config.json\n",
            "-rw-r--r-- 1 root root 309M Jan 26 01:23 adapter_model.safetensors\n",
            "-rw-r--r-- 1 root root  707 Jan 26 01:23 added_tokens.json\n",
            "-rw-r--r-- 1 root root 4.1K Jan 26 01:23 chat_template.jinja\n",
            "drwxr-xr-x 3 root root 4.0K Jan 26 01:21 checkpoint-100\n",
            "drwxr-xr-x 3 root root 4.0K Jan 26 01:23 checkpoint-125\n",
            "-rw-r--r-- 1 root root 1.6M Jan 26 01:23 merges.txt\n",
            "-rw-r--r-- 1 root root 5.1K Jan 26 01:23 README.md\n",
            "-rw-r--r-- 1 root root  613 Jan 26 01:23 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 5.3K Jan 26 01:23 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  11M Jan 26 01:23 tokenizer.json\n",
            "-rw-r--r-- 1 root root 7.0K Jan 26 01:23 training_args.bin\n",
            "-rw-r--r-- 1 root root 2.7M Jan 26 01:23 vocab.json\n"
          ]
        }
      ],
      "source": [
        "!ls -lah output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "m7xusd5s6CS8"
      },
      "outputs": [],
      "source": [
        "!pip -q install -U transformers peft accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXqR3Uh36LFm",
        "outputId": "6ef56200-3914-41bf-bff7-b93545f590ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:919: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Instruction:\n",
            "Explain what LoRA is in 3 bullet points.\n",
            "\n",
            "### Response:\n",
            "LoRA stands for LoRA Optimization. It is an optimization technique used to adjust the weights of a neural network to improve its performance on a given task. It works by modifying the weights of the neural network to better fit the task, allowing it to make more accurate predictions. LoRA can be used to improve the performance of neural networks on tasks such as classification and regression, as well as to improve the performance of deep learning models. It is a fast and effective way to optimize deep learning models and can be used in any number of applications. It is particularly useful when the data is large and the task is complex, as it allows for faster and more accurate results. LoRA can be used in any number of applications, including computer vision, natural language processing, and robotics. It is a powerful and efficient way to improve the performance of neural networks. It can be used in any number of applications and is particularly useful when the data is large and the task is complex. It is a powerful and\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "BASE = \"Qwen/Qwen3-0.6B\"\n",
        "ADAPTER_DIR = \"output\"   # your training output folder\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE,\n",
        "    torch_dtype=torch.float16,   # use fp16 for Colab T4; use bfloat16 on A100/L4 if supported\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(model, ADAPTER_DIR)\n",
        "model.eval()\n",
        "\n",
        "prompt = \"### Instruction:\\nExplain what LoRA is in 3 bullet points.\\n\\n### Response:\\n\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=200,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "print(tokenizer.decode(out[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Merge LoRA into the base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Rh2-Ee36Ops",
        "outputId": "57da8abd-967a-4649-a327-631d1a9bb36c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:572: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications. You can opt to merge the adapter after cloning the weights (to untie the embeddings). You can untie the embeddings by loading the model with `tie_word_embeddings=False`. For example:\n",
            "```python\n",
            "from transformers import AutoModelForCausalLM\n",
            "\n",
            "# Load original tied model\n",
            "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", tie_word_embeddings=False)\n",
            "\n",
            "# Set the randomly initialized lm_head to the previously tied embeddings\n",
            "model.lm_head.weight.data = model.model.embed_tokens.weight.data.clone()\n",
            "\n",
            "# Save the untied model\n",
            "untied_model_dir = \"dir/for/untied/model\"\n",
            "model.save_pretrained(untied_model_dir)\n",
            "model.config.save_pretrained(untied_model_dir)\n",
            "\n",
            "# Now use the original model but in untied format\n",
            "model = AutoModelForCausalLM.from_pretrained(untied_model_dir)\n",
            "```\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved merged model to: output_merged\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "BASE = \"Qwen/Qwen3-0.6B\"\n",
        "ADAPTER_DIR = \"output\"\n",
        "MERGED_DIR = \"output_merged\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE, trust_remote_code=True)\n",
        "\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cpu\",  # merge on CPU to save GPU memory; you can use \"auto\" if you have room\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "peft_model = PeftModel.from_pretrained(base, ADAPTER_DIR)\n",
        "merged = peft_model.merge_and_unload()  # merges LoRA weights into base weights\n",
        "\n",
        "merged.save_pretrained(MERGED_DIR, safe_serialization=True)\n",
        "tokenizer.save_pretrained(MERGED_DIR)\n",
        "\n",
        "print(\"Saved merged model to:\", MERGED_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inference with merged model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfsrfbbt6fvy",
        "outputId": "5c1466a5-b485-4c67-ab1b-1adc14a1bc37"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer you are loading from 'output_merged' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Write a short email asking for a meeting time next week. \n",
            "\n",
            "### Response:\n",
            "Hi [First Name], \n",
            "\n",
            "I'd love to hear from you next week. Could you please tell me the time you'd like to meet? I'd be happy to help you out! \n",
            "\n",
            "Best regards, \n",
            "[Your Name] \n",
            "\n",
            "### Response:\n",
            "Hi [First Name], \n",
            "\n",
            "I'd love to hear from you next week. Could you please tell me the time you'd like to meet? I'd be happy to help you out! \n",
            "\n",
            "Best regards, \n",
            "[Your Name] \n",
            "\n",
            "### Response:\n",
            "Hi [First Name], \n",
            "\n",
            "I'd love to hear from you next week. Could you please tell me the time you'd like to meet? I'd be happy to help you out! \n",
            "\n",
            "Best regards, \n",
            "[\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "MERGED_DIR = \"output_merged\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MERGED_DIR, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MERGED_DIR,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "prompt = \"Write a short email asking for a meeting time next week.\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "out = model.generate(**inputs, max_new_tokens=150)\n",
        "print(tokenizer.decode(out[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8U24CAv6moF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
