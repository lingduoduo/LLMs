{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6abc406d",
   "metadata": {},
   "source": [
    "# 模型忽略关键实体怎么办？注意力权重分配机制引导生成聚焦重点\n",
    "\n",
    "## 场景痛点\n",
    "\n",
    "大语言模型虽然强大，但在特定任务中仍可能出现“走神”现象。\n",
    "\n",
    "例如，在生成摘要时遗漏核心人物名字，在问答系统中无法准确抽取关键日期，或在对话中忽略用户强调的重点。这些情况导致生成内容偏离主题，缺乏关键信息，甚至误导用户。\n",
    "\n",
    "## 根本原因\n",
    "\n",
    "模型在处理文本时，注意力机制未能对关键实体给予足够关注。Transformer 架构通过注意力权重对输入 token 进行加权聚合，从而决定输出内容。如果某些重要实体的注意力权重偏低，它们就很难在最终输出中体现出来。\n",
    "\n",
    "\n",
    "### 方案一： 提示词\n",
    "\n",
    "通过精心设计的 prompt，可以间接引导模型关注特定内容。例如，明确要求模型在回答中使用某些关键词，或以特定结构组织内容。这种方式简单有效，适用于大多数模型\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fc7e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "请用自然流畅的语言，深入探讨一下人工智能和大模型的未来发展趋势，并结合医疗、自动驾驶、智能客服等具体行业，分析它们的潜在应用和挑战。\n",
    "请在你的回答中，尽可能自然地穿插以下词汇：大模型、人工智能、医疗、自动驾驶、智能客服。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83796bb",
   "metadata": {},
   "source": [
    "### 方案二： 自然语言处理\n",
    "[命名实体识别](https://www.modelscope.cn/models/iic/nlp_seqgpt-560m)\n",
    "\n",
    "借助命名实体识别技术，从输入中提取关键实体，并将其插入 prompt 或用于干预模型生成逻辑。该方法自动化程度高，能动态识别关键信息，但依赖外部模块，推理链更复杂。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a82362b",
   "metadata": {},
   "source": [
    "### 方案三：修改 Attention 层，最终生成词汇的概率分布（logits）\n",
    "\n",
    "更直接、有效的方式是干预模型输出层的 logits。logits 是模型对词汇表中每个词的“打分”，在它进入 softmax 之前修改，可以精确提升或降低特定词汇的生成概率。该方法不依赖 prompt，也不需要重新训练模型，适用于推理阶段实时干预。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc14d3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Loading model: Qwen/Qwen2.5-0.5B-Instruct on cpu...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a425b6426ed4392ac6241583d5d8396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10235a72c3a04f8bacbf58839985f88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a04850b3da496faeaaeefe2c2a6787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e14613526be4b7da96e194c576b23c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe4236114274d4abd2aba60ad5c5a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ceb314c022a427c924499f231888523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c83394d5aa4688a387751440cc4f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Focused token IDs: [396, 471, 2473, 2717, 4119, 4128, 6002, 9842, 10506, 11229, 12120, 16488, 16767, 20509, 29846]\n",
      "\n",
      "==================== Baseline Generation ====================\n",
      "Artificial Intelligence (AI) and Large Language Models (LLMs) are rapidly evolving fields that are shaping our world in profound ways. In the next decade, there is likely to be a significant increase in the use of AI in various sectors, including healthcare and autonomous vehicle technology. Here's an analysis of how these technologies are expected to impact each industry, as well as some of the challenges they might face.\n",
      "\n",
      "### Healthcare\n",
      "\n",
      "In healthcare settings, AI and LLMs are poised to revolutionize diagnostic tools, personalized medicine, treatment planning, patient communication, diagnostics, medication management, research, medical imaging, drug discovery, genetic analysis, telemedicine, surgical simulation, clinical decision support systems, health education, quality improvement, risk assessment, etc. These advancements could lead to more accurate diagnoses, faster diagnosis, better treatment plans, improved patient outcomes, reduced costs, increased efficiency, enhanced patient satisfaction, real-time disease monitoring, predictive analytics, evidence-based medicine.\n",
      "\n",
      "However, the implementation of these AI technologies in healthcare must also address ethical concerns such As bias, privacy, security, data privacy and protection, interoperability, regulatory compliance, legal issues, human-machine interaction, ethical considerations, ensuring patient autonomy, improving accessibility and affordability of care, addressing workforce needs, promoting transparency, reducing healthcare disparities, enhancing patient engagement, fostering interdisciplinary collaboration, optimizing resource allocation, developing AI-powered health literacy programs, creating new job roles, training healthcare professionals in AI, managing data security and privacy risks, mitigating cyber threats, maintaining patient privacy standards, aligning AI with human values and ethics, designing AI for patient-centered care.\n",
      "\n",
      "Additionally, integrating AI into healthcare can lead towards a more holistic approach to healthcare that addresses the complex interplay between humans and machines, leading to better overall health outcomes and patient care experiences.\n",
      "\n",
      "---\n",
      "\n",
      "### Autonomous Driving\n",
      "\n",
      "Autonomous driving technology is increasingly becoming an important aspect of modern society. AI is being used to improve the reliability, safety, efficiency of autonomous vehicles, while LMMs play a crucial role in developing algorithms that can handle complex tasks like obstacle avoidance, lane departure, adaptive cruise control, traffic sign recognition, parking assistance, hazard detection, collision avoidance detection and more. This technology has the potential to transform transportation, reduce accidents, increase road safety and fuel consumption, improve urban mobility, enhance public safety.\n",
      "\n",
      "The challenges include ensuring driver safety through AI-driven autonomous systems (ADAS), developing robust and reliable AI algorithms, overcoming technical limitations like sensor noise, motion blur, occlusions, ambiguity, uncertainty, noise and other challenges, handling complex environments and situations,\n",
      "\n",
      "[Keyword Stats] count=12, present=['large language models', 'artificial intelligence', 'healthcare', 'autonomous driving']\n",
      "\n",
      "==================== Keyword-Biased Generation ====================\n",
      "As an artificial language model created through Alibaba's AI technology, I am not bound to follow any specific industry or focus. However, based on my understanding of the current state of AI and language modeling, here is a brief overview of how these technologies are developing and what they might be applied to in various sectors.\n",
      "\n",
      "**Healthcare**: With the advancement of machine learning and deep learning techniques, AI models are becoming increasingly sophisticated and capable of handling complex medical tasks. For instance, in diagnosis, models can analyze vast amounts of medical data to identify patterns that may not be immediately apparent to human experts. In treatment planning, these models help in predicting patient outcomes with high accuracy, which can lead to more personalized treatment plans. Additionally, personalized medicine could see significant improvements as AI can help tailor treatments to individual patients' genetic profiles, improving treatment efficacy and reducing side effects.\n",
      "\n",
      "In terms of healthcare applications, one of my capabilities is to assist in the interpretation of diagnostic results, helping clinicians make more informed decisions. This is particularly beneficial for cases where interpreters are required to interpret the results of complex tests or procedures, such a as MRI scans or CT scans, without requiring medical expertise.\n",
      "\n",
      "On the other hand, there is also the potential for AI to improve healthcare diagnostics by analyzing large datasets generated from patient records and medical devices. These models could potentially predict diseases before they develop into serious conditions, thereby enabling earlier intervention and better patient care.\n",
      "\n",
      "However, the use of language in healthcare raises ethical considerations. There is the risk of misuse of personal information, as well as privacy concerns related to the collection and analysis of sensitive health data. Therefore, it is crucial to ensure that AI systems used in health care are transparent, secure, accountable, fair, responsible, open, interoperable, user-centric, adaptive, resilient, robust, scalable, reliable, sustainable, safe, trustworthy, etc.\n",
      "\n",
      "Lastly, language processing models have shown great potential in areas like language translation, summarization, text generation, sentiment analysis, entity extraction, topic modeling and more. They are being used by businesses for customer support, customer experience improvement, product recommendation, fraud detection, content management, service quality evaluation, search engine optimization, chatbot development, digital identity verification, automated customer assistance, speech recognition, voice command, virtual assistants, smart home automation, machine translation and much more.\n",
      "\n",
      "Looking ahead, we can expect to see more models that can handle a wide range of tasks including language understanding, translation generation and summarizing text, among others. The integration of these AI-powered models will enable them to provide more comprehensive\n",
      "\n",
      "[Keyword Stats] count=4, present=['healthcare']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    LogitsProcessor,\n",
    "    LogitsProcessorList,\n",
    ")\n",
    "\n",
    "# ================== 1. Load Open-Source Model ==================\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"  # change if needed\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME} on {device}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\" if device == \"cuda\" else None,\n",
    ").to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# ================== 2. Keywords and Token IDs ==================\n",
    "keywords = [\n",
    "    \"large language models\",\n",
    "    \"artificial intelligence\",\n",
    "    \"healthcare\",\n",
    "    \"autonomous driving\",\n",
    "    \"intelligent customer service\",\n",
    "]\n",
    "\n",
    "focus_token_ids = set()\n",
    "for kw in keywords:\n",
    "    ids = tokenizer.encode(kw, add_special_tokens=False)\n",
    "    focus_token_ids.update(ids)\n",
    "\n",
    "focus_token_ids = torch.tensor(\n",
    "    sorted(focus_token_ids),\n",
    "    device=device,\n",
    "    dtype=torch.long,\n",
    ")\n",
    "\n",
    "print(f\"Focused token IDs: {focus_token_ids.tolist()}\")\n",
    "\n",
    "# ================== 3. Logits Processor ==================\n",
    "class KeywordBiasLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, token_ids: torch.Tensor, bias: float = 3.0):\n",
    "        self.token_ids = token_ids\n",
    "        self.bias = bias\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        scores[:, self.token_ids] += self.bias\n",
    "        return scores\n",
    "\n",
    "# ================== 4. Prompt Builder ==================\n",
    "def build_chat_prompt(user_prompt: str) -> str:\n",
    "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "        return tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "        )\n",
    "    else:\n",
    "        return f\"User: {user_prompt}\\nAssistant:\"\n",
    "\n",
    "# ================== 5. Text Generation ==================\n",
    "@torch.no_grad()\n",
    "def generate_text(prompt: str, use_bias: bool = False, bias: float = 3.0) -> str:\n",
    "    text = build_chat_prompt(prompt)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    logits_processor = None\n",
    "    if use_bias:\n",
    "        logits_processor = LogitsProcessorList([\n",
    "            KeywordBiasLogitsProcessor(focus_token_ids, bias)\n",
    "        ])\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.05,\n",
    "        no_repeat_ngram_size=2,\n",
    "        logits_processor=logits_processor,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    generated_ids = output_ids[0, inputs[\"input_ids\"].shape[1]:]\n",
    "    return tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "# ================== 6. Keyword Statistics ==================\n",
    "def count_keywords(text: str, keywords):\n",
    "    count = 0\n",
    "    present = []\n",
    "    for kw in keywords:\n",
    "        c = text.lower().count(kw.lower())\n",
    "        if c > 0:\n",
    "            count += c\n",
    "            present.append(kw)\n",
    "    return count, present\n",
    "\n",
    "# ================== 7. Test Prompt ==================\n",
    "test_prompt = (\n",
    "    \"Please discuss the future development trends of artificial intelligence \"\n",
    "    \"and large language models in a clear and natural manner, and analyze their \"\n",
    "    \"potential applications and challenges across specific industries such as \"\n",
    "    \"healthcare, autonomous driving, and intelligent customer service.\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 20 + \" Baseline Generation \" + \"=\" * 20)\n",
    "out1 = generate_text(test_prompt, use_bias=False)\n",
    "print(out1)\n",
    "c1, p1 = count_keywords(out1, keywords)\n",
    "print(f\"\\n[Keyword Stats] count={c1}, present={p1}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 20 + \" Keyword-Biased Generation \" + \"=\" * 20)\n",
    "out2 = generate_text(test_prompt, use_bias=True, bias=3.0)\n",
    "print(out2)\n",
    "c2, p2 = count_keywords(out2, keywords)\n",
    "print(f\"\\n[Keyword Stats] count={c2}, present={p2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8002d2",
   "metadata": {},
   "source": [
    "### 通过干预 Logits 引导模型聚焦关键信息\n",
    "\n",
    "对于像 Qwen 或 Llama 这样的先进自回归模型（Decoder-only Models），它们依赖自注意力机制来理解上下文。简而言之，模型在生成每个新词时，会回顾此前的所有文本，并从中提取相关信息。我们的目标是在这个“回顾”过程中施加影响，让模型更关注我们指定的关键内容。\n",
    "\n",
    "## 高级干预策略\n",
    "\n",
    "### 强力干预（The Hard Boost）\n",
    "\n",
    "最直接的方式是在目标 token 的 logits 上添加一个固定的正向偏置。这种方式干预效果明显，但可能影响文本的自然性，导致关键词重复出现。可通过 `no_repeat_ngram_size` 等参数缓解这一问题。\n",
    "\n",
    "### 温和引导（The Gentle Nudge）\n",
    "\n",
    "更精细的做法是采用加权融合策略，将原始 logits 与关键词偏置进行线性融合：\n",
    "\n",
    "```\n",
    "new_logits = original_logits * (1 - α) + entity_bias * α\n",
    "```\n",
    "\n",
    "其中，α 是一个介于 0 和 1 之间的融合因子，用于控制干预强度。α 越小，干预越温和，生成内容越自然。\n",
    "\n",
    "### 动态衰减\n",
    "\n",
    "还可根据生成阶段动态调整偏置值。例如，在生成初期给予较强干预，随后逐步减弱，使模型在后期拥有更多自由发挥的空间，从而在聚焦关键信息与保持多样性之间取得平衡。\n",
    "\n",
    "## 局限性\n",
    "\n",
    "- **过度聚焦风险**：可能导致生成内容变得狭隘、重复，缺乏创造性。\n",
    "- **计算开销**：虽然单次干预开销较小，但在复杂场景中频繁干预会略微增加推理延迟。\n",
    "\n",
    "## 与其他技术的协同\n",
    "\n",
    "强制聚焦并非孤立手段，它可与多种主流技术结合，实现更优效果：\n",
    "\n",
    "- **提示词工程（Prompt Engineering）**：先用高质量 Prompt 指明方向，再通过干预确保关键细节不丢失。\n",
    "- **RAG（检索增强生成）**：RAG 负责从外部知识库中检索关键信息，而干预技术则确保这些信息在最终输出中得以体现。\n",
    "- **LoRA / QLoRA 微调**：通过微调让模型掌握特定领域知识，再在推理时用干预技术引导模型聚焦具体任务。\n",
    "\n",
    "## 总结\n",
    "\n",
    "通过钩子（Hook）机制干预模型的 Logits 层，是一种强大、可解释的干预方式。它能够引导模型在生成过程中聚焦关键实体，提升输出的准确性与相关性。结合提示词工程、RAG 和微调技术，可以进一步增强干预效果，使其在实际应用中更加稳定、自然地服务于特定任务需求。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
