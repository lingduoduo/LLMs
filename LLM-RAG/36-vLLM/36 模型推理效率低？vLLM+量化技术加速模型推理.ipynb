{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76c82ac2",
   "metadata": {},
   "source": [
    "## 模型推理效率低？vLLM+量化技术加速模型推理\n",
    "\n",
    "在大模型应用的生产部署中，推理效率往往成为制约系统性能的关键瓶颈。\n",
    "\n",
    "本讲通过对比主流推理框架的技术特点，使用 vLLM 结合量化技术的实战案例，为你提供一套完整的性能优化解决方案。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36539d58",
   "metadata": {},
   "source": [
    "## 1. 推理效率问题分析\n",
    "\n",
    "### 1.1 大模型推理的性能瓶颈\n",
    "\n",
    "大模型推理的性能瓶颈主要体现在以下几个维度：\n",
    "\n",
    "- 内存瓶颈\n",
    "- 计算瓶颈\n",
    "- I/O瓶颈\n",
    "\n",
    "### 1.2 传统方案的局限性\n",
    "\n",
    "HuggingFace Transformers：\n",
    "- 设计初衷为研究和原型开发，生产优化不足\n",
    "- 静态批处理机制，无法动态调整批次大小\n",
    "- Python解释器开销，执行效率相对较低\n",
    "  \n",
    "原生PyTorch推理：\n",
    "- 缺乏专门的推理优化，内存管理粗放\n",
    "- 不支持高级优化技术如KV Cache复用\n",
    "- 并发处理能力有限，难以满足高吞吐需求\n",
    "\n",
    "### 1.3 业务场景的性能需求\n",
    "\n",
    "不同业务场景对推理性能的要求差异显著：\n",
    "\n",
    "| 场景类型   | 延迟要求  | 吞吐量要求 | 并发数     | 主要挑战   |\n",
    "|------------|-----------|-------------|-------------|------------|\n",
    "| 在线对话   | <200ms    | 中等        | 100-1000    | 低延迟     |\n",
    "| 批量处理   | 秒级      | 高          | 大批量      | 高吞吐     |\n",
    "| 实时推荐   | <50ms     | 高          | 10000+      | 超低延迟   |\n",
    "| 内容生成   | 1-5s      | 低          | 10-100      | 长文本     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4f941a",
   "metadata": {},
   "source": [
    "## 2. 推理框架对比分析\n",
    "\n",
    "### 2.1 架构设计对比\n",
    "\n",
    "**vLLM架构特点**\n",
    "\n",
    "vLLM采用革命性的PagedAttention技术，核心创新包括：\n",
    "\n",
    "**PagedAttention机制**：\n",
    "- 将KV Cache分割为固定大小的块（默认16 tokens）\n",
    "- 通过块表维护逻辑地址到物理地址的映射\n",
    "- 支持非连续内存分配，显著提升内存利用率\n",
    "  \n",
    "**连续批处理**：\n",
    "- 动态调整批次大小，新请求可随时加入\n",
    "- 支持不同长度序列的并行处理\n",
    "- 实现请求级别的调度和优先级管理\n",
    "  \n",
    "**分布式推理支持**：\n",
    "- 张量并行：模型参数分片到多GPU\n",
    "- 流水线并行：计算流程分阶段执行\n",
    "- 数据并行：多实例并行处理请求\n",
    "  \n",
    "**HuggingFace Transformers架构**\n",
    "\n",
    "**设计理念**：\n",
    "- 面向研究和快速原型开发\n",
    "- 统一的模型接口和丰富的预训练模型\n",
    "- Python原生实现，易于理解和修改\n",
    "  \n",
    "**技术特点**：\n",
    "- 静态批处理，批次大小固定\n",
    "- 内存预分配策略，存在较多碎片\n",
    "- 支持多种硬件后端（CPU/GPU/TPU）\n",
    "  \n",
    "**Ollama架构**\n",
    "\n",
    "**设计目标**：\n",
    "- 简化本地部署流程，一键安装使用\n",
    "- 跨平台支持，降低硬件门槛\n",
    "- 模型管理便捷，支持热切换\n",
    "  \n",
    "**技术实现**：\n",
    "- 基于llama.cpp引擎，C++实现\n",
    "- GGUF模型格式，支持高效量化\n",
    "- CPU/GPU混合推理，适配低端硬件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1efc829",
   "metadata": {},
   "source": [
    "### 2.2 性能对比数据\n",
    "\n",
    "基于LLaMA-13B模型在A100-80G GPU上的测试结果：\n",
    "\n",
    "| 框架           | 吞吐量 (tokens/s) | 平均延迟 (ms) | 显存占用 (GB) | 并发支持  |\n",
    "|----------------|-------------------|---------------|----------------|-----------|\n",
    "| vLLM           | 4150              | 95            | 19.4           | 100+      |\n",
    "| HuggingFace    | 170               | 2350          | 45.2           | 10-20     |\n",
    "| Ollama         | 50                | 800           | 12.8           | 1-5       |\n",
    "\n",
    "关键指标分析：\n",
    "\n",
    "1. 吞吐量提升：vLLM相比HuggingFace提升24倍，相比Ollama提升83倍\n",
    "2. 延迟优化：vLLM延迟降低95%，满足实时交互需求\n",
    "3. 内存效率：vLLM显存利用率达96%，是HuggingFace的2.3倍\n",
    "4. 并发能力：vLLM支持100+并发，适合生产环境高负载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b00e16",
   "metadata": {},
   "source": [
    "###2.3 适用场景与选型建议\n",
    "\n",
    "**vLLM适用场景**\n",
    "- 生产环境高并发服务：在线对话、API服务\n",
    "- 大规模批量处理：内容生成、数据分析\n",
    "- 资源受限环境：需要最大化硬件利用率\n",
    "  \n",
    "优势：性能最优、内存效率高、并发能力强\n",
    "劣势：部署复杂度相对较高、学习成本\n",
    "\n",
    "**HuggingFace Transformers适用场景**\n",
    "- 研究和原型开发：模型实验、算法验证\n",
    "- 小规模推理任务：个人项目、概念验证\n",
    "- 模型微调和训练：结合训练流程的推理\n",
    "  \n",
    "优势：生态丰富、易于使用、文档完善\n",
    "劣势：生产性能不足、内存效率低\n",
    "\n",
    "**Ollama适用场景**\n",
    "- 个人和小团队使用：本地AI助手、学习工具\n",
    "- 边缘设备部署：树莓派、嵌入式设备\n",
    "- 快速体验和测试：模型试用、功能验证\n",
    "  \n",
    "优势：部署简单、硬件门槛低、跨平台支持\n",
    "劣势：性能有限、并发能力弱"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e954c7c",
   "metadata": {},
   "source": [
    "## 3. vLLM核心技术解析\n",
    "\n",
    "### 3.1 PagedAttention算法原理\n",
    "\n",
    "PagedAttention是vLLM的核心创新，借鉴了操作系统虚拟内存管理的思想：\n",
    "\n",
    "**PagedAttention的解决方案**\n",
    "\n",
    "```python\n",
    "# Block management for PagedAttention\n",
    "class PagedAttention:\n",
    "    def __init__(self, block_size=16, num_blocks=1024):\n",
    "        self.block_size = block_size\n",
    "        # Physical block pool, allocated on demand\n",
    "        self.physical_blocks = torch.zeros(\n",
    "            num_blocks, num_heads, block_size, head_dim * 2\n",
    "        )\n",
    "        self.free_blocks = list(range(num_blocks))\n",
    "        # Block table per sequence\n",
    "        self.block_tables = {}\n",
    "        \n",
    "    def allocate_sequence(self, seq_id):\n",
    "        \"\"\"Allocate a block table for a new sequence\"\"\"\n",
    "        self.block_tables[seq_id] = []\n",
    "        \n",
    "    def append_tokens(self, seq_id, new_kv, num_tokens):\n",
    "        \"\"\"Append KV data for newly generated tokens\"\"\"\n",
    "        block_table = self.block_tables[seq_id]\n",
    "        \n",
    "        for i in range(num_tokens):\n",
    "            # Check whether the current block is full\n",
    "            if len(block_table) == 0 or self._is_block_full(block_table[-1]):\n",
    "                # Allocate a new physical block\n",
    "                new_block_id = self.free_blocks.pop()\n",
    "                block_table.append(new_block_id)\n",
    "            \n",
    "            # Store KV data into the selected block\n",
    "            block_id = block_table[-1]\n",
    "            offset = self._get_block_offset(block_id)\n",
    "            self.physical_blocks[block_id, :, offset, :] = new_kv[i]\n",
    "    \n",
    "    def attention_compute(self, seq_id, query):\n",
    "        \"\"\"Compute attention using the sequence's block table\"\"\"\n",
    "        block_table = self.block_tables[seq_id]\n",
    "        \n",
    "        # Gather all KV blocks for this sequence\n",
    "        kv_blocks = []\n",
    "        for block_id in block_table:\n",
    "            kv_blocks.append(self.physical_blocks[block_id])\n",
    "        \n",
    "        # Concatenate into a full KV sequence\n",
    "        full_kv = torch.cat(kv_blocks, dim=1)\n",
    "        return attention_kernel(query, full_kv)\n",
    "```\n",
    "\n",
    "**Copy-on-Write机制**\n",
    "\n",
    "```python\n",
    "class CopyOnWriteManager:\n",
    "    def __init__(self):\n",
    "        self.block_ref_counts = {}  # Reference count per block\n",
    "        \n",
    "    def fork_sequence(self, parent_seq_id, child_seq_id):\n",
    "        \"\"\"Fork a child sequence from a parent sequence, sharing the KV cache\"\"\"\n",
    "        parent_blocks = self.block_tables[parent_seq_id]\n",
    "        child_blocks = []\n",
    "        \n",
    "        for block_id in parent_blocks:\n",
    "            # Increase reference count\n",
    "            self.block_ref_counts[block_id] = \\\n",
    "                self.block_ref_counts.get(block_id, 1) + 1\n",
    "            child_blocks.append(block_id)\n",
    "        \n",
    "        self.block_tables[child_seq_id] = child_blocks\n",
    "    \n",
    "    def copy_on_write(self, seq_id, block_idx):\n",
    "        \"\"\"Copy-on-write: only copy a block when it is about to be modified\"\"\"\n",
    "        block_table = self.block_tables[seq_id]\n",
    "        old_block_id = block_table[block_idx]\n",
    "        \n",
    "        # Check whether a copy is required\n",
    "        if self.block_ref_counts.get(old_block_id, 1) > 1:\n",
    "            # Allocate a new block and copy data\n",
    "            new_block_id = self.free_blocks.pop()\n",
    "            self.physical_blocks[new_block_id] = \\\n",
    "                self.physical_blocks[old_block_id].clone()\n",
    "            \n",
    "            # Update reference counts\n",
    "            self.block_ref_counts[old_block_id] -= 1\n",
    "            self.block_ref_counts[new_block_id] = 1\n",
    "            \n",
    "            # Update the block table\n",
    "            block_table[block_idx] = new_block_id\n",
    "            return new_block_id\n",
    "        \n",
    "        return old_block_id\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0680c5ed",
   "metadata": {},
   "source": [
    "### 3.2 内存管理优化机制\n",
    "\n",
    "**动态内存分配**\n",
    "\n",
    "vLLM的内存管理策略包括：\n",
    "\n",
    "1. 按需分配：只在需要时分配新的内存块\n",
    "2. 即时回收：序列结束后立即释放内存块\n",
    "3. 碎片整理：定期整理内存碎片，提高利用率\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d00656c",
   "metadata": {},
   "source": [
    "## 4. 量化技术深度解析\n",
    "\n",
    "### 4.1 量化算法原理对比\n",
    "\n",
    "**AWQ (Activation-aware Weight Quantization)**\n",
    "\n",
    "AWQ通过分析激活分布来确定重要权重，是目前最先进的量化技术之一：\n",
    "\n",
    "核心原理：\n",
    "1. 激活感知：基于激活值分布而非权重分布确定重要通道\n",
    "2. 通道保护：保护0.1%-1%的显著权重通道不进行量化\n",
    "3. 逐通道缩放：对每个通道应用不同的缩放因子\n",
    "\n",
    "```python\n",
    "class AWQQuantizer:\n",
    "    def __init__(self, w_bit=4, group_size=128):\n",
    "        self.w_bit = w_bit\n",
    "        self.group_size = group_size\n",
    "        \n",
    "    def quantize_model(self, model, calibration_data):\n",
    "        \"\"\"Main AWQ quantization workflow\"\"\"\n",
    "        # 1) Collect activation statistics\n",
    "        activation_stats = self._collect_activation_stats(model, calibration_data)\n",
    "        \n",
    "        # 2) Compute per-layer importance scores\n",
    "        importance_scores = self._calculate_importance_scores(activation_stats)\n",
    "        \n",
    "        # 3) Select weight channels to protect\n",
    "        protected_channels = self._select_protected_channels(importance_scores)\n",
    "        \n",
    "        # 4) Apply quantization\n",
    "        quantized_model = self._apply_quantization(model, protected_channels)\n",
    "        \n",
    "        return quantized_model\n",
    "    \n",
    "    def _collect_activation_stats(self, model, calibration_data):\n",
    "        \"\"\"Collect activation statistics\"\"\"\n",
    "        stats = {}\n",
    "        \n",
    "        def hook_fn(name):\n",
    "            def hook(module, input, output):\n",
    "                if name not in stats:\n",
    "                    stats[name] = []\n",
    "                # Record activation statistics\n",
    "                stats[name].append({\n",
    "                    \"mean\": output.mean(dim=0),\n",
    "                    \"std\": output.std(dim=0),\n",
    "                    \"max\": output.max(dim=0)[0],\n",
    "                })\n",
    "            return hook\n",
    "        \n",
    "        # Register forward hooks\n",
    "        hooks = []\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, (torch.nn.Linear, torch.nn.Conv2d)):\n",
    "                hook = module.register_forward_hook(hook_fn(name))\n",
    "                hooks.append(hook)\n",
    "        \n",
    "        # Run calibration data\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in calibration_data:\n",
    "                model(batch)\n",
    "        \n",
    "        # Remove hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def _calculate_importance_scores(self, activation_stats):\n",
    "        \"\"\"Compute importance scores for weight channels\"\"\"\n",
    "        importance_scores = {}\n",
    "        \n",
    "        for layer_name, stats_list in activation_stats.items():\n",
    "            # Aggregate stats across multiple batches\n",
    "            mean_acts = torch.stack([s[\"mean\"] for s in stats_list]).mean(0)\n",
    "            max_acts = torch.stack([s[\"max\"] for s in stats_list]).max(0)[0]\n",
    "            \n",
    "            # Importance score (combining mean and max)\n",
    "            importance = mean_acts * 0.7 + max_acts * 0.3\n",
    "            importance_scores[layer_name] = importance\n",
    "        \n",
    "        return importance_scores\n",
    "    \n",
    "    def _select_protected_channels(self, importance_scores, protect_ratio=0.01):\n",
    "        \"\"\"Select weight channels to protect\"\"\"\n",
    "        protected_channels = {}\n",
    "        \n",
    "        for layer_name, scores in importance_scores.items():\n",
    "            # Protect the top 1% channels\n",
    "            num_protect = max(1, int(len(scores) * protect_ratio))\n",
    "            _, top_indices = torch.topk(scores, num_protect)\n",
    "            protected_channels[layer_name] = top_indices\n",
    "        \n",
    "        return protected_channels\n",
    "```\n",
    "\n",
    "\n",
    "**GPTQ (Gradient Post-training Quantization)**\n",
    "\n",
    "GPTQ基于梯度信息进行后训练量化：\n",
    "\n",
    "```python\n",
    "class GPTQQuantizer:\n",
    "    def __init__(self, w_bit=4, group_size=128, damp_percent=0.01):\n",
    "        self.w_bit = w_bit\n",
    "        self.group_size = group_size\n",
    "        self.damp_percent = damp_percent\n",
    "        \n",
    "    def quantize_layer(self, layer, input_data):\n",
    "        \"\"\"Quantize a single linear layer\"\"\"\n",
    "        W = layer.weight.data.clone()\n",
    "        H = self._compute_hessian(layer, input_data)\n",
    "        \n",
    "        # Add a damping term for numerical stability\n",
    "        damp = self.damp_percent * torch.mean(torch.diag(H))\n",
    "        diag = torch.arange(H.shape[0], device=H.device)\n",
    "        H[diag, diag] += damp\n",
    "        \n",
    "        # Cholesky-based factorization to obtain an upper-triangular factor\n",
    "        H = torch.linalg.cholesky(H)\n",
    "        H = torch.cholesky_inverse(H)\n",
    "        H = torch.linalg.cholesky(H, upper=True)\n",
    "        Hinv = H\n",
    "        \n",
    "        # Quantize column-by-column\n",
    "        for i in range(W.shape[1]):\n",
    "            # Quantize the current column\n",
    "            w_col = W[:, i]\n",
    "            q_col = self._quantize_column(w_col)\n",
    "            \n",
    "            # Quantization error\n",
    "            error = w_col - q_col\n",
    "            \n",
    "            # Update subsequent weights to compensate for the error\n",
    "            W[:, i:] -= (error.unsqueeze(1) * Hinv[i, i:].unsqueeze(0))\n",
    "            \n",
    "            # Write back the quantized column\n",
    "            W[:, i] = q_col\n",
    "        \n",
    "        layer.weight.data = W\n",
    "        return layer\n",
    "    \n",
    "    def _compute_hessian(self, layer, input_data):\n",
    "        \"\"\"Compute the Hessian matrix (approximation)\"\"\"\n",
    "        # Use input data to accumulate second-order information\n",
    "        H = torch.zeros(\n",
    "            (layer.in_features, layer.in_features),\n",
    "            device=layer.weight.device\n",
    "        )\n",
    "        \n",
    "        for batch in input_data:\n",
    "            # Outer product of inputs\n",
    "            inp = batch.view(-1, layer.in_features)\n",
    "            H += inp.t() @ inp\n",
    "        \n",
    "        return H / len(input_data)\n",
    "    \n",
    "    def _quantize_column(self, w_col):\n",
    "        \"\"\"Quantize a single weight column\"\"\"\n",
    "        # Compute the quantization range\n",
    "        w_min = w_col.min()\n",
    "        w_max = w_col.max()\n",
    "        \n",
    "        # Symmetric quantization\n",
    "        scale = max(abs(w_min), abs(w_max)) / (2 ** (self.w_bit - 1) - 1)\n",
    "        \n",
    "        # Quantize and dequantize\n",
    "        q_col = torch.round(w_col / scale).clamp(\n",
    "            -(2 ** (self.w_bit - 1)), 2 ** (self.w_bit - 1) - 1\n",
    "        )\n",
    "        q_col = q_col * scale\n",
    "        \n",
    "        return q_col\n",
    "```\n",
    "\n",
    "**FP8量化技术**\n",
    "\n",
    "FP8使用8位浮点数表示，相比INT8能更好地保持数值精度：\n",
    "\n",
    "```python\n",
    "class FP8Quantizer:\n",
    "    def __init__(self, format=\"E4M3\"):  # E4M3 or E5M2\n",
    "        self.format = format\n",
    "        if format == \"E4M3\":\n",
    "            self.exp_bits = 4\n",
    "            self.mantissa_bits = 3\n",
    "        else:  # E5M2\n",
    "            self.exp_bits = 5\n",
    "            self.mantissa_bits = 2\n",
    "            \n",
    "    def quantize_tensor(self, tensor):\n",
    "        \"\"\"Quantize an FP32 tensor to FP8\"\"\"\n",
    "        # Compute the quantization range\n",
    "        if self.format == \"E4M3\":\n",
    "            max_val = 448.0  # Maximum value for the E4M3 format\n",
    "        else:\n",
    "            max_val = 57344.0  # Maximum value for the E5M2 format\n",
    "        \n",
    "        # Scale into the FP8 range\n",
    "        scale = max_val / tensor.abs().max()\n",
    "        scaled_tensor = tensor * scale\n",
    "        \n",
    "        # Simulate the FP8 quantization process\n",
    "        quantized = self._simulate_fp8_quantization(scaled_tensor)\n",
    "        \n",
    "        return quantized / scale, scale\n",
    "    \n",
    "    def _simulate_fp8_quantization(self, tensor):\n",
    "        \"\"\"Simulate FP8 quantization\"\"\"\n",
    "        # Extract the sign bit\n",
    "        sign = torch.sign(tensor)\n",
    "        abs_tensor = torch.abs(tensor)\n",
    "        \n",
    "        # Convert into an FP8-like representation\n",
    "        if self.format == \"E4M3\":\n",
    "            # E4M3: 1 sign bit + 4 exponent bits + 3 mantissa bits\n",
    "            quantized = self._quantize_e4m3(abs_tensor)\n",
    "        else:\n",
    "            # E5M2: 1 sign bit + 5 exponent bits + 2 mantissa bits\n",
    "            quantized = self._quantize_e5m2(abs_tensor)\n",
    "        \n",
    "        return sign * quantized\n",
    "    \n",
    "    def _quantize_e4m3(self, tensor):\n",
    "        \"\"\"Quantize using the E4M3 format\"\"\"\n",
    "        # Compute exponent and mantissa\n",
    "        log2_tensor = torch.log2(tensor + 1e-8)\n",
    "        exponent = torch.floor(log2_tensor).clamp(-6, 8)  # 4-bit exponent range\n",
    "        \n",
    "        # Compute mantissa\n",
    "        mantissa_scale = 2 ** exponent\n",
    "        mantissa = tensor / mantissa_scale - 1.0\n",
    "        \n",
    "        # Quantize mantissa to 3-bit precision\n",
    "        mantissa_quantized = torch.round(mantissa * 8) / 8\n",
    "        \n",
    "        # Reconstruct the quantized value\n",
    "        quantized = (1.0 + mantissa_quantized) * mantissa_scale\n",
    "        \n",
    "        return quantized\n",
    "```\n",
    "\n",
    "### 4.2 量化技术性能对比\n",
    "\n",
    "基于Qwen3-8B模型的量化效果对比：\n",
    "\n",
    "\n",
    "| 量化方法     | 模型大小 | 内存占用 | 推理速度 | MMLU分数 | 精度损失 |\n",
    "|--------------|----------|----------|----------|----------|----------|\n",
    "| FP32         | 32GB     | 32GB     | 基准     | 74.7     | 0%       |\n",
    "| FP16         | 16GB     | 16GB     | 1.8x     | 74.5     | 0.3%     |\n",
    "| AWQ-4bit     | 4.5GB    | 6GB      | 3.2x     | 72.1     | 3.5%     |\n",
    "| GPTQ-4bit    | 4.5GB    | 6GB      | 2.8x     | 71.3     | 4.6%     |\n",
    "| FP8          | 8GB      | 8GB      | 2.1x     | 74.2     | 0.7%     |\n",
    "\n",
    "\n",
    "\n",
    "选择建议：\n",
    "- 追求极致性能：AWQ-4bit，在可接受的精度损失下获得最大加速\n",
    "- 平衡性能和精度：FP8，较小的精度损失，适中的性能提升\n",
    "- 兼容性优先：GPTQ-4bit，生态支持最好，部署最稳定\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289e9eff",
   "metadata": {},
   "source": [
    "  \n",
    "## 5. Qwen3-8B实战部署\n",
    "\n",
    "### 5.1 环境搭建和依赖配置\n",
    "\n",
    "系统要求\n",
    "\n",
    "- 操作系统：Ubuntu 20.04+ / CentOS 8+\n",
    "- Python版本：3.9-3.11\n",
    "- CUDA版本：12.1+\n",
    "- GPU要求：RTX 3060 12GB以上（推荐RTX 4090/A100）\n",
    "- 内存要求：32GB以上系统内存\n",
    "  \n",
    "环境安装脚本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e019c1",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "# qwen3_vllm_setup.sh - Setup script for Qwen3-8B + vLLM deployment environment\n",
    "\n",
    "set -e\n",
    "\n",
    "echo \"=== Setting up Qwen3-8B vLLM Deployment Environment ===\"\n",
    "\n",
    "# 1. Check system prerequisites\n",
    "echo \"Checking system prerequisites...\"\n",
    "if ! command -v nvidia-smi &> /dev/null; then\n",
    "    echo \"ERROR: NVIDIA GPU driver not detected\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "if ! command -v nvcc &> /dev/null; then\n",
    "    echo \"ERROR: CUDA toolkit (nvcc) not detected\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# Check CUDA version\n",
    "CUDA_VERSION=$(nvcc --version | grep \"release\" | awk '{print $6}' | cut -c2-)\n",
    "echo \"Detected CUDA version: $CUDA_VERSION\"\n",
    "\n",
    "# 2. Create Python environment\n",
    "echo \"Creating Python virtual environment...\"\n",
    "if ! command -v conda &> /dev/null; then\n",
    "    echo \"Installing Miniconda...\"\n",
    "    wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "    bash Miniconda3-latest-Linux-x86_64.sh -b -p $HOME/miniconda3\n",
    "    source $HOME/miniconda3/bin/activate\n",
    "    conda init bash\n",
    "fi\n",
    "\n",
    "conda create -n qwen3-vllm python=3.11 -y\n",
    "conda activate qwen3-vllm\n",
    "\n",
    "# 3. Install PyTorch\n",
    "echo \"Installing PyTorch...\"\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# 4. Install vLLM\n",
    "echo \"Installing vLLM...\"\n",
    "pip install vllm\n",
    "\n",
    "# 5. Install additional dependencies\n",
    "echo \"Installing additional dependencies...\"\n",
    "pip install transformers accelerate bitsandbytes\n",
    "pip install fastapi uvicorn\n",
    "pip install numpy pandas matplotlib seaborn\n",
    "pip install psutil GPUtil\n",
    "pip install vllm\n",
    "\n",
    "# 6. Verify installation\n",
    "echo \"Verifying installation...\"\n",
    "python -c \"\n",
    "import torch\n",
    "import vllm\n",
    "import transformers\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "print(f'Number of GPUs: {torch.cuda.device_count()}')\n",
    "print(f'vLLM version: {vllm.__version__}')\n",
    "print(f'Transformers version: {transformers.__version__}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU model: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')\n",
    "\"\n",
    "echo \"Setup complete!\"\n",
    "echo \"Activate the environment with: conda activate qwen3-vllm\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312306af",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "source": [
    "**模型部署和运行**\n",
    "\n",
    "VLLM_USE_MODELSCOPE=True vllm serve Qwen/Qwen3-8B --dtype auto  --max-model-len 16384   --api-key token-123\n",
    "\n",
    "VLLM_USE_MODELSCOPE=true vllm serve Qwen/Qwen3-8B --enable-reasoning --reasoning-parser deepseek_r1\n",
    "\n",
    "\n",
    "如果尚未下载模型，系统会自动开始下载。\n",
    "\n",
    "若出现网络超时或下载失败，建议从魔搭（ModelScope）社区获取模型。\n",
    "\n",
    "默认情况下，vLLM 从 Hugging Face 下载模型，如需切换至魔搭，请设置环境变量：\n",
    "\n",
    "**export VLLM_USE_MODELSCOPE=True**\n",
    "\n",
    "你可以像调用 OpenAI API 一样调用 vLLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e89b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"token-123\",\n",
    ")\n",
    " \n",
    "completion = client.chat.completions.create(\n",
    "  model=\"Qwen/Qwen3-8B\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"你是谁？\"}\n",
    "  ]\n",
    ")\n",
    " \n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded4220a",
   "metadata": {},
   "source": [
    "## 5.2 原始模型部署和性能基准\n",
    "\n",
    "**基础部署代码**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a9322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen3_basic_deployment.py - Basic deployment for Qwen3-8B\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import torch\n",
    "import psutil\n",
    "import GPUtil\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "class Qwen3BasicDeployment:\n",
    "    def __init__(self, model_path: str, gpu_memory_utilization: float = 0.8):\n",
    "        self.model_path = model_path\n",
    "        self.gpu_memory_utilization = gpu_memory_utilization\n",
    "        self.llm = None\n",
    "        self.performance_metrics: Dict[str, Any] = {}\n",
    "\n",
    "    def initialize_model(self):\n",
    "        \"\"\"Initialize the Qwen3-8B model\"\"\"\n",
    "        print(\"Initializing Qwen3-8B model...\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        self.llm = LLM(\n",
    "            model=self.model_path,\n",
    "            tensor_parallel_size=1,  # Single-GPU deployment\n",
    "            gpu_memory_utilization=self.gpu_memory_utilization,\n",
    "            max_model_len=32768,  # Qwen3 supports 32K context\n",
    "            trust_remote_code=True,\n",
    "            enforce_eager=False,  # Use CUDA Graph optimization\n",
    "            swap_space=4,  # 4GB swap space\n",
    "            enable_thinking=False,  # Setting enable_thinking=False disables thinking mode\n",
    "        )\n",
    "\n",
    "        init_time = time.time() - start_time\n",
    "        print(f\"Model initialization complete. Time taken: {init_time:.2f}s\")\n",
    "\n",
    "        # Record initialization metrics\n",
    "        self.performance_metrics[\"init_time\"] = init_time\n",
    "        self.performance_metrics[\"model_size_gb\"] = self._get_model_size_gb()\n",
    "        self.performance_metrics[\"gpu_memory\"] = self._get_gpu_memory_usage()\n",
    "\n",
    "    def _get_model_size_gb(self) -> float:\n",
    "        \"\"\"Get model size (GB)\"\"\"\n",
    "        total_size = 0\n",
    "        for root, _, files in os.walk(self.model_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                total_size += os.path.getsize(file_path)\n",
    "        return total_size / (1024**3)\n",
    "\n",
    "    def _get_gpu_memory_usage(self) -> Dict[str, float]:\n",
    "        \"\"\"Get GPU memory usage\"\"\"\n",
    "        if torch.cuda.is_available() and GPUtil.getGPUs():\n",
    "            gpu = GPUtil.getGPUs()[0]\n",
    "            return {\n",
    "                \"used_gb\": gpu.memoryUsed / 1024,\n",
    "                \"total_gb\": gpu.memoryTotal / 1024,\n",
    "                \"utilization\": gpu.memoryUsed / gpu.memoryTotal if gpu.memoryTotal else 0.0,\n",
    "            }\n",
    "        return {}\n",
    "\n",
    "    def run_performance_benchmark(self, test_cases: List[str], num_runs: int = 3):\n",
    "        \"\"\"Run performance benchmark\"\"\"\n",
    "        if self.llm is None:\n",
    "            raise RuntimeError(\"Model is not initialized. Call initialize_model() first.\")\n",
    "\n",
    "        print(f\"Running performance benchmark ({num_runs} runs)...\")\n",
    "\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            max_tokens=512,\n",
    "            stop=[\"<|im_end|>\"],\n",
    "        )\n",
    "\n",
    "        all_results = []\n",
    "\n",
    "        tokenizer = self.llm.get_tokenizer()\n",
    "\n",
    "        for run in range(num_runs):\n",
    "            print(f\"Run {run + 1}...\")\n",
    "\n",
    "            # Warm-up\n",
    "            if run == 0:\n",
    "                print(\"Warming up the model...\")\n",
    "                _ = self.llm.generate(test_cases[:2], sampling_params)\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.synchronize()\n",
    "\n",
    "            # Record start state\n",
    "            start_memory = self._get_gpu_memory_usage()\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Run inference\n",
    "            outputs = self.llm.generate(test_cases, sampling_params)\n",
    "\n",
    "            # Record end state\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            end_time = time.time()\n",
    "            end_memory = self._get_gpu_memory_usage()\n",
    "\n",
    "            # Compute metrics\n",
    "            total_time = end_time - start_time\n",
    "            total_input_tokens = sum(len(tokenizer.encode(prompt)) for prompt in test_cases)\n",
    "            total_output_tokens = sum(len(out.outputs[0].token_ids) for out in outputs)\n",
    "\n",
    "            run_results = {\n",
    "                \"run\": run + 1,\n",
    "                \"total_time\": total_time,\n",
    "                \"avg_latency\": total_time / len(test_cases),\n",
    "                \"throughput_tokens_per_sec\": total_output_tokens / total_time if total_time > 0 else 0.0,\n",
    "                \"throughput_requests_per_sec\": len(test_cases) / total_time if total_time > 0 else 0.0,\n",
    "                \"total_input_tokens\": total_input_tokens,\n",
    "                \"total_output_tokens\": total_output_tokens,\n",
    "                \"memory_usage\": end_memory,\n",
    "                \"memory_increase_gb\": (\n",
    "                    end_memory.get(\"used_gb\", 0.0) - start_memory.get(\"used_gb\", 0.0)\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            all_results.append(run_results)\n",
    "            print(f\"  Latency:    {run_results['avg_latency']:.3f}s\")\n",
    "            print(f\"  Throughput: {run_results['throughput_tokens_per_sec']:.1f} tokens/s\")\n",
    "\n",
    "        # Aggregate metrics\n",
    "        avg_results = self._calculate_average_metrics(all_results)\n",
    "        self.performance_metrics[\"benchmark\"] = avg_results\n",
    "        self.performance_metrics[\"benchmark_runs\"] = all_results\n",
    "\n",
    "        return avg_results\n",
    "\n",
    "    def _calculate_average_metrics(self, results: List[Dict[str, Any]]) -> Dict[str, float]:\n",
    "        \"\"\"Compute average performance metrics\"\"\"\n",
    "        metrics = [\n",
    "            \"total_time\",\n",
    "            \"avg_latency\",\n",
    "            \"throughput_tokens_per_sec\",\n",
    "            \"throughput_requests_per_sec\",\n",
    "        ]\n",
    "\n",
    "        avg_results: Dict[str, float] = {}\n",
    "        for metric in metrics:\n",
    "            values = [r[metric] for r in results]\n",
    "            mean = sum(values) / len(values) if values else 0.0\n",
    "            var = sum((x - mean) ** 2 for x in values) / len(values) if values else 0.0\n",
    "            avg_results[f\"avg_{metric}\"] = mean\n",
    "            avg_results[f\"std_{metric}\"] = var ** 0.5\n",
    "\n",
    "        return avg_results\n",
    "\n",
    "    def generate_text(self, prompt: str, max_tokens: int = 512) -> str:\n",
    "        \"\"\"Generate text\"\"\"\n",
    "        if self.llm is None:\n",
    "            raise RuntimeError(\"Model is not initialized. Call initialize_model() first.\")\n",
    "\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            max_tokens=max_tokens,\n",
    "            stop=[\"<|im_end|>\"],\n",
    "        )\n",
    "\n",
    "        outputs = self.llm.generate([prompt], sampling_params)\n",
    "        return outputs[0].outputs[0].text\n",
    "\n",
    "    def save_metrics(self, filepath: str):\n",
    "        \"\"\"Save performance metrics to a file\"\"\"\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self.performance_metrics, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Performance metrics saved to: {filepath}\")\n",
    "\n",
    "\n",
    "# Test cases\n",
    "TEST_CASES = [\n",
    "    \"Please explain the principle of attention mechanisms in deep learning in detail.\",\n",
    "    \"How would you design a distributed system architecture for high concurrency?\",\n",
    "    \"Analyze current trends and challenges in AI technology.\",\n",
    "    \"Write a Python function to implement the quicksort algorithm.\",\n",
    "    \"Explain core concepts and application scenarios of blockchain technology.\",\n",
    "    \"How can you optimize database query performance? Provide specific methods.\",\n",
    "    \"Analyze the differences and connections between cloud computing and edge computing.\",\n",
    "    \"Introduce the overfitting problem in machine learning and how to address it.\",\n",
    "]\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entry point\"\"\"\n",
    "    model_path = \"./models/qwen3-8b-original\"\n",
    "\n",
    "    # Check model path\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"ERROR: Model path does not exist: {model_path}\")\n",
    "        print(\"Please run download_qwen3.py to download the model first.\")\n",
    "        return\n",
    "\n",
    "    # Create deployment instance\n",
    "    deployment = Qwen3BasicDeployment(model_path)\n",
    "\n",
    "    # Initialize model\n",
    "    deployment.initialize_model()\n",
    "\n",
    "    # Run benchmark\n",
    "    benchmark_results = deployment.run_performance_benchmark(TEST_CASES)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n=== Performance Benchmark Results ===\")\n",
    "    print(f\"Average latency:    {benchmark_results['avg_avg_latency']:.3f}s\")\n",
    "    print(f\"Average throughput: {benchmark_results['avg_throughput_tokens_per_sec']:.1f} tokens/s\")\n",
    "\n",
    "    gpu_mem = deployment.performance_metrics.get(\"gpu_memory\", {})\n",
    "    if gpu_mem:\n",
    "        print(f\"GPU memory used:    {gpu_mem.get('used_gb', 0.0):.1f} GB\")\n",
    "        print(f\"Memory utilization: {gpu_mem.get('utilization', 0.0):.1%}\")\n",
    "\n",
    "    # Save metrics\n",
    "    deployment.save_metrics(\"qwen3_original_metrics.json\")\n",
    "\n",
    "    # Interactive loop\n",
    "    print(\"\\n=== Interactive Test ===\")\n",
    "    while True:\n",
    "        user_input = input(\"\\nEnter a question (type 'quit' to exit): \")\n",
    "        if user_input.lower() == \"quit\":\n",
    "            break\n",
    "\n",
    "        start_time = time.time()\n",
    "        response = deployment.generate_text(user_input)\n",
    "        end_time = time.time()\n",
    "\n",
    "        print(f\"\\nResponse: {response}\")\n",
    "        print(f\"Response time: {end_time - start_time:.2f}s\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bee90b",
   "metadata": {},
   "source": [
    "### 5.3 量化模型部署和性能对比\n",
    "\n",
    "**量化模型部署代码**\n",
    "\n",
    "模型加载时自动转为 FP8 格式进行计算,FP8：几乎无损，速度快，显存节省明显\n",
    "\n",
    "```python\n",
    "# 在初始化时加入：\n",
    "self.llm = LLM(\n",
    "    model=self.model_path,\n",
    "    tensor_parallel_size=1,\n",
    "    gpu_memory_utilization=0.95,\n",
    "    max_model_len=32768,\n",
    "    trust_remote_code=True,\n",
    "    enforce_eager=False,\n",
    "    swap_space=4,\n",
    "    \n",
    "    quantization=\"fp8\",           # ← 启用 FP8\n",
    "    dtype=torch.float16,         # ← 必须指定\n",
    ")\n",
    "```\n",
    "\n",
    "**使用 AWQ 4-bit 量化**\n",
    "\n",
    "如果你希望进一步节省显存，建议使用 AWQ 量化版模型，例如从 Huggingface 下载 Qwen3-8B-AWQ\n",
    "\n",
    "然后修改代码中 model_path 和添加 quantization=\"awq\"\n",
    "\n",
    "```python\n",
    "self.llm = LLM(\n",
    "    model=\"./models/qwen3-8b-awq\",           # 指向 AWQ 模型\n",
    "    tensor_parallel_size=1,\n",
    "    gpu_memory_utilization=0.95,            # 可适当提高\n",
    "    max_model_len=16384,                    # 可支持更长上下文\n",
    "    trust_remote_code=True,\n",
    "    enforce_eager=False,\n",
    "    swap_space=2,\n",
    "    \n",
    "    # 启用 AWQ 量化\n",
    "    quantization=\"awq\",\n",
    "    dtype=\"auto\",\n",
    ")\n",
    "```\n",
    "\n",
    "**如果你想尝试 GPTQ（类似）**\n",
    "```python\n",
    "self.llm = LLM(\n",
    "    model=\"./models/qwen3-8b-gptq\",      # 必须是 GPTQ 量化后的模型\n",
    "    quantization=\"gptq\",\n",
    "    dtype=\"auto\",\n",
    "    ...\n",
    ")\n",
    "```\n",
    "\n",
    "###  常见问题与建议\n",
    "\n",
    "| 问题 | 原因分析 | 推荐解决方案 |\n",
    "|------|----------|---------------|\n",
    "| `ValueError: FP8 is not supported on this device` | 当前 GPU 不支持 FP8 计算（需 Ampere 架构及以上） | - 使用 A10、A100、L4、H100 等支持 Tensor Core 的显卡<br>- 改用 AWQ/GPTQ 量化或降低 `max_model_len` |\n",
    "| `CUDA out of memory` | KV Cache 显存不足，尤其是长上下文场景 | - 启用 `FP8` 或 `AWQ` 量化<br>- 降低 `max_model_len`（如设为 `8192` 或 `16384`）<br>- 提高 `gpu_memory_utilization` 至 `0.95` |\n",
    "| 量化未生效 / 没有性能提升 | 未正确配置 `quantization` 参数或模型不匹配 | - 确保 `quantization=\"fp8\"` 且 `dtype=torch.float16`<br>- AWQ/GPTQ 需使用**预先量化好的模型文件**，不能直接对原模型启用 |\n",
    "| 启动报错：`Unknown quantization method` | vLLM 版本过旧，不支持该量化方式 | 升级 vLLM 到最新版本：<br>`pip install -U vllm` |\n",
    "| 推理速度慢 | 未启用 CUDA Graph 或硬件利用率低 | - 设置 `enforce_eager=False`（启用 CUDA Graph）<br>- 使用更高吞吐的量化格式（如 FP8 可提速 2x） |\n",
    "| 多轮对话崩溃 | 上下文过长导致显存溢出 | - 限制输入长度<br>- 使用滑动窗口或摘要机制管理历史记录 |\n",
    "\n",
    "### 温馨提示\n",
    "- **FP8**：适合支持设备，几乎无损，推荐优先尝试。\n",
    "- **AWQ/GPTQ**：极致省显存，适合 16GB 以下显卡部署大模型。\n",
    "- **不要混合使用量化方式**：只能选择一种 `quantization` 参数。\n",
    "- 查看日志确认是否成功加载量化：搜索 `Using FP8` 或 `Quantization: awq` 等关键字。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1da81fd",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "性能提升：\n",
    "- 吞吐量提升24倍（相比HuggingFace Transformers）\n",
    "- 延迟降低95%，满足实时交互需求\n",
    "- 显存利用率提升至96%，支持更大并发\n",
    "  \n",
    "成本优化：\n",
    "- AWQ量化可节省硬件成本60%以上\n",
    "- 运营成本降低，投资回报率超过50%\n",
    "- 支持在消费级GPU上部署企业级服务\n",
    "\n",
    "技术选型建议：\n",
    "- 小型团队：RTX 4090 + AWQ量化，成本效益最优\n",
    "- 中型企业：A100 + GPTQ量化，平衡性能和成本\n",
    "- 大规模部署：A100集群 + FP8量化，追求极致性能"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
