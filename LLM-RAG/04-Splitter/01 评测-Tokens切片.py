# pip install llama-index llama-index-llms-openai-like llama-index-embeddings-dashscope llama-index-readers-file

import os
from llama_index.core import VectorStoreIndex, Settings, Document
from llama_index.core.node_parser import (
    SentenceWindowNodeParser,
    SemanticSplitterNodeParser,
    TokenTextSplitter,
)
from llama_index.core.postprocessor import MetadataReplacementPostProcessor

# Import OpenAILike LLM (for DashScope compatible-mode, Qwen models)
from llama_index.llms.openai_like import OpenAILike

# Import DashScopeEmbedding (Alibaba Cloud DashScope embedding models)
from llama_index.embeddings.dashscope import DashScopeEmbedding

# --- Configure Alibaba Cloud DashScope API Key ---
# Make sure you have DASHSCOPE_API_KEY set in your environment variables
# Or set it here directly (NOT recommended for production):
# os.environ["DASHSCOPE_API_KEY"] = "sk-xxx"

# Read API Key from env
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY")

# Initialize global LlamaIndex settings

# 1) Configure LLM (use DashScope qwen-plus via OpenAILike)
Settings.llm = OpenAILike(
    model="qwen-plus",
    api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
    api_key=DASHSCOPE_API_KEY,
    is_chat_model=True,
    temperature=0.1,  # controls randomness
)

# 2) Configure embedding model (use DashScopeEmbedding)
Settings.embed_model = DashScopeEmbedding(
    model_name="text-embedding-v4",
    api_key=DASHSCOPE_API_KEY,
)


def evaluate_splitter(splitter, documents, question, splitter_name):
    """
    Evaluate different document chunking/splitting strategies.
    Prints retrieval-related outputs manually so you can compare results directly.
    """
    print(f"\n{'=' * 50}")
    print(f"Testing with splitter: {splitter_name} ...")
    print(f"{'=' * 50}\n")

    # Show raw chunks generated by the splitter
    print(f"[{splitter_name}] Raw document chunks (Nodes):")
    raw_nodes = splitter.get_nodes_from_documents(documents)
    for i, node in enumerate(raw_nodes, 1):
        print(f"\n   Chunk {i}:")
        if isinstance(splitter, SentenceWindowNodeParser):
            original_text = node.metadata.get("original_text", node.get_content())
            window_context = node.metadata.get("window", "N/A - window context not generated")
            print(f'   Core sentence: "{original_text}"')
            print(f'   Full window context (for LLM): "{window_context}"')
        else:
            print(f'   Content: "{node.get_content()}"')
        # Print metadata for debugging if needed
        # print(f"   Metadata: {node.metadata}")
        print("   " + "-" * 40)
    print("\n" + "=" * 50)

    # --- Build index / query engine / run query (optional) ---
    # print("Building index...")
    # nodes = splitter.get_nodes_from_documents(documents)
    # index = VectorStoreIndex(nodes, embed_model=Settings.embed_model)
    #
    # print("Creating query engine...")
    # query_engine_params = {
    #     "similarity_top_k": 5,
    #     "streaming": True,
    # }
    #
    # # If using Sentence Window splitting, add a postprocessor
    # if isinstance(splitter, SentenceWindowNodeParser):
    #     query_engine_params["node_postprocessors"] = [
    #         MetadataReplacementPostProcessor(target_metadata_key="window")
    #     ]
    #     print("üí° Sentence Window detected ‚Äî added MetadataReplacementPostProcessor.")
    #
    # query_engine = index.as_query_engine(**query_engine_params)
    #
    # # Run query
    # print(f"\nTest question: {question}")
    # print("\nModel answer:")
    # response = query_engine.query(question)
    # response.print_response_stream()
    #
    # # Print retrieved source chunks
    # print(f"\nRetrieved source chunks for {splitter_name}:")
    # if response.source_nodes:
    #     for i, node in enumerate(response.source_nodes, 1):
    #         print(f"\n--- Source chunk {i} ---")
    #
    #         if isinstance(splitter, SentenceWindowNodeParser):
    #             window_content = node.metadata.get("window", "N/A - Window content missing")
    #             original_text = node.metadata.get("original_text", "N/A - Original sentence missing")
    #             print(f"[Retrieved window context]\n{window_content}")
    #             print(f"\n[Core sentence in the window]\n{original_text}")
    #         else:
    #             print(f"[Retrieved content]\n{node.get_content()}")
    #
    #         print("-" * 50)
    # else:
    #     print("No chunks retrieved. Check your document or splitting strategy.")

    print(f"\n{splitter_name} test completed.")
    print(f"{'=' * 50}\n")


# --- Example document and question ---
documents = [
    Document(
        text="""
        LlamaIndex is a data framework for building LLM applications.
        It provides a set of tools that help developers connect private data to large language models (LLMs),
        enabling capabilities such as question answering and Retrieval-Augmented Generation (RAG).
        LlamaIndex supports many data sources, including PDFs, databases, APIs, and more.
        Its core concepts include document loaders, node parsers, indexes, and query engines.

        Document loaders ingest data from various formats and sources into LlamaIndex.
        Node parsers then break the loaded documents into smaller, more manageable units called nodes.
        These nodes are often sentences or paragraphs, depending on the parsing strategy.
        Indexes are data structures built over these nodes to enable efficient storage and retrieval,
        typically involving vector embeddings for semantic search.
        Finally, query engines enable interaction with the indexed data, allowing users to ask questions
        and using the LLM plus retrieved information to synthesize answers.

        --- The following content is less directly related to LlamaIndex ---

        In addition, Python is a general-purpose programming language whose simplicity and rich ecosystem
        make it popular in AI. For example, NumPy and Pandas are foundational for data processing and provide
        powerful tools for numerical operations and structured data. Scikit-learn provides a comprehensive
        suite of machine learning algorithms for classification, regression, clustering, and more.
        Together, these tools form a strong toolbox for data scientists and AI practitioners,
        enabling efficient development and deployment of complex AI models.

        --- Another related but conceptually independent section ---

        Sentence Window chunking is an advanced chunking strategy that includes a target sentence in each chunk,
        and adds a number of surrounding ‚Äúwindow‚Äù sentences as context.
        This provides richer local context during retrieval so the LLM can generate more coherent answers.
        Semantic chunking tries to split text based on semantic meaning rather than fixed character count or
        sentence count. It uses embeddings to measure semantic similarity between sentences or phrases and
        detects natural breakpoints where the topic or meaning shifts.
        Both advanced methods can improve recall and generation quality for RAG applications.
        Choosing the right chunking strategy depends on your data characteristics and expected query types.
        """
    )
]

question = (
    "What are LlamaIndex's main functionalities and core concepts? "
    "Also, what is the difference between the two advanced chunking strategies?"
)

# --- Test different chunking strategies ---

# Token splitting (character/token-based)
token_splitter = TokenTextSplitter(
    chunk_size=30,   # small chunk size to demonstrate forced breaks
    chunk_overlap=0, # no overlap for clear, distinct chunks
)
evaluate_splitter(token_splitter, documents, question, "Token Split (chunk_size=30)")

token_splitter = TokenTextSplitter(
    chunk_size=30,    # small chunk size to demonstrate forced breaks
    chunk_overlap=10, # overlap for shared context across chunks
)
evaluate_splitter(token_splitter, documents, question, "Token Split (chunk_size=30, chunk_overlap=10)")

# Original doc: [ ...X... | Y | ...Z... ]
#                    ‚Üë        ‚Üë
#               end of A   start of B (B starts from 50 chars before end of A)
#
# Chunk A: [ ...X... | Y ] (512 chars)
# Chunk B:            [ Y | ...Z... ] (512 chars)
