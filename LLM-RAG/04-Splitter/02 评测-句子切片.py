import os
from llama_index.core import VectorStoreIndex, Settings, Document
from llama_index.core.node_parser import (
    SentenceWindowNodeParser,
    SemanticSplitterNodeParser,
    TokenTextSplitter,
    SentenceSplitter,
)
from llama_index.core.postprocessor import MetadataReplacementPostProcessor

# Import OpenAILike LLM (DashScope compatible mode for Qwen models)
from llama_index.llms.openai_like import OpenAILike

# Import DashScopeEmbedding (Alibaba Cloud DashScope embedding models)
from llama_index.embeddings.dashscope import DashScopeEmbedding

# Get API key from environment
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY")

# ------------------------------------------------------
# Initialize global LlamaIndex settings
# ------------------------------------------------------

# 1. Configure LLM (use DashScope qwen-plus via OpenAILike)
Settings.llm = OpenAILike(
    model="qwen-plus",
    api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
    api_key=DASHSCOPE_API_KEY,
    is_chat_model=True,
    temperature=0.1,  # Controls randomness of generation
)

# 2. Configure embedding model (DashScope embedding)
Settings.embed_model = DashScopeEmbedding(
    model_name="text-embedding-v4",
    api_key=DASHSCOPE_API_KEY,
)


def evaluate_splitter(splitter, documents, question, splitter_name):
    """
    Evaluate different document chunking strategies.

    This function prints the raw chunks generated by each splitter
    so that different splitting behaviors can be compared directly.
    """
    print(f"\n{'=' * 50}")
    print(f"Testing splitter: {splitter_name}")
    print(f"{'=' * 50}\n")

    # Show raw chunks generated by the splitter
    print(f"[{splitter_name}] Generated document chunks (Nodes):")
    raw_nodes = splitter.get_nodes_from_documents(documents)

    for i, node in enumerate(raw_nodes, 1):
        print(f"\n   Chunk {i}:")
        if isinstance(splitter, SentenceWindowNodeParser):
            # For Sentence Window, show the core sentence and its full window context
            original_text = node.metadata.get("original_text", node.get_content())
            window_context = node.metadata.get("window", "N/A - window context not generated")
            print(f'   Core sentence: "{original_text}"')
            print(f'   Full window context (for LLM): "{window_context}"')
        else:
            print(f'   Content: "{node.get_content()}"')

        # Uncomment for debugging metadata
        # print(f"   Metadata: {node.metadata}")
        print("   " + "-" * 40)

    print("\n" + "=" * 50)

    # --------------------------------------------------
    # Optional: Build index and run retrieval + QA
    # --------------------------------------------------
    # print("Building index...")
    # nodes = splitter.get_nodes_from_documents(documents)
    # index = VectorStoreIndex(nodes, embed_model=Settings.embed_model)
    #
    # print("Creating query engine...")
    # query_engine_params = {
    #     "similarity_top_k": 5,
    #     "streaming": True,
    # }
    #
    # if isinstance(splitter, SentenceWindowNodeParser):
    #     query_engine_params["node_postprocessors"] = [
    #         MetadataReplacementPostProcessor(target_metadata_key="window")
    #     ]
    #     print("üí° Sentence Window detected ‚Äî MetadataReplacementPostProcessor added.")
    #
    # query_engine = index.as_query_engine(**query_engine_params)
    #
    # print(f"\nTest question: {question}")
    # print("\nModel response:")
    # response = query_engine.query(question)
    # response.print_response_stream()
    #
    # print(f"\nRetrieved source chunks ({splitter_name}):")
    # if response.source_nodes:
    #     for i, node in enumerate(response.source_nodes, 1):
    #         print(f"\n--- Source chunk {i} ---")
    #         if isinstance(splitter, SentenceWindowNodeParser):
    #             window_content = node.metadata.get("window", "N/A - window missing")
    #             original_text = node.metadata.get("original_text", "N/A - core sentence missing")
    #             print(f"[Window context]\n{window_content}")
    #             print(f"\n[Core sentence]\n{original_text}")
    #         else:
    #             print(f"[Chunk content]\n{node.get_content()}")
    #         print("-" * 50)
    # else:
    #     print("No chunks retrieved. Check document content or chunking strategy.")

    print(f"\n{splitter_name} test completed.")
    print(f"{'=' * 50}\n")


# ------------------------------------------------------
# Example document and question
# ------------------------------------------------------
documents = [
    Document(
        text="""
        LlamaIndex is a data framework for building LLM applications.
        It provides a suite of tools that help developers connect private data
        with large language models (LLMs), enabling use cases such as
        question answering and Retrieval-Augmented Generation (RAG).
        LlamaIndex supports many data sources, including PDFs, databases, and APIs.
        Its core concepts include document loaders, node parsers, indexes, and query engines.

        Document loaders ingest data from various formats and sources into LlamaIndex.
        Node parsers then break the loaded documents into smaller, more manageable units
        called nodes. These nodes are usually sentences or paragraphs, depending on
        the parsing strategy.
        Indexes are data structures built on top of these nodes to support efficient
        storage and retrieval, typically using vector embeddings for semantic search.
        Finally, query engines enable interaction with indexed data, allowing users
        to ask questions and synthesize answers using both the LLM and retrieved context.

        --- The following content is less directly related to LlamaIndex ---

        Python is a general-purpose programming language widely used in AI due to its
        simplicity and rich ecosystem. For example, NumPy and Pandas are fundamental
        libraries for data processing and numerical computation.
        Scikit-learn provides a comprehensive set of machine learning algorithms for
        classification, regression, and clustering.
        Together, these tools form a powerful toolbox for data scientists and AI engineers.

        --- Another related but conceptually independent section ---

        Sentence Window chunking is an advanced strategy that includes a target sentence
        along with a number of surrounding ‚Äúwindow‚Äù sentences as context.
        This provides richer local context during retrieval, improving answer coherence.
        Semantic chunking attempts to split text based on semantic meaning rather than
        fixed character or sentence counts.
        It uses embeddings to measure semantic similarity between sentences or phrases
        and detects natural breakpoints where the topic shifts.
        Both methods can significantly improve recall and generation quality in RAG systems.
        Choosing the right chunking strategy depends on data characteristics and query intent.
        """
    )
]

question = (
    "What are the main functionalities and core concepts of LlamaIndex, "
    "and what is the difference between the two advanced chunking strategies?"
)

# ------------------------------------------------------
# Run chunking experiments
# ------------------------------------------------------

sentence_splitter = SentenceSplitter(
    chunk_size=512,
    chunk_overlap=50,
)
evaluate_splitter(sentence_splitter, documents, question, "SentenceSplitter")
