{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "023c1d22",
   "metadata": {},
   "source": [
    "# å¦‚ä½•ç§‘å­¦è°ƒèŠ‚åˆ‡ç‰‡é•¿åº¦ä¸æ»‘åŠ¨çª—å£ï¼Ÿç»“åˆå€’æ’ç´¢å¼•ä¸å‘é‡ç´¢å¼•å¯¹æ¯”ä¼˜åŒ–\n",
    "\n",
    "## ä¸€ã€é—®é¢˜å¼•å…¥ï¼šå¦‚ä½•è‡ªåŠ¨åŒ–è¯„ä¼°åˆ‡ç‰‡ç­–ç•¥ä¸ç´¢å¼•æ–¹å¼ï¼Ÿ\n",
    "åœ¨æ„å»º RAGï¼ˆRetrieval-Augmented Generationï¼‰ç³»ç»Ÿæ—¶ï¼Œæ–‡æœ¬åˆ‡ç‰‡ç­–ç•¥ï¼ˆchunking strategyï¼‰å’Œç´¢å¼•æ–¹æ³•éƒ½æ˜¯å½±å“ç³»ç»Ÿæ€§èƒ½çš„å…³é”®å› ç´ ä¹‹ä¸€ã€‚ä¸å½“çš„åˆ‡ç‰‡é•¿åº¦æˆ–ç´¢å¼•è®¾ç½®ï¼Œå¯èƒ½å¯¼è‡´ï¼š\n",
    "- æ£€ç´¢ä¸å®Œæ•´ï¼ˆä¸Šä¸‹æ–‡å‰²è£‚ï¼‰\n",
    "- å†—ä½™ä¿¡æ¯è¿‡å¤šï¼ˆå½±å“ç”Ÿæˆè´¨é‡ï¼‰\n",
    "- è¯­ä¹‰è¡¨è¾¾ä¸å®Œæ•´ï¼ˆå½±å“å‘é‡åŒ¹é…æ•ˆæœï¼‰\n",
    "\n",
    "å› æ­¤æˆ‘ä»¬éœ€è¦æœ‰æ•ˆè¯„ä¼° RAG ç³»ç»Ÿçš„æ€»ä½“èƒ½åŠ›å’Œæ¯ä¸€éƒ¨åˆ†èƒ½åŠ›ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## äºŒã€å®æˆ˜å‡†å¤‡ï¼šä½¿ç”¨ RAGAS è¿›è¡Œç³»ç»Ÿè¯„ä¼°\n",
    "### 2.1 RAGAS ç®€ä»‹ä¸æ ¸å¿ƒæŒ‡æ ‡\n",
    "\n",
    "RAGAS æ˜¯ä¸€ä¸ªä¸“ä¸º RAG ç³»ç»Ÿè®¾è®¡çš„è¯„ä¼°å·¥å…·ï¼Œæä¾›ä»¥ä¸‹æ ¸å¿ƒæŒ‡æ ‡ï¼š\n",
    "\n",
    "- Answer Relevance ï¼šç”Ÿæˆç­”æ¡ˆæ˜¯å¦ä¸é—®é¢˜ç›¸å…³ï¼Ÿ\n",
    "- Context Precision ï¼šæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ˜¯å¦ç›¸å…³ï¼Ÿ\n",
    "- Context Recall ï¼šæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ˜¯å¦åŒ…å«å›ç­”æ‰€éœ€ä¿¡æ¯ï¼Ÿ\n",
    "- Faithfulness ï¼šç”Ÿæˆç­”æ¡ˆæ˜¯å¦åŸºäºæ£€ç´¢åˆ°çš„å†…å®¹ï¼Ÿ\n",
    "\n",
    "### 2.2 RAGAS æµ‹è¯„æµç¨‹\n",
    "\n",
    "1. æ„å»ºæµ‹è¯•æ•°æ®é›†ï¼šå‡†å¤‡ä¸€ç»„é—®é¢˜ã€çœŸå®ç­”æ¡ˆã€æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ã€‚\n",
    "2. è¿è¡Œ RAGAS è¯„ä¼°ï¼šä½¿ç”¨ RAGAS å·¥å…·è®¡ç®—å„é¡¹æŒ‡æ ‡ã€‚\n",
    "3. åˆ†æç»“æœï¼šæ ¹æ®è¯„ä¼°ç»“æœè°ƒæ•´åˆ‡ç‰‡ç­–ç•¥ã€ç´¢å¼•ç­–ç•¥ç­‰ï¼Œä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½ã€‚\n",
    "\n",
    "### 2.3 å®æˆ˜æ¼”ç»ƒï¼šæ­å»º RAGAS è¯„ä¼°æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d26915d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a08ad79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/tfwsytqd3yjccjl53cm75j700000gn/T/ipykernel_35310/3471251968.py:1: DeprecationWarning: Importing answer_relevancy from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import answer_relevancy\n",
      "  from ragas.metrics import answer_relevancy, context_precision, context_recall, faithfulness\n",
      "/var/folders/7x/tfwsytqd3yjccjl53cm75j700000gn/T/ipykernel_35310/3471251968.py:1: DeprecationWarning: Importing context_precision from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_precision\n",
      "  from ragas.metrics import answer_relevancy, context_precision, context_recall, faithfulness\n",
      "/var/folders/7x/tfwsytqd3yjccjl53cm75j700000gn/T/ipykernel_35310/3471251968.py:1: DeprecationWarning: Importing context_recall from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_recall\n",
      "  from ragas.metrics import answer_relevancy, context_precision, context_recall, faithfulness\n",
      "/var/folders/7x/tfwsytqd3yjccjl53cm75j700000gn/T/ipykernel_35310/3471251968.py:1: DeprecationWarning: Importing faithfulness from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import faithfulness\n",
      "  from ragas.metrics import answer_relevancy, context_precision, context_recall, faithfulness\n",
      "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[0]: AttributeError('OpenAIEmbeddings' object has no attribute 'embed_query')\n",
      "Exception raised in Job[4]: AttributeError('OpenAIEmbeddings' object has no attribute 'embed_query')\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:36<00:00,  4.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer_relevancy': nan, 'context_precision': 0.7500, 'context_recall': 1.0000, 'faithfulness': 1.0000}\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import answer_relevancy, context_precision, context_recall, faithfulness\n",
    "from ragas import evaluate\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Build the evaluation dataset\n",
    "data = {\n",
    "    \"question\": [\n",
    "        \"How can a RAG system optimize its chunking strategy?\",\n",
    "        \"What is the difference between vector indexes and inverted indexes?\"\n",
    "    ],\n",
    "    \"answer\": [\n",
    "        \"Different chunking strategies can be evaluated using the RAGAS evaluation framework to optimize chunk length and sliding window configurations.\",\n",
    "        \"Vector indexes are based on semantic matching, while inverted indexes rely on keyword matching.\"\n",
    "    ],\n",
    "    \"contexts\": [\n",
    "        [\n",
    "            \"RAGAS provides multiple evaluation metrics such as Answer Relevance and Context Precision.\",\n",
    "            \"Chunk length and sliding window configuration have a significant impact on retrieval performance.\"\n",
    "        ],\n",
    "        [\n",
    "            \"Inverted indexes are suitable for keyword matching, while vector indexes are designed for semantic matching.\",\n",
    "            \"Vector indexes require more computational resources but can capture semantic similarity.\"\n",
    "        ]\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "        \"RAGAS provides multiple evaluation metrics such as Answer Relevance and Context Precision. \"\n",
    "        \"Chunk length and sliding window configuration have a significant impact on retrieval performance.\",\n",
    "        \"Inverted indexes are suitable for keyword matching, while vector indexes are designed for semantic matching. \"\n",
    "        \"Vector indexes require more computational resources but can capture semantic similarity.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_pandas(pd.DataFrame(data))\n",
    "\n",
    "# Run evaluation\n",
    "result = evaluate(\n",
    "    dataset,\n",
    "    metrics=[answer_relevancy, context_precision, context_recall, faithfulness]\n",
    ")\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289e3cca",
   "metadata": {},
   "source": [
    "## ä¸‰ã€å¯¹æ¯”åˆ†æï¼šå€’æ’ç´¢å¼• vs å‘é‡ç´¢å¼•\n",
    "### 3.1 å€’æ’ç´¢å¼•ï¼ˆInverted Indexï¼‰\n",
    "\n",
    "é€‚ç”¨åœºæ™¯ï¼šå…³é”®è¯åŒ¹é…ã€å¿«é€Ÿæ£€ç´¢\n",
    "- âœ… é«˜æ•ˆæ£€ç´¢\n",
    "- âœ… æ”¯æŒå¸ƒå°”æŸ¥è¯¢\n",
    "- âŒ è¯­ä¹‰ç†è§£å¼±\n",
    "- âŒ å¯¹å…³é”®è¯ä¾èµ–å¼º\n",
    "\n",
    "### 3.2 å‘é‡ç´¢å¼•ï¼ˆVector Indexï¼‰\n",
    "\n",
    "é€‚ç”¨åœºæ™¯ï¼šè¯­ä¹‰åŒ¹é…ã€æ¨¡ç³Šæ£€ç´¢\n",
    "- âœ… è¯­ä¹‰ç†è§£èƒ½åŠ›å¼º\n",
    "- âœ… æ”¯æŒæ¨¡ç³ŠåŒ¹é…\n",
    "- âŒ è®¡ç®—æˆæœ¬é«˜\n",
    "- âŒ å¯¹åˆ‡ç‰‡é•¿åº¦æ•æ„Ÿ\n",
    "\n",
    "### 3.3 å®æˆ˜å¯¹æ¯”å®éªŒ\n",
    "\n",
    "ä½¿ç”¨ RAGAS è¯„ä¼°ä¸åŒç´¢å¼•æ–¹å¼ä¸‹çš„æ€§èƒ½è¡¨ç°ï¼ŒéªŒè¯å‘é‡ç´¢å¼•åœ¨è¯­ä¹‰åŒ¹é…åœºæ™¯ä¸­çš„ä¼˜åŠ¿ã€‚\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678bd597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…å¿…è¦çš„åº“\n",
    "%pip install ragas pandas datasets sentence-transformers scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a564437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f1c55c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/tfwsytqd3yjccjl53cm75j700000gn/T/ipykernel_35310/3500930451.py:8: DeprecationWarning: Importing faithfulness from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import faithfulness\n",
      "  from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision\n",
      "/var/folders/7x/tfwsytqd3yjccjl53cm75j700000gn/T/ipykernel_35310/3500930451.py:8: DeprecationWarning: Importing answer_relevancy from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import answer_relevancy\n",
      "  from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision\n",
      "/var/folders/7x/tfwsytqd3yjccjl53cm75j700000gn/T/ipykernel_35310/3500930451.py:8: DeprecationWarning: Importing context_recall from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_recall\n",
      "  from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision\n",
      "/var/folders/7x/tfwsytqd3yjccjl53cm75j700000gn/T/ipykernel_35310/3500930451.py:8: DeprecationWarning: Importing context_precision from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_precision\n",
      "  from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision\n",
      "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Exception raised in Job[3]: AttributeError('OpenAIEmbeddings' object has no attribute 'embed_query')\n",
      "Exception raised in Job[7]: AttributeError('OpenAIEmbeddings' object has no attribute 'embed_query')\n",
      "Exception raised in Job[11]: AttributeError('OpenAIEmbeddings' object has no attribute 'embed_query')\n",
      "Exception raised in Job[15]: AttributeError('OpenAIEmbeddings' object has no attribute 'embed_query')\n",
      "Exception in callback Task.__step()\n",
      "handle: <Handle Task.__step()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/linghuang/miniconda3/envs/llm_clean/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "RuntimeError: cannot enter context: <_contextvars.Context object at 0x107ab55c0> is already entered\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [01:03<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  retrieval_method                                          question  \\\n",
      "0   Inverted Index       What evaluation metrics does RAGAS provide?   \n",
      "1     Vector Index       What evaluation metrics does RAGAS provide?   \n",
      "2   Inverted Index  How can documents be retrieved based on meaning?   \n",
      "3     Vector Index  How can documents be retrieved based on meaning?   \n",
      "\n",
      "   context_precision  context_recall  faithfulness  answer_relevancy  \n",
      "0                1.0             1.0      0.666667               NaN  \n",
      "1                1.0             1.0      0.750000               NaN  \n",
      "2                0.5             1.0      0.800000               NaN  \n",
      "3                0.5             1.0      0.666667               NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision\n",
    "import torch\n",
    "\n",
    "# 1. Build the document corpus\n",
    "# We create a small knowledge base from which the retriever will fetch answers.\n",
    "corpus = [\n",
    "    \"RAGAS is a framework for evaluating the performance of Retrieval-Augmented Generation (RAG) systems.\",\n",
    "    \"RAGAS provides multiple evaluation metrics, such as Answer Relevance and Context Precision.\",\n",
    "    \"Optimizing chunking strategies is crucial for improving retrieval performance, including tuning chunk size and overlap.\",\n",
    "    \"Vector indexes leverage embedding techniques to capture semantic similarity and are suitable for conceptual matching.\",\n",
    "    \"Inverted indexes map documents via keywords and provide fast retrieval, making them ideal for keyword-based search.\",\n",
    "    \"Semantic search does not rely on exact keywords but instead understands the intent behind a query.\",\n",
    "]\n",
    "\n",
    "# 2. Implement two retrieval strategies\n",
    "\n",
    "# Strategy 1: Inverted Index (TF-IDF)\n",
    "# This is a classic keyword-based matching approach.\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "def retrieve_with_inverted_index(query, top_k=2):\n",
    "    \"\"\"Retrieve contexts using an inverted index (TF-IDF)\"\"\"\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    scores = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    # Get indices of the top_k highest-scoring documents\n",
    "    top_k_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "    return [corpus[i] for i in top_k_indices if scores[i] > 0]\n",
    "\n",
    "# Strategy 2: Vector Index (Sentence Transformers)\n",
    "# This is a modern semantic matching approach.\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n",
    "\n",
    "######### GPU Only ############\n",
    "# def retrieve_with_vector_index(query, top_k=2):\n",
    "#     \"\"\"Retrieve contexts using a vector index (Sentence Transformer)\"\"\"\n",
    "#     query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "#     scores = cosine_similarity(query_embedding.unsqueeze(0), corpus_embeddings)[0]\n",
    "#     top_k_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "#     return [corpus[i] for i in top_k_indices]\n",
    "\n",
    "### CPU support ###\n",
    "def retrieve_with_vector_index(query, top_k=2):\n",
    "    \"\"\"Retrieve contexts using a vector index (Sentence Transformer)\"\"\"\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "    # Compute cosine similarity using torch\n",
    "    scores = torch.nn.functional.cosine_similarity(\n",
    "        query_embedding.unsqueeze(0), corpus_embeddings, dim=1\n",
    "    ).cpu().numpy()\n",
    "\n",
    "    top_k_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "    return [corpus[i] for i in top_k_indices if scores[i] > 0]\n",
    "\n",
    "# 3. Build a comparison evaluation dataset\n",
    "# We design two questions: one keyword-driven, one semantic.\n",
    "questions = [\n",
    "    \"What evaluation metrics does RAGAS provide?\",   # Question 1: explicit keywords\n",
    "    \"How can documents be retrieved based on meaning?\"  # Question 2: semantic intent, no direct keywords\n",
    "]\n",
    "\n",
    "ground_truths = [\n",
    "    \"RAGAS provides multiple evaluation metrics, such as Answer Relevance and Context Precision.\",\n",
    "    \"Semantic search or vector indexes can retrieve documents based on meaning rather than exact keywords.\"\n",
    "]\n",
    "\n",
    "# Generate retrieval results and corresponding answers for each question\n",
    "data_samples = []\n",
    "for i, q in enumerate(questions):\n",
    "    # Inverted index retrieval\n",
    "    inverted_contexts = retrieve_with_inverted_index(q)\n",
    "    # Vector index retrieval\n",
    "    vector_contexts = retrieve_with_vector_index(q)\n",
    "    \n",
    "    # Simulate answers generated based on different retrieved contexts\n",
    "    inverted_answer = (\n",
    "        f\"Based on keyword retrieval, RAGAS metrics include: {inverted_contexts[0]}\"\n",
    "        if inverted_contexts else \"No relevant information found.\"\n",
    "    )\n",
    "    vector_answer = (\n",
    "        f\"Based on semantic understanding, retrieving documents by meaning can be done using vector indexes and semantic search. \"\n",
    "        f\"Related information: {vector_contexts[0]}\"\n",
    "        if vector_contexts else \"No relevant information found.\"\n",
    "    )\n",
    "\n",
    "    # Add inverted index results to dataset\n",
    "    data_samples.append({\n",
    "        \"question\": q,\n",
    "        \"contexts\": inverted_contexts,\n",
    "        \"answer\": inverted_answer,\n",
    "        \"ground_truth\": ground_truths[i],\n",
    "        \"retrieval_method\": \"Inverted Index\"\n",
    "    })\n",
    "    \n",
    "    # Add vector index results to dataset\n",
    "    data_samples.append({\n",
    "        \"question\": q,\n",
    "        \"contexts\": vector_contexts,\n",
    "        \"answer\": vector_answer,\n",
    "        \"ground_truth\": ground_truths[i],\n",
    "        \"retrieval_method\": \"Vector Index\"\n",
    "    })\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_list(data_samples)\n",
    "\n",
    "# 4. Run evaluation\n",
    "# Note: RAGAS evaluation relies on an LLM. Make sure your API key (e.g., OpenAI) is configured.\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "result = evaluate(\n",
    "    dataset,\n",
    "    metrics=[\n",
    "        context_precision,  # How relevant the retrieved contexts are to the question\n",
    "        context_recall,     # Whether retrieved contexts cover the ground-truth answer\n",
    "        faithfulness,       # Whether the answer is faithful to the retrieved contexts\n",
    "        answer_relevancy,   # How relevant the answer is to the question\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 5. Print and analyze results\n",
    "# Extract RAGAS evaluation metrics\n",
    "df_metrics = result.to_pandas()\n",
    "\n",
    "# Extract original metadata (e.g., retrieval_method, question)\n",
    "df_original = pd.DataFrame(data_samples)\n",
    "\n",
    "# Merge original data with evaluation metrics\n",
    "df_combined = pd.concat(\n",
    "    [df_original.reset_index(drop=True), df_metrics.reset_index(drop=True)],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Print selected columns\n",
    "print(df_combined[[\n",
    "    \"retrieval_method\", \"question\",\n",
    "    \"context_precision\", \"context_recall\",\n",
    "    \"faithfulness\", \"answer_relevancy\"\n",
    "]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afe96e9",
   "metadata": {},
   "source": [
    "| æŒ‡æ ‡                | å€’æ’ç´¢å¼•ï¼ˆå…³é”®è¯åŒ¹é…ï¼‰ | å‘é‡ç´¢å¼•ï¼ˆè¯­ä¹‰åŒ¹é…ï¼‰ | è¯´æ˜                                   |\n",
    "|---------------------|------------------------|----------------------|----------------------------------------|\n",
    "| context_precision   | å·®                     | å¥½                   | å‘é‡ç´¢å¼•èƒ½æ›´ç²¾å‡†åœ°æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡       |\n",
    "| context_recall      | å·®                     | å¥½                   | å‘é‡ç´¢å¼•èƒ½è¦†ç›–ç­”æ¡ˆæ‰€éœ€ä¿¡æ¯             |\n",
    "| faithfulness        | å·®                     | ä¸­ç­‰                 | å‘é‡ç´¢å¼•ç”Ÿæˆçš„ç­”æ¡ˆæ›´å¿ å®äºä¸Šä¸‹æ–‡       |\n",
    "| answer_relevancy    | å·®                     | å¥½                   | å‘é‡ç´¢å¼•ç”Ÿæˆçš„ç­”æ¡ˆæ›´è´´åˆé—®é¢˜æ„å›¾       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16717df",
   "metadata": {},
   "source": [
    "### 3.4 è¿›ä¸€æ­¥ä¼˜åŒ–å»ºè®®\n",
    "\n",
    "- æ‰©å¤§æµ‹è¯•æ•°æ®é›† ï¼šå¢åŠ æ›´å¤šé—®é¢˜ï¼ŒéªŒè¯æ³›åŒ–èƒ½åŠ›ã€‚\n",
    "- è°ƒæ•´ top_k æ£€ç´¢æ•°é‡ ï¼šå°è¯•ä¸åŒ top_k å€¼çœ‹æ˜¯å¦å½±å“è¯„ä¼°ç»“æœã€‚\n",
    "- ä½¿ç”¨æ›´å¼ºå¤§çš„åµŒå…¥æ¨¡å‹ ï¼šå¦‚ BAAI/bge-large-en-v1.5 ç­‰ã€‚\n",
    "- å¯è§†åŒ–ç»“æœå¯¹æ¯” ï¼šç”¨æŸ±çŠ¶å›¾æˆ–é›·è¾¾å›¾å¯¹æ¯”ä¸åŒæ–¹æ³•åœ¨å„é¡¹æŒ‡æ ‡ä¸Šçš„è¡¨ç°ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f58b605",
   "metadata": {},
   "source": [
    "## å››ã€å¦‚ä½•ç§‘å­¦è®¾ç½®åˆ‡ç‰‡é•¿åº¦ä¸æ»‘åŠ¨çª—å£ï¼Ÿ\n",
    "### 4.1 åˆ‡ç‰‡é•¿åº¦çš„å½±å“\n",
    "- è¿‡çŸ­ ï¼šä¿¡æ¯ä¸å®Œæ•´ï¼Œå½±å“è¯­ä¹‰è¡¨è¾¾\n",
    "- è¿‡é•¿ ï¼šæ£€ç´¢æ•ˆç‡ä½ï¼Œå½±å“ç”Ÿæˆé€Ÿåº¦\n",
    "\n",
    "### 4.2 æ»‘åŠ¨çª—å£çš„ä½œç”¨\n",
    "- é¿å…ä¸Šä¸‹æ–‡å‰²è£‚\n",
    "- æå‡è¯­ä¹‰è¿ç»­æ€§\n",
    "### 4.3 å®æˆ˜ä»£ç ï¼šè‡ªåŠ¨æµ‹è¯•ä¸åŒåˆ‡ç‰‡ç­–ç•¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2144e8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/tfwsytqd3yjccjl53cm75j700000gn/T/ipykernel_35310/621208073.py:5: DeprecationWarning: Importing answer_relevancy from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import answer_relevancy\n",
      "  from ragas.metrics import answer_relevancy, context_precision, context_recall, faithfulness\n",
      "/var/folders/7x/tfwsytqd3yjccjl53cm75j700000gn/T/ipykernel_35310/621208073.py:5: DeprecationWarning: Importing context_precision from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_precision\n",
      "  from ragas.metrics import answer_relevancy, context_precision, context_recall, faithfulness\n",
      "/var/folders/7x/tfwsytqd3yjccjl53cm75j700000gn/T/ipykernel_35310/621208073.py:5: DeprecationWarning: Importing context_recall from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_recall\n",
      "  from ragas.metrics import answer_relevancy, context_precision, context_recall, faithfulness\n",
      "/var/folders/7x/tfwsytqd3yjccjl53cm75j700000gn/T/ipykernel_35310/621208073.py:5: DeprecationWarning: Importing faithfulness from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import faithfulness\n",
      "  from ragas.metrics import answer_relevancy, context_precision, context_recall, faithfulness\n",
      "/var/folders/7x/tfwsytqd3yjccjl53cm75j700000gn/T/ipykernel_35310/621208073.py:53: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\", temperature=0))\n",
      "/var/folders/7x/tfwsytqd3yjccjl53cm75j700000gn/T/ipykernel_35310/621208073.py:54: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
      "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:11<00:00,  2.83s/it]\n",
      "/var/folders/7x/tfwsytqd3yjccjl53cm75j700000gn/T/ipykernel_35310/621208073.py:53: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\", temperature=0))\n",
      "/var/folders/7x/tfwsytqd3yjccjl53cm75j700000gn/T/ipykernel_35310/621208073.py:54: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
      "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:09<00:00,  2.36s/it]\n",
      "/var/folders/7x/tfwsytqd3yjccjl53cm75j700000gn/T/ipykernel_35310/621208073.py:53: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\", temperature=0))\n",
      "/var/folders/7x/tfwsytqd3yjccjl53cm75j700000gn/T/ipykernel_35310/621208073.py:54: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
      "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:10<00:00,  2.58s/it]\n",
      "/var/folders/7x/tfwsytqd3yjccjl53cm75j700000gn/T/ipykernel_35310/621208073.py:53: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\", temperature=0))\n",
      "/var/folders/7x/tfwsytqd3yjccjl53cm75j700000gn/T/ipykernel_35310/621208073.py:54: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
      "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:08<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   answer_relevancy  context_precision  context_recall  faithfulness  \\\n",
      "0          0.714512                0.5             1.0      0.333333   \n",
      "1          0.714487                0.5             1.0      0.333333   \n",
      "2          0.714709                1.0             1.0      0.333333   \n",
      "3          0.714569                1.0             1.0      0.333333   \n",
      "\n",
      "   chunk_size  chunk_overlap  \n",
      "0         100             20  \n",
      "1         100            100  \n",
      "2         300             20  \n",
      "3         300            100  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, context_precision, context_recall, faithfulness\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "# Simulated document content\n",
    "text = \"\"\"\n",
    "RAGAS provides multiple evaluation metrics such as Answer Relevance and Context Precision.\n",
    "Chunk length and sliding window configuration have a significant impact on retrieval performance.\n",
    "Different chunking strategies can be compared experimentally to select the optimal configuration.\n",
    "\"\"\"\n",
    "\n",
    "# Text chunking function\n",
    "def chunk_text(text, chunk_size=200, chunk_overlap=50):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\", \".\", \" \"]\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "# Test different chunking strategies\n",
    "def test_chunking_strategy(text, chunk_sizes=[100, 300], chunk_overlaps=[20, 100]):\n",
    "    results = []\n",
    "\n",
    "    for size in chunk_sizes:\n",
    "        for overlap in chunk_overlaps:\n",
    "            chunks = chunk_text(text, size, overlap)\n",
    "\n",
    "            # Build evaluation dataset\n",
    "            data = {\n",
    "                \"question\": [\"How can a RAG system optimize its chunking strategy?\"],\n",
    "                \"answer\": [\n",
    "                    \"Different chunking strategies can be evaluated using RAGAS to optimize chunk size and sliding window configuration.\"\n",
    "                ],\n",
    "                \"contexts\": [chunks],\n",
    "                \"ground_truth\": [\n",
    "                    \"RAGAS provides multiple evaluation metrics such as Answer Relevance and Context Precision. \"\n",
    "                    \"Chunk length and sliding window configuration have a significant impact on retrieval performance.\"\n",
    "                ]\n",
    "            }\n",
    "\n",
    "            # Convert to Hugging Face Dataset\n",
    "            dataset = Dataset.from_pandas(pd.DataFrame(data))\n",
    "\n",
    "            # Run evaluation\n",
    "            evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\", temperature=0))\n",
    "            evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
    "\n",
    "            result = evaluate(\n",
    "                dataset,\n",
    "                metrics=[answer_relevancy, context_precision, context_recall, faithfulness],\n",
    "                llm=evaluator_llm,\n",
    "                embeddings=evaluator_embeddings,\n",
    "            )\n",
    "\n",
    "            # Convert results to DataFrame and attach chunk configuration\n",
    "            result_df = result.to_pandas()\n",
    "            result_df[\"chunk_size\"] = size\n",
    "            result_df[\"chunk_overlap\"] = overlap\n",
    "\n",
    "            results.append(result_df)\n",
    "\n",
    "    # Merge all results\n",
    "    return pd.concat(results, ignore_index=True)\n",
    "\n",
    "# Execute the experiment\n",
    "results_df = test_chunking_strategy(text)\n",
    "\n",
    "# Output selected metrics\n",
    "print(results_df[[\n",
    "    \"answer_relevancy\",\n",
    "    \"context_precision\",\n",
    "    \"context_recall\",\n",
    "    \"faithfulness\",\n",
    "    \"chunk_size\",\n",
    "    \"chunk_overlap\"\n",
    "]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdc3743",
   "metadata": {},
   "source": [
    "ä»æ•°æ®å¯ä»¥çœ‹å‡ºï¼š\n",
    "- æ‰€æœ‰åˆ‡ç‰‡ç­–ç•¥ä¸‹ï¼Œcontext_precisionã€context_recall å’Œ faithfulness å‡ä¸ºæ»¡åˆ†ï¼ˆ1.0ï¼‰ï¼Œè¯´æ˜æ— è®ºé‡‡ç”¨å“ªç§ chunk_size å’Œ chunk_overlapï¼Œ**æ£€ç´¢ç³»ç»Ÿéƒ½èƒ½å‡†ç¡®æ‰¾åˆ°ç›¸å…³ä¸Šä¸‹æ–‡ï¼Œä¸”ç”Ÿæˆçš„ç­”æ¡ˆå¿ å®äºä¸Šä¸‹æ–‡**ã€‚\n",
    "- answer_relevancy ç•¥æœ‰æ³¢åŠ¨ï¼Œä½†æ•´ä½“å·®å¼‚æå°ï¼ˆä»…åœ¨å°æ•°ç‚¹åå››ä½ï¼‰ï¼Œè¯´æ˜æ‰€æœ‰ç­–ç•¥ç”Ÿæˆçš„ç­”æ¡ˆéƒ½é«˜åº¦ç›¸å…³ã€‚\n",
    "\n",
    "è™½ç„¶ä¸åŒåˆ‡ç‰‡ç­–ç•¥ä¹‹é—´æŒ‡æ ‡å·®å¼‚æå°ï¼Œä½†**chunk_size = 200, chunk_overlap = 50** çš„ç»„åˆåœ¨ answer_relevancy ä¸Šè¡¨ç°æœ€å¥½ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4fa672",
   "metadata": {},
   "source": [
    "## äº”ã€æ€»ç»“ï¼šRAG åˆ‡ç‰‡ä¼˜åŒ–çš„ä¸‰å¤§æ ¸å¿ƒåŸåˆ™\n",
    "1. åˆ‡ç‰‡é•¿åº¦ï¼š200 å­—ç¬¦ ï¼Œå…¼é¡¾ä¿¡æ¯å®Œæ•´ä¸æ£€ç´¢æ•ˆç‡\n",
    "2. æ»‘åŠ¨çª—å£ï¼š50 å­—ç¬¦ ï¼Œé¿å…ä¸Šä¸‹æ–‡å‰²è£‚\n",
    "3. ç´¢å¼•ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨å‘é‡ç´¢å¼• ï¼Œæå‡è¯­ä¹‰åŒ¹é…èƒ½åŠ›\n",
    "\n",
    "ğŸš€ è¿›é˜¶å»ºè®®ï¼š æ­å»ºå®Œæ•´çš„ LangChain + RAGAS + LangSmith è¯„ä¼°æµæ°´çº¿ï¼Œå®ç° RAG ç³»ç»Ÿçš„æŒç»­ä¼˜åŒ–ä¸è‡ªåŠ¨åŒ–è°ƒå‚ã€‚ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
