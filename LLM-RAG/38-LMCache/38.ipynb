{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d6ff39a",
   "metadata": {},
   "source": [
    "## 高频请求扛不住？多级缓存架构+Redis集群设计保障性能\n",
    "\n",
    "在高并发场景下，首Token延迟（TTFT） 常成为大模型服务的性能瓶颈。\n",
    "\n",
    "核心问题在于：每次推理都需从头计算 Key-Value 缓存（KV Cache），导致 GPU 显存和计算资源被大量重复消耗。\n",
    "\n",
    "LMCache 通过创新的 KV Cache 复用机制，实现 TTFT 大幅降低，显著优化了用户体验与系统效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe0364e",
   "metadata": {},
   "source": [
    "## 1. LMCache是什么？它解决了哪些痛点？\n",
    "\n",
    "以 vLLM 为代表的主流推理引擎，在实际应用中暴露出以下几个关键问题：\n",
    "\n",
    "1. 重复计算严重\n",
    "2. 显存开销巨大\n",
    "3. 长文本响应慢\n",
    "4. 资源利用率低下\n",
    "\n",
    "## 2. LMCache 的三大核心技术突破\n",
    "\n",
    "LMCache 创新性地将 KV Cache 的生成与使用解耦，构建了一套高效的多级缓存体系，从根本上提升推理效率。\n",
    "\n",
    "1. 多级存储架构：按需缓存，灵活下沉\n",
    "采用 GPU 显存 → CPU 内存 → 本地磁盘 的三级缓存结构\n",
    "\n",
    "2. 跨请求 KV 复用：不止于 Prefix Caching\n",
    "传统方案仅支持相同前缀复用（Prefix Caching），而 LMCache 实现了更细粒度的 任意位置 KV 片段提取与重用。\n",
    "\n",
    "3. 分布式共享缓存池：集群协同，避免重复劳动\n",
    "多个 vLLM 推理实例可接入统一的分布式缓存池（基于 Redis 集群构建）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea38f466",
   "metadata": {},
   "source": [
    "## 3. LMCache 工作原理：基于场景分层的三级缓存体系\n",
    "\n",
    "LMCache 的核心设计理念是 “按访问频率分级存储，按延迟需求就近读取”。\n",
    "\n",
    "通过构建 L1–L3 三级缓存架构，实现从纳秒到毫秒级响应的无缝衔接，在保证性能的同时支持大规模扩展。\n",
    "\n",
    "**L1 缓存：Worker 内显存级缓存（纳秒级响应）**\n",
    "\n",
    "定位：极致低延迟，零锁竞争，服务高频局部请求\n",
    "\n",
    "L1 缓存部署在每个推理 Worker 节点内部，采用高效的 LRU（Least Recently Used）内存缓存机制，直接驻留在 GPU 显存中，专用于存储当前会话中最活跃的 KV Cache 片段。\n",
    "\n",
    "工作流程如下：\n",
    "1. 当 vLLM 生成新 Token 时，StorageManager 自动提取当前输入序列的语义指纹（如 SHA-256 哈希值）；\n",
    "2. 在本地 L1 缓存中查找匹配项；\n",
    "3. 若命中成功，则将预计算的 KV Cache 直接注入 Attention 层，跳过全部前缀计算；\n",
    "4. 若未命中，进入下一级缓存查询。\n",
    "\n",
    "此外，系统会对常用提示词（如 system prompt）、高频对话模板等标记为 hot_cache，常驻 L1 缓存，确保“热数据永不落盘”。\n",
    "\n",
    "**L2 缓存：节点级共享存储（微秒级响应）**\n",
    "\n",
    "定位：跨 Worker 共享 + 避免重复计算，减少网络开销\n",
    "\n",
    "L2 缓存位于单个物理节点的本地高速存储设备上（如 NVMe SSD），作为该节点所有 Worker 实例共享的二级缓存池。\n",
    "\n",
    "L2 的关键价值在于：\n",
    "支持同一节点内多个 Worker 实例间的缓存共享；\n",
    "显著减少长上下文重复计算带来的冷启动开销；\n",
    "利用 SSD 大容量特性，可缓存数百万级 KV 片段。\n",
    "\n",
    "**L3 缓存：分布式远程缓存池（毫秒级响应）**\n",
    "\n",
    "定位：跨节点协同 + PB 级扩展能力，支撑集群级复用\n",
    "L3 是整个架构中的顶层缓存层，基于高性能分布式键值存储构建，支持 Redis Cluster ，构成了全局共享的 KV Cache 中心仓库。\n",
    "\n",
    "当 L1/L2 均未命中时，触发 L3 查询\n",
    "\n",
    "L3 的意义在于打破“孤岛效应”：\n",
    "- 不同用户、不同会话间的相似内容可被统一识别与复用；\n",
    "- 新上线节点可快速借助已有缓存加速冷启动；\n",
    "- 支持弹性扩容，缓存容量随集群规模线性增长。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3fdbf0",
   "metadata": {},
   "source": [
    "## 4. 10分钟搞定生产级部署\n",
    "\n",
    "**环境准备**\n",
    "\n",
    "```bash\n",
    "Ubuntu 22.04\n",
    "\n",
    "# 基础依赖\n",
    "pip install torch==2.1.2+cu118 --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "conda install -y cuda-toolkit=11.8\n",
    "\n",
    "# 安装LMCache（注意必须从源码构建）\n",
    "git clone https://github.com/LMCache/LMCache.git\n",
    "cd LMCache\n",
    "pip install -e . --no-cache-dir  # 避免依赖冲突\n",
    "\n",
    "# 验证安装\n",
    "python -c \"from lmcache import CacheEngine; print('LMCache 加载成功')\"\n",
    "```\n",
    "\n",
    "**LMCache配置文件**\n",
    "\n",
    "创建lmcache_config.yaml：\n",
    "\n",
    "```yaml\n",
    "# LMCache核心配置\n",
    "cache_config:\n",
    "  # GPU显存缓存配置\n",
    "  max_gpu_cache_ratio: 0.3  # GPU显存最大占用30%\n",
    "  hot_cache_ttl: 3600       # 热数据保留1小时\n",
    "  \n",
    "  # 本地磁盘缓存配置\n",
    "  local_cache_dir: \"/lmcache\"  # 使用高速SSD\n",
    "  max_local_cache_size: 50          # 本地缓存上限50GB\n",
    "  \n",
    "  # 分布式缓存配置\n",
    "  remote_url: \"redis://10.0.0.1:6379\"\n",
    "  remote_ttl: 604800  # 远程缓存保留7天\n",
    "\n",
    "# 性能优化配置\n",
    "performance:\n",
    "  prefetch_enabled: true    # 启用预取\n",
    "  compression_enabled: true # 启用压缩\n",
    "  batch_size: 32           # 批处理大小\n",
    "```\n",
    "\n",
    "\n",
    "**与vLLM集成**\n",
    "\n",
    "在启动vLLM时注入KV Connector：\n",
    "\n",
    "```bash\n",
    "export LMCACHE_REMOTE_URL=\"redis://10.0.0.1:6379\"  # 指向Redis集群\n",
    "export LMCACHE_LOCAL_DISK_SIZE=128  # 本地磁盘缓存上限128GB\n",
    "\n",
    "# 启动vLLM并挂载LMCache\n",
    "python -m vllm.entrypoints.api_server \\\n",
    "    --model Qwen/Qwen3-8B \\\n",
    "    --kv-transfer-config '{\"kv_connector\": \"lmcacheconnector\"}' \\\n",
    "    --max-model-len 128000  # 支持长上下文\n",
    "```\n",
    "\n",
    "**Redis集群配置**\n",
    "\n",
    "LMCache需要Redis作为分布式存储后端：\n",
    "\n",
    "\n",
    "```bash\n",
    "# 启动Redis集群（3主3从配置）\n",
    "docker run -d --name redis-node-1 -p 7001:7001 \\\n",
    "  redis:7.0-alpine redis-server --port 7001 --cluster-enabled yes \\\n",
    "  --cluster-config-file nodes.conf --cluster-node-timeout 5000\n",
    "\n",
    "docker run -d --name redis-node-2 -p 7002:7002 \\\n",
    "  redis:7.0-alpine redis-server --port 7002 --cluster-enabled yes \\\n",
    "  --cluster-config-file nodes.conf --cluster-node-timeout 5000\n",
    "\n",
    "docker run -d --name redis-node-3 -p 7003:7003 \\\n",
    "  redis:7.0-alpine redis-server --port 7003 --cluster-enabled yes \\\n",
    "  --cluster-config-file nodes.conf --cluster-node-timeout 5000\n",
    "\n",
    "# 初始化集群\n",
    "redis-cli --cluster create 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 \\\n",
    "  --cluster-replicas 0 --cluster-yes\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bc89c8",
   "metadata": {},
   "source": [
    "## 5. 常见问题\n",
    "\n",
    "\n",
    "### 1. 显存爆炸：别让缓存吃掉你的 GPU 资源\n",
    "\n",
    "问题现象：\n",
    "\n",
    "开启缓存后，GPU 显存迅速打满，导致新请求 OOM（Out of Memory），推理任务失败。\n",
    "\n",
    "根本原因：\n",
    "\n",
    "默认配置下，LMCache 会尽可能多地缓存 KV 数据到 GPU 显存。对于大模型（如 70B）或高并发场景，极易超出显存容量。\n",
    "\n",
    "解决方案：\n",
    "\n",
    "在 lmcache_config.yaml 中严格限制 GPU 缓存比例，并设置热数据存活时间：\n",
    "\n",
    "max_gpu_cache_ratio: 0.3    # 控制 GPU 缓存不超过显存总容量的 30%\n",
    "\n",
    "hot_cache_ttl: 3600         # 热数据最长保留 1 小时，避免长期堆积\n",
    "\n",
    "建议策略：\n",
    "\n",
    "- 高吞吐场景可设为 0.2~0.4，留足空间给推理计算；\n",
    "- 对系统提示词等关键内容，可通过 pin=True 强制定点驻留。\n",
    "\n",
    ">提示：使用 nvidia-smi 或 Prometheus + DCGM 监控显存趋势，及时调整阈值。 \n",
    "\n",
    "\n",
    "### 2. 缓存雪崩：防止“集体失效”引发集群过载\n",
    "\n",
    "问题现象：\n",
    "\n",
    "大量请求同时未命中缓存，瞬间触发全量 KV Cache 重建，GPU 计算负载飙升，TTFT 暴涨甚至服务不可用。\n",
    "\n",
    "根本原因：\n",
    "\n",
    "当热点缓存集中过期，且无并发控制机制时，所有请求都会穿透到后端重新计算，形成“雪崩效应”。\n",
    "\n",
    "解决方案：\n",
    "\n",
    "引入轻量级分布式锁（如 lua-resty-lock），确保同一 key 只有一个请求回源重建：\n",
    "\n",
    "```python\n",
    "cache = CacheEngine(\n",
    "    lock_timeout=0.5,     # 获取锁超时 500ms\n",
    "    fallback_on_timeout=True  # 超时自动降级，避免阻塞\n",
    ")\n",
    "```\n",
    "\n",
    "工作流程：\n",
    "\n",
    "1. 请求尝试获取该文本指纹对应的锁；\n",
    "2. 成功者执行 KV 计算并写入缓存；\n",
    "3. 其他请求等待结果或短暂降级为直接计算。\n",
    "\n",
    "效果：将并发回源压力降低 90% 以上，保护 GPU 不被突发流量击穿。\n",
    "\n",
    "### 3. 内存泄漏防护：定期清理过期缓存文件\n",
    "\n",
    "问题现象：\n",
    "\n",
    "运行数天后，本地 SSD 空间耗尽，L2 缓存无法写入，性能回落至原始水平。\n",
    "\n",
    "根本原因：\n",
    "\n",
    "虽然内存中的缓存有 TTL 管理，但磁盘上的缓存文件若未主动清理，会长期残留，形成“僵尸数据”。\n",
    "\n",
    "解决方案：\n",
    "\n",
    "通过定时任务定期扫描并删除陈旧缓存文件。\n",
    "\n",
    "添加到 crontab，每小时执行一次清理：\n",
    "\n",
    "0 * * * * find /nvme/lmcache -name \"*.cache\" -mtime +1 -delete\n",
    "\n",
    "使用 ionice 和 nice 降低清理任务对主线程的影响：\n",
    "\n",
    "0 * * * * ionice -c3 nice -n19 find /nvme/lmcache -name \"*.cache\" -mtime +1 -delete\n",
    "\n",
    "> 提示：结合监控告警：当磁盘使用率 >80% 时触发预警。\n",
    ">\n",
    "> 缓存命中率监控\n",
    ">\n",
    "> curl http://localhost:8000/metrics | grep lmcache_hit_rate\n",
    ">\n",
    "> GPU显存使用监控  \n",
    ">\n",
    "> nvidia-smi --query-gpu=memory.used,memory.total --format=csv\n",
    ">\n",
    "> 延迟监控\n",
    ">\n",
    "> curl http://localhost:8000/metrics | grep request_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f25857",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "MCache 通过创新的 KV Cache 复用机制 与 多级缓存协同架构，实现了大模型推理性能的跨越式提升，正在成为高并发、长上下文场景下的标准优化方案。\n",
    "\n",
    "**核心价值**：\n",
    "\n",
    "1.  **显著降低延迟**\n",
    "2.  **大幅节省资源**\n",
    "3.  **提升系统吞吐量**\n",
    "4.  **降低运营成本**\n",
    "\n",
    "\n",
    "- 三级缓存体系：构建从 GPU 显存 → CPU 内存 → 分布式存储的全链路缓存路径，兼顾速度与扩展性。\n",
    "\n",
    "- 智能冷热分离：动态识别热点内容（如系统提示词、通用指令），实现热数据常驻、冷数据自动下沉。\n",
    "\n",
    "- 无缝集成现有生态：原生兼容 vLLM 等主流推理引擎，无需修改业务代码，开箱即用。\n",
    "\n",
    "- 生产级就绪能力：支持集群化部署、分布式锁、缓存过期策略与完整监控告警体系，满足企业级 SLA 要求。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
