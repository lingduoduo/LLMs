{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e6ea5ff",
   "metadata": {},
   "source": [
    "## 服务动态扩容？Kubernetes弹性伸缩+负载均衡实现自动化运维\n",
    "\n",
    "以下场景是是不是你每天要面对的？\n",
    "\n",
    "1. 业务高峰期8个GPU节点全部爆满，用户排队等推理结果\n",
    "2. 凌晨3点突发流量激增，手动扩容根本来不及，服务直接崩了。\n",
    "3. 单月GPU费用烧了12万美金！\n",
    "4. ...\n",
    "\n",
    "## 核心架构：三层弹性伸缩体系\n",
    "\n",
    "### 第一层：应用层弹性 (HPA) - GPU感知的Pod自动扩缩容\n",
    "- 监控GPU利用率、显存使用率、推理队列长度\n",
    "- 当GPU利用率>70%时自动扩容，<30%时缩容\n",
    "- 解决\"用户排队等推理\"的问题\n",
    "  \n",
    "### 第二层：资源层弹性 (VPA) - 动态调整Pod资源配置  \n",
    "- 7B模型用16GB显存，70B模型用80GB显存\n",
    "- 根据实际使用情况智能推荐资源配置\n",
    "- 解决\"资源配置不匹配\"的问题\n",
    "  \n",
    "### 第三层：集群层弹性 (CA) - 自动增减GPU节点\n",
    "- 当Pod因资源不足无法调度时自动加节点\n",
    "- 节点空闲超过30秒开始缩容评估\n",
    "- 解决\"节点数量跟不上业务变化\"的问题\n",
    "\n",
    "这三层协同工作，就像给你的GPU集群装了个\"智能大脑\"。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d5af29",
   "metadata": {},
   "source": [
    "## 第一步：HPA配置 - GPU感知的Pod自动扩缩容\n",
    "\n",
    "传统HPA就像个近视眼，只能看到CPU和内存，完全看不见GPU这个\"耗电大户\"。我曾经遇到过这样的尴尬：CPU使用率只有30%，HPA觉得很轻松不扩容，但GPU利用率已经95%了，用户请求全在排队！\n",
    "\n",
    "**解决方案：让HPA长出\"GPU眼睛\"** \n",
    "\n",
    "我们用KEDA + Prometheus给HPA装上\"GPU眼睛\"，让它能看懂GPU的真实负载。\n",
    "\n",
    "四个步骤\n",
    "\n",
    "1. 安装GPU监控组件\n",
    "2. 配置GPU指标采集\n",
    "3. 部署大模型Agent服务\n",
    "4. 配置基于GPU的HPA\n",
    "\n",
    "具体操作如下：\n",
    "\n",
    "### 步骤1: 安装GPU监控组件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a419b11",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# install-gpu-monitoring.sh\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "# 安装NVIDIA GPU Operator（官方安装方式）\n",
    "echo \"正在安装NVIDIA GPU Operator...\"\n",
    "helm repo add nvidia https://helm.ngc.nvidia.com/nvidia\n",
    "helm repo update\n",
    "\n",
    "helm install --wait gpu-operator \\\n",
    "  -n gpu-operator --create-namespace \\\n",
    "  nvidia/gpu-operator \\\n",
    "  --set driver.enabled=true \\\n",
    "  --set toolkit.enabled=true \\\n",
    "  --set devicePlugin.enabled=true \\\n",
    "  --set dcgmExporter.enabled=true \\\n",
    "  --set gfd.enabled=true\n",
    "\n",
    "# 安装DCGM-Exporter用于GPU指标采集\n",
    "echo \"正在安装DCGM-Exporter...\"\n",
    "helm repo add gpu-helm-charts https://nvidia.github.io/dcgm-exporter/helm-charts\n",
    "helm repo update\n",
    "\n",
    "helm install dcgm-exporter gpu-helm-charts/dcgm-exporter \\\n",
    "  --namespace gpu-operator \\\n",
    "  --set serviceMonitor.enabled=true \\\n",
    "  --set serviceMonitor.additionalLabels.release=prometheus\n",
    "\n",
    "# 安装KEDA (事件驱动自动扩缩容)\n",
    "echo \"正在安装KEDA...\"\n",
    "helm repo add kedacore https://kedacore.github.io/charts\n",
    "helm repo update\n",
    "\n",
    "helm install keda kedacore/keda \\\n",
    "  --namespace keda-system \\\n",
    "  --create-namespace \\\n",
    "  --set prometheus.metricServer.enabled=true \\\n",
    "  --set prometheus.operator.enabled=true\n",
    "\n",
    "# 验证安装\n",
    "echo \"验证安装状态...\"\n",
    "kubectl get pods -n gpu-operator\n",
    "kubectl get pods -n keda-system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74b3c18",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 步骤2: 配置GPU指标采集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72df9c23",
   "metadata": {
    "vscode": {
     "languageId": "yaml"
    }
   },
   "outputs": [],
   "source": [
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: gpu-metrics-config\n",
    "  namespace: gpu-operator\n",
    "data:\n",
    "  prometheus.yml: |\n",
    "    global:\n",
    "      scrape_interval: 15s\n",
    "      evaluation_interval: 15s\n",
    "    \n",
    "    rule_files:\n",
    "      - \"gpu_rules.yml\"\n",
    "    \n",
    "    scrape_configs:\n",
    "    - job_name: 'dcgm-exporter'\n",
    "      kubernetes_sd_configs:\n",
    "      - role: endpoints\n",
    "        namespaces:\n",
    "          names:\n",
    "          - gpu-operator\n",
    "      relabel_configs:\n",
    "      - source_labels: [__meta_kubernetes_service_name]\n",
    "        action: keep\n",
    "        regex: dcgm-exporter\n",
    "      - source_labels: [__meta_kubernetes_endpoint_port_name]\n",
    "        action: keep\n",
    "        regex: metrics\n",
    "      metrics_path: /metrics\n",
    "      scrape_interval: 10s\n",
    "      scrape_timeout: 10s\n",
    "      \n",
    "    - job_name: 'gpu-node-exporter'\n",
    "      kubernetes_sd_configs:\n",
    "      - role: node\n",
    "      relabel_configs:\n",
    "      - source_labels: [__meta_kubernetes_node_label_accelerator]\n",
    "        action: keep\n",
    "        regex: nvidia.*\n",
    "      - source_labels: [__address__]\n",
    "        regex: '(.*):10250'\n",
    "        target_label: __address__\n",
    "        replacement: '${1}:9100'\n",
    "      metrics_path: /metrics\n",
    "      scrape_interval: 15s\n",
    "\n",
    "  gpu_rules.yml: |\n",
    "    groups:\n",
    "    - name: gpu.rules\n",
    "      rules:\n",
    "      - alert: GPUHighUtilization\n",
    "        expr: DCGM_FI_DEV_GPU_UTIL > 90\n",
    "        for: 5m\n",
    "        labels:\n",
    "          severity: warning\n",
    "        annotations:\n",
    "          summary: \"GPU utilization is high\"\n",
    "          description: \"GPU {{ $labels.gpu }} on {{ $labels.instance }} has been above 90% for more than 5 minutes\"\n",
    "      \n",
    "      - alert: GPUHighMemoryUsage\n",
    "        expr: (DCGM_FI_DEV_FB_USED / DCGM_FI_DEV_FB_TOTAL) * 100 > 85\n",
    "        for: 5m\n",
    "        labels:\n",
    "          severity: warning\n",
    "        annotations:\n",
    "          summary: \"GPU memory usage is high\"\n",
    "          description: \"GPU {{ $labels.gpu }} memory usage on {{ $labels.instance }} is above 85%\"\n",
    "      \n",
    "      - alert: GPUTemperatureHigh\n",
    "        expr: DCGM_FI_DEV_GPU_TEMP > 80\n",
    "        for: 3m\n",
    "        labels:\n",
    "          severity: critical\n",
    "        annotations:\n",
    "          summary: \"GPU temperature is high\"\n",
    "          description: \"GPU {{ $labels.gpu }} temperature on {{ $labels.instance }} is above 80°C\"\n",
    "\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: gpu-metrics-service\n",
    "  namespace: gpu-operator\n",
    "  labels:\n",
    "    app: gpu-metrics\n",
    "spec:\n",
    "  selector:\n",
    "    app: dcgm-exporter\n",
    "  ports:\n",
    "  - name: metrics\n",
    "    port: 9400\n",
    "    targetPort: 9400\n",
    "    protocol: TCP\n",
    "  type: ClusterIP\n",
    "\n",
    "---\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: ServiceMonitor\n",
    "metadata:\n",
    "  name: gpu-metrics-monitor\n",
    "  namespace: gpu-operator\n",
    "  labels:\n",
    "    app: gpu-metrics\n",
    "    release: prometheus\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: dcgm-exporter\n",
    "  endpoints:\n",
    "  - port: metrics\n",
    "    interval: 10s\n",
    "    path: /metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf940a4",
   "metadata": {},
   "source": [
    "### 步骤3: 部署大模型Agent服务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e5599b",
   "metadata": {
    "vscode": {
     "languageId": "yaml"
    }
   },
   "outputs": [],
   "source": [
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: llm-agent\n",
    "  namespace: default\n",
    "  labels:\n",
    "    app: llm-agent\n",
    "    version: v1\n",
    "spec:\n",
    "  replicas: 2\n",
    "  strategy:\n",
    "    type: RollingUpdate\n",
    "    rollingUpdate:\n",
    "      maxSurge: 1\n",
    "      maxUnavailable: 0\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: llm-agent\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: llm-agent\n",
    "        version: v1\n",
    "      annotations:\n",
    "        prometheus.io/scrape: \"true\"\n",
    "        prometheus.io/port: \"9090\"\n",
    "        prometheus.io/path: \"/metrics\"\n",
    "    spec:\n",
    "      nodeSelector:\n",
    "        accelerator: nvidia-tesla-gpu\n",
    "      affinity:\n",
    "        podAntiAffinity:\n",
    "          preferredDuringSchedulingIgnoredDuringExecution:\n",
    "          - weight: 100\n",
    "            podAffinityTerm:\n",
    "              labelSelector:\n",
    "                matchExpressions:\n",
    "                - key: app\n",
    "                  operator: In\n",
    "                  values:\n",
    "                  - llm-agent\n",
    "              topologyKey: kubernetes.io/hostname\n",
    "      containers:\n",
    "      - name: llm-agent\n",
    "        image: your-registry/llm-agent:latest\n",
    "        imagePullPolicy: Always\n",
    "        resources:\n",
    "          requests:\n",
    "            nvidia.com/gpu: 1\n",
    "            cpu: 2000m\n",
    "            memory: 8Gi\n",
    "            ephemeral-storage: 10Gi\n",
    "          limits:\n",
    "            nvidia.com/gpu: 1\n",
    "            cpu: 4000m\n",
    "            memory: 16Gi\n",
    "            ephemeral-storage: 20Gi\n",
    "        env:\n",
    "        - name: MODEL_PATH\n",
    "          value: \"/models/your-model\"\n",
    "        - name: MAX_BATCH_SIZE\n",
    "          value: \"8\"\n",
    "        - name: CUDA_VISIBLE_DEVICES\n",
    "          value: \"0\"\n",
    "        - name: NVIDIA_VISIBLE_DEVICES\n",
    "          value: \"all\"\n",
    "        - name: NVIDIA_DRIVER_CAPABILITIES\n",
    "          value: \"compute,utility\"\n",
    "        - name: MAX_CONCURRENT_REQUESTS\n",
    "          value: \"32\"\n",
    "        - name: MODEL_CACHE_SIZE\n",
    "          value: \"4096\"\n",
    "        - name: PROMETHEUS_PORT\n",
    "          value: \"9090\"\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "          name: http\n",
    "          protocol: TCP\n",
    "        - containerPort: 9090\n",
    "          name: metrics\n",
    "          protocol: TCP\n",
    "        readinessProbe:\n",
    "          httpGet:\n",
    "            path: /health/ready\n",
    "            port: 8000\n",
    "            scheme: HTTP\n",
    "          initialDelaySeconds: 60\n",
    "          periodSeconds: 10\n",
    "          timeoutSeconds: 5\n",
    "          successThreshold: 1\n",
    "          failureThreshold: 3\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /health/live\n",
    "            port: 8000\n",
    "            scheme: HTTP\n",
    "          initialDelaySeconds: 120\n",
    "          periodSeconds: 30\n",
    "          timeoutSeconds: 10\n",
    "          successThreshold: 1\n",
    "          failureThreshold: 3\n",
    "        startupProbe:\n",
    "          httpGet:\n",
    "            path: /health/startup\n",
    "            port: 8000\n",
    "            scheme: HTTP\n",
    "          initialDelaySeconds: 30\n",
    "          periodSeconds: 10\n",
    "          timeoutSeconds: 5\n",
    "          successThreshold: 1\n",
    "          failureThreshold: 30\n",
    "        volumeMounts:\n",
    "        - name: model-storage\n",
    "          mountPath: /models\n",
    "          readOnly: true\n",
    "        - name: cache-storage\n",
    "          mountPath: /cache\n",
    "        - name: tmp-storage\n",
    "          mountPath: /tmp\n",
    "        securityContext:\n",
    "          runAsNonRoot: true\n",
    "          runAsUser: 1000\n",
    "          runAsGroup: 1000\n",
    "          allowPrivilegeEscalation: false\n",
    "          readOnlyRootFilesystem: true\n",
    "          capabilities:\n",
    "            drop:\n",
    "            - ALL\n",
    "      volumes:\n",
    "      - name: model-storage\n",
    "        persistentVolumeClaim:\n",
    "          claimName: model-pvc\n",
    "      - name: cache-storage\n",
    "        emptyDir:\n",
    "          sizeLimit: 5Gi\n",
    "      - name: tmp-storage\n",
    "        emptyDir:\n",
    "          sizeLimit: 2Gi\n",
    "      tolerations:\n",
    "      - key: nvidia.com/gpu\n",
    "        operator: Exists\n",
    "        effect: NoSchedule\n",
    "      - key: node.kubernetes.io/instance-type\n",
    "        operator: Equal\n",
    "        value: gpu-instance\n",
    "        effect: NoSchedule\n",
    "      terminationGracePeriodSeconds: 60\n",
    "\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: llm-agent-service\n",
    "  namespace: default\n",
    "  labels:\n",
    "    app: llm-agent\n",
    "spec:\n",
    "  selector:\n",
    "    app: llm-agent\n",
    "  ports:\n",
    "  - name: http\n",
    "    port: 80\n",
    "    targetPort: 8000\n",
    "    protocol: TCP\n",
    "  - name: metrics\n",
    "    port: 9090\n",
    "    targetPort: 9090\n",
    "    protocol: TCP\n",
    "  type: ClusterIP\n",
    "  sessionAffinity: None\n",
    "\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: model-pvc\n",
    "  namespace: default\n",
    "spec:\n",
    "  accessModes:\n",
    "    - ReadOnlyMany\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 50Gi\n",
    "  storageClassName: fast-ssd\n",
    "\n",
    "---\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: ServiceMonitor\n",
    "metadata:\n",
    "  name: llm-agent-monitor\n",
    "  namespace: default\n",
    "  labels:\n",
    "    app: llm-agent\n",
    "    release: prometheus\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: llm-agent\n",
    "  endpoints:\n",
    "  - port: metrics\n",
    "    interval: 15s\n",
    "    path: /metrics\n",
    "    honorLabels: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c91e3d",
   "metadata": {},
   "source": [
    "### 步骤4: 配置基于GPU的HPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f9b985",
   "metadata": {
    "vscode": {
     "languageId": "yaml"
    }
   },
   "outputs": [],
   "source": [
    "apiVersion: keda.sh/v1alpha1\n",
    "kind: ScaledObject\n",
    "metadata:\n",
    "  name: llm-agent-scaler\n",
    "  namespace: default\n",
    "  labels:\n",
    "    app: llm-agent\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    name: llm-agent\n",
    "  minReplicaCount: 2\n",
    "  maxReplicaCount: 10\n",
    "  pollingInterval: 30\n",
    "  cooldownPeriod: 300\n",
    "  idleReplicaCount: 1\n",
    "  fallback:\n",
    "    failureThreshold: 3\n",
    "    replicas: 3\n",
    "  advanced:\n",
    "    restoreToOriginalReplicaCount: true\n",
    "    horizontalPodAutoscalerConfig:\n",
    "      behavior:\n",
    "        scaleDown:\n",
    "          stabilizationWindowSeconds: 300\n",
    "          policies:\n",
    "          - type: Percent\n",
    "            value: 50\n",
    "            periodSeconds: 60\n",
    "        scaleUp:\n",
    "          stabilizationWindowSeconds: 60\n",
    "          policies:\n",
    "          - type: Percent\n",
    "            value: 100\n",
    "            periodSeconds: 30\n",
    "          - type: Pods\n",
    "            value: 2\n",
    "            periodSeconds: 60\n",
    "          selectPolicy: Max\n",
    "  triggers:\n",
    "  # GPU利用率触发器\n",
    "  - type: prometheus\n",
    "    metadata:\n",
    "      serverAddress: http://prometheus.monitoring.svc.cluster.local:9090\n",
    "      metricName: gpu_utilization_avg\n",
    "      threshold: '70'\n",
    "      query: |\n",
    "        avg(\n",
    "          DCGM_FI_DEV_GPU_UTIL{\n",
    "            job=\"dcgm-exporter\",\n",
    "            kubernetes_pod_name=~\"llm-agent-.*\"\n",
    "          }\n",
    "        )\n",
    "    authenticationRef:\n",
    "      name: prometheus-auth\n",
    "  \n",
    "  # GPU显存使用率触发器\n",
    "  - type: prometheus\n",
    "    metadata:\n",
    "      serverAddress: http://prometheus.monitoring.svc.cluster.local:9090\n",
    "      metricName: gpu_memory_utilization_avg\n",
    "      threshold: '80'\n",
    "      query: |\n",
    "        avg(\n",
    "          (DCGM_FI_DEV_FB_USED{\n",
    "            job=\"dcgm-exporter\",\n",
    "            kubernetes_pod_name=~\"llm-agent-.*\"\n",
    "          } / DCGM_FI_DEV_FB_TOTAL{\n",
    "            job=\"dcgm-exporter\",\n",
    "            kubernetes_pod_name=~\"llm-agent-.*\"\n",
    "          }) * 100\n",
    "        )\n",
    "    authenticationRef:\n",
    "      name: prometheus-auth\n",
    "  \n",
    "  # 请求队列长度触发器\n",
    "  - type: prometheus\n",
    "    metadata:\n",
    "      serverAddress: http://prometheus.monitoring.svc.cluster.local:9090\n",
    "      metricName: request_queue_length_avg\n",
    "      threshold: '10'\n",
    "      query: |\n",
    "        avg(\n",
    "          llm_request_queue_length{\n",
    "            job=\"llm-agent\",\n",
    "            kubernetes_pod_name=~\"llm-agent-.*\"\n",
    "          }\n",
    "        )\n",
    "    authenticationRef:\n",
    "      name: prometheus-auth\n",
    "  \n",
    "  # 请求响应时间触发器\n",
    "  - type: prometheus\n",
    "    metadata:\n",
    "      serverAddress: http://prometheus.monitoring.svc.cluster.local:9090\n",
    "      metricName: request_duration_p95\n",
    "      threshold: '5000'  # 5秒\n",
    "      query: |\n",
    "        histogram_quantile(0.95,\n",
    "          rate(llm_request_duration_seconds_bucket{\n",
    "            job=\"llm-agent\",\n",
    "            kubernetes_pod_name=~\"llm-agent-.*\"\n",
    "          }[5m])\n",
    "        ) * 1000\n",
    "    authenticationRef:\n",
    "      name: prometheus-auth\n",
    "\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: prometheus-auth\n",
    "  namespace: default\n",
    "type: Opaque\n",
    "data:\n",
    "  # 如果Prometheus需要认证，在这里配置\n",
    "  # username: <base64-encoded-username>\n",
    "  # password: <base64-encoded-password>\n",
    "\n",
    "---\n",
    "apiVersion: keda.sh/v1alpha1\n",
    "kind: TriggerAuthentication\n",
    "metadata:\n",
    "  name: prometheus-auth\n",
    "  namespace: default\n",
    "spec:\n",
    "  secretTargetRef:\n",
    "  - parameter: username\n",
    "    name: prometheus-auth\n",
    "    key: username\n",
    "  - parameter: password\n",
    "    name: prometheus-auth\n",
    "    key: password\n",
    "\n",
    "---\n",
    "# 备用HPA配置（如果KEDA不可用）\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: llm-agent-hpa-backup\n",
    "  namespace: default\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: llm-agent\n",
    "  minReplicas: 2\n",
    "  maxReplicas: 8\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: memory\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 80\n",
    "  behavior:\n",
    "    scaleDown:\n",
    "      stabilizationWindowSeconds: 300\n",
    "      policies:\n",
    "      - type: Percent\n",
    "        value: 50\n",
    "        periodSeconds: 60\n",
    "    scaleUp:\n",
    "      stabilizationWindowSeconds: 60\n",
    "      policies:\n",
    "      - type: Percent\n",
    "        value: 100\n",
    "        periodSeconds: 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f142830",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Deployment script\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"Starting deployment of GPU monitoring and auto-scaling system...\"\n",
    "\n",
    "# Step 1: Install GPU monitoring components\n",
    "echo \"Step 1: Installing GPU monitoring components\"\n",
    "bash install-gpu-monitoring.sh\n",
    "\n",
    "# Wait for GPU Operator to be ready\n",
    "echo \"Waiting for GPU Operator to become ready...\"\n",
    "kubectl wait --for=condition=ready pod \\\n",
    "  -l app=nvidia-operator-validator \\\n",
    "  -n gpu-operator \\\n",
    "  --timeout=300s\n",
    "\n",
    "# Step 2: Configure GPU metrics collection\n",
    "echo \"Step 2: Configuring GPU metrics collection\"\n",
    "kubectl apply -f gpu-metrics-config.yaml\n",
    "\n",
    "# Step 3: Deploy the LLM Agent service\n",
    "echo \"Step 3: Deploying the Large Language Model (LLM) Agent service\"\n",
    "kubectl apply -f llm-agent-deployment.yaml\n",
    "\n",
    "# Wait for the service to be ready\n",
    "echo \"Waiting for the LLM Agent service to become ready...\"\n",
    "kubectl wait --for=condition=ready pod \\\n",
    "  -l app=llm-agent \\\n",
    "  --timeout=600s\n",
    "\n",
    "# Step 4: Configure GPU-based Horizontal Pod Autoscaling (HPA)\n",
    "echo \"Step 4: Configuring GPU-based HPA\"\n",
    "kubectl apply -f llm-agent-hpa.yaml\n",
    "\n",
    "# Verify deployment\n",
    "echo \"Verifying deployment status...\"\n",
    "kubectl get pods -n gpu-operator\n",
    "kubectl get pods -l app=llm-agent\n",
    "kubectl get scaledobjects\n",
    "kubectl get hpa\n",
    "\n",
    "echo \"Deployment complete!\"\n",
    "echo \"You can view GPU metrics using the following command:\"\n",
    "echo \"kubectl port-forward -n gpu-operator svc/dcgm-exporter 9400:9400\"\n",
    "echo \"Then visit http://localhost:9400/metrics\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3f19db",
   "metadata": {},
   "source": [
    "## 第二步：VPA配置 - 动态资源调整\n",
    "\n",
    "核心问题：模型大小差异巨大，资源需求天差地别\n",
    "\n",
    "**解决方案：VPA当\"智能裁缝\"**\n",
    "\n",
    "VPA就像个智能裁缝，会根据实际使用情况推荐最合适的\"衣服尺寸\"。\n",
    "\n",
    "### 1. 安装 VPA 组件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aad2eb",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# Install Vertical Pod Autoscaler (revised version)\n",
    "echo \"Installing Vertical Pod Autoscaler (VPA) components...\"\n",
    "\n",
    "# Method 1: Use the officially recommended installation method\n",
    "git clone https://github.com/kubernetes/autoscaler.git\n",
    "cd autoscaler/vertical-pod-autoscaler/\n",
    "\n",
    "# Check Kubernetes version compatibility\n",
    "KUBE_VERSION=$(kubectl version --short | grep \"Server Version\" | awk '{print $3}' | sed 's/v//')\n",
    "echo \"Detected Kubernetes version: $KUBE_VERSION\"\n",
    "\n",
    "# Install VPA components\n",
    "./hack/vpa-up.sh\n",
    "\n",
    "# Verify installation\n",
    "echo \"Verifying VPA component installation...\"\n",
    "kubectl get pods -n kube-system | grep vpa\n",
    "kubectl get crd | grep verticalpodautoscaler\n",
    "\n",
    "# Method 2: Install using Helm (recommended for production)\n",
    "# helm repo add cowboysysop https://cowboysysop.github.io/charts/\n",
    "# helm install vpa cowboysysop/vertical-pod-autoscaler \\\n",
    "#   --namespace kube-system \\\n",
    "#   --set recommender.enabled=true \\\n",
    "#   --set updater.enabled=false \\\n",
    "#   --set admissionController.enabled=false\n",
    "\n",
    "echo \"VPA installation completed!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bea54b0",
   "metadata": {},
   "source": [
    "### 2. 配置VPA策略（VPA最佳实践配置）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee91baa",
   "metadata": {
    "vscode": {
     "languageId": "yaml"
    }
   },
   "outputs": [],
   "source": [
    "# Best Practice 1: Environment-specific VPA config (Dev)\n",
    "apiVersion: autoscaling.k8s.io/v1\n",
    "kind: VerticalPodAutoscaler\n",
    "metadata:\n",
    "  name: llm-agent-vpa-dev\n",
    "  namespace: development\n",
    "spec:\n",
    "  targetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: llm-agent\n",
    "  updatePolicy:\n",
    "    updateMode: \"Auto\"  # In dev, you can allow automatic updates\n",
    "    minReplicas: 1      # Dev can allow a single replica\n",
    "  resourcePolicy:\n",
    "    containerPolicies:\n",
    "    - containerName: llm-agent\n",
    "      minAllowed:\n",
    "        cpu: 500m\n",
    "        memory: 2Gi\n",
    "      maxAllowed:\n",
    "        cpu: 4000m\n",
    "        memory: 16Gi\n",
    "      controlledResources: [\"cpu\", \"memory\"]\n",
    "\n",
    "---\n",
    "# Best Practice 2: Conservative production config (Prod)\n",
    "apiVersion: autoscaling.k8s.io/v1\n",
    "kind: VerticalPodAutoscaler\n",
    "metadata:\n",
    "  name: llm-agent-vpa-prod\n",
    "  namespace: production\n",
    "spec:\n",
    "  targetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: llm-agent\n",
    "  updatePolicy:\n",
    "    updateMode: \"Initial\"  # In prod, apply only at pod creation time\n",
    "  resourcePolicy:\n",
    "    containerPolicies:\n",
    "    - containerName: llm-agent\n",
    "      minAllowed:\n",
    "        cpu: 2000m     # Higher minimums in production\n",
    "        memory: 8Gi\n",
    "      maxAllowed:\n",
    "        cpu: 6000m     # Conservative maximums\n",
    "        memory: 24Gi\n",
    "      controlledResources: [\"cpu\", \"memory\"]\n",
    "      controlledValues: \"RequestsOnly\"  # Only adjust requests\n",
    "\n",
    "---\n",
    "# Best Practice 3: VPA with a PodDisruptionBudget (PDB)\n",
    "apiVersion: policy/v1\n",
    "kind: PodDisruptionBudget\n",
    "metadata:\n",
    "  name: llm-agent-pdb\n",
    "  namespace: default\n",
    "spec:\n",
    "  minAvailable: 1  # Ensure at least 1 pod is always running\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: llm-agent\n",
    "\n",
    "---\n",
    "# Best Practice 4: VPA + HPA working together\n",
    "apiVersion: autoscaling.k8s.io/v1\n",
    "kind: VerticalPodAutoscaler\n",
    "metadata:\n",
    "  name: llm-agent-vpa-with-hpa\n",
    "  namespace: default\n",
    "  annotations:\n",
    "    vpa.kubernetes.io/hpa-compatible: \"true\"  # Mark as HPA-compatible\n",
    "spec:\n",
    "  targetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: llm-agent\n",
    "  updatePolicy:\n",
    "    updateMode: \"Off\"  # Recommended to disable auto-updates when used with HPA\n",
    "  resourcePolicy:\n",
    "    containerPolicies:\n",
    "    - containerName: llm-agent\n",
    "      minAllowed:\n",
    "        cpu: 2000m\n",
    "        memory: 8Gi\n",
    "      maxAllowed:\n",
    "        cpu: 8000m\n",
    "        memory: 32Gi\n",
    "      controlledResources: [\"cpu\", \"memory\"]\n",
    "      # IMPORTANT: special configuration when used with HPA\n",
    "      mode: Auto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeec82bd",
   "metadata": {},
   "source": [
    "### 3. VPA监控和告警配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c31575",
   "metadata": {
    "vscode": {
     "languageId": "yaml"
    }
   },
   "outputs": [],
   "source": [
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: ServiceMonitor\n",
    "metadata:\n",
    "  name: vpa-recommender-monitor\n",
    "  namespace: kube-system\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: vpa-recommender\n",
    "  endpoints:\n",
    "  - port: metrics\n",
    "    interval: 30s\n",
    "    path: /metrics\n",
    "\n",
    "---\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: PrometheusRule\n",
    "metadata:\n",
    "  name: vpa-alerts\n",
    "  namespace: kube-system\n",
    "spec:\n",
    "  groups:\n",
    "  - name: vpa.rules\n",
    "    rules:\n",
    "    - alert: VPARecommendationDeviation\n",
    "      expr: |\n",
    "        (\n",
    "          kube_pod_container_resource_requests{resource=\"cpu\"} -\n",
    "          vpa_recommendation_target{resource=\"cpu\"}\n",
    "        ) / vpa_recommendation_target{resource=\"cpu\"} * 100 > 50\n",
    "      for: 10m\n",
    "      labels:\n",
    "        severity: warning\n",
    "      annotations:\n",
    "        summary: \"VPA recommended value deviates too much from the current configuration\"\n",
    "        description: \"Pod {{ $labels.pod }} CPU requests differ from the VPA recommendation by more than 50%\"\n",
    "\n",
    "    - alert: VPARecommenderDown\n",
    "      expr: up{job=\"vpa-recommender\"} == 0\n",
    "      for: 5m\n",
    "      labels:\n",
    "        severity: critical\n",
    "      annotations:\n",
    "        summary: \"VPA Recommender service is unavailable\"\n",
    "        description: \"VPA Recommender has been down for more than 5 minutes\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b96421c",
   "metadata": {},
   "source": [
    "## 第三步：CA配置 - 集群节点自动扩缩容\n",
    "\n",
    "核心问题：GPU节点贵得要命，必须精打细算\n",
    "\n",
    "解决方案：Karpenter - 比传统CA聪明10倍的\"节点管家\"\n",
    "\n",
    "Karpenter是AWS开源的下一代集群自动扩缩容器，专门为GPU这种\"金贵\"资源设计。\n",
    "\n",
    "- 秒级决策：传统CA需要几分钟考虑，Karpenter几秒钟就决定\n",
    "- 成本优先：自动选择最便宜的实例类型组合\n",
    "- 多实例类型：可以混合使用不同GPU型号，灵活调度\n",
    "\n",
    "\n",
    "核心配置举例：\n",
    "\n",
    "### 1. 智能实例选择策略 - 成本优化核心\n",
    "\n",
    "```yaml\n",
    "# 最关键的成本优化配置\n",
    "spec:\n",
    "  template:\n",
    "    spec:\n",
    "      requirements:\n",
    "      - key: node.kubernetes.io/instance-type\n",
    "        operator: In\n",
    "        values: [\"g4dn.xlarge\", \"g4dn.2xlarge\", \"p3.2xlarge\"]  # 按成本排序\n",
    "      - key: karpenter.sh/capacity-type\n",
    "        operator: In\n",
    "        values: [\"spot\", \"on-demand\"]  # 优先竞价实例，节省70%成本\n",
    "```\n",
    "核心优势:\n",
    "\n",
    "- 自动选择成本最低的GPU实例类型\n",
    "- 竞价实例优先，成本降低70%\n",
    "- 多实例类型支持，提高可用性\n",
    "\n",
    "### 2. 激进缩容策略 - 防止资源浪费\n",
    "\n",
    "```yaml\n",
    "# 激进的缩容配置 - 省钱核心\n",
    "disruption:\n",
    "  consolidationPolicy: WhenEmpty    # 节点空闲立即评估缩容\n",
    "  consolidateAfter: 30s            # 30秒空闲即缩容（关键！）\n",
    "  expireAfter: 2160h               # 90天强制替换，避免老化\n",
    "```\n",
    "核心优势:\n",
    "\n",
    "- 30秒缩容 : 比传统5分钟节省大量成本\n",
    "- WhenEmpty策略 : 节点完全空闲立即回收\n",
    "- 自动节点替换 : 避免节点老化问题\n",
    "\n",
    "\n",
    "### 3. GPU节点专用污点 - 资源隔离\n",
    "\n",
    "```yaml\n",
    "# GPU节点隔离配置\n",
    "taints:\n",
    "- key: nvidia.com/gpu\n",
    "  value: \"true\"\n",
    "  effect: NoSchedule  # 只有GPU工作负载才能调度到GPU节点\n",
    "```\n",
    "核心优势:\n",
    "\n",
    "- 防止非GPU工作负载占用昂贵的GPU节点\n",
    "- 确保GPU资源专用，提高利用率\n",
    "- 避免资源浪费和成本泄漏\n",
    "\n",
    "### 4. 多可用区容错配置\n",
    "```yaml\n",
    "# 高可用配置\n",
    "subnetSelectorTerms:\n",
    "- tags:\n",
    "    karpenter.sh/discovery: ${CLUSTER_NAME}  # 自动发现所有可用区\n",
    "```\n",
    "\n",
    "核心优势:\n",
    "\n",
    "- 自动跨可用区分布\n",
    "- 避免单点故障\n",
    "- 提高服务可用性\n",
    "\n",
    "\n",
    "### 5. 自动GPU驱动安装\n",
    "```yaml\n",
    "# 自动化GPU环境配置\n",
    "userData: |\n",
    "  #!/bin/bash\n",
    "  /etc/eks/bootstrap.sh ${CLUSTER_NAME}\n",
    "  # 自动安装NVIDIA驱动\n",
    "  yum install -y nvidia-driver-latest-dkms\n",
    "  # 自动启动GPU监控\n",
    "  systemctl enable nvidia-dcgm\n",
    "  systemctl start nvidia-dcgm\n",
    "```\n",
    "核心优势:\n",
    "\n",
    "- 零人工干预的GPU环境配置\n",
    "- 自动安装最新驱动\n",
    "- 内置GPU监控支持\n",
    "\n",
    "### 完整的成本优化配置\n",
    "```yaml\n",
    "# 终极成本优化的Karpenter配置\n",
    "apiVersion: karpenter.sh/v1\n",
    "kind: NodePool\n",
    "metadata:\n",
    "  name: cost-optimized-gpu-pool\n",
    "spec:\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        node-type: gpu-optimized\n",
    "        cost-tier: spot-first\n",
    "    spec:\n",
    "      requirements:\n",
    "      # 按成本优先级排序的实例类型\n",
    "      - key: node.kubernetes.io/instance-type\n",
    "        operator: In\n",
    "        values: [\n",
    "          \"g4dn.xlarge\",    # 最便宜的GPU实例\n",
    "          \"g4dn.2xlarge\",   # 性价比次选\n",
    "          \"g4ad.xlarge\",    # AMD GPU备选\n",
    "          \"p3.2xlarge\"      # 高性能备选\n",
    "        ]\n",
    "      # 竞价实例优先策略\n",
    "      - key: karpenter.sh/capacity-type\n",
    "        operator: In\n",
    "        values: [\"spot\", \"on-demand\"]\n",
    "      # 多可用区分布\n",
    "      - key: topology.kubernetes.io/zone\n",
    "        operator: In\n",
    "        values: [\"us-west-2a\", \"us-west-2b\", \"us-west-2c\"]\n",
    "      \n",
    "      nodeClassRef:\n",
    "        group: karpenter.k8s.aws\n",
    "        kind: EC2NodeClass\n",
    "        name: optimized-gpu-nodeclass\n",
    "      \n",
    "      # GPU专用污点\n",
    "      taints:\n",
    "      - key: nvidia.com/gpu\n",
    "        value: \"dedicated\"\n",
    "        effect: NoSchedule\n",
    "      - key: workload-type\n",
    "        value: \"ml-inference\"\n",
    "        effect: NoSchedule\n",
    "  \n",
    "  # 严格的资源限制\n",
    "  limits:\n",
    "    cpu: 500      # 控制最大CPU数量\n",
    "    memory: 500Gi # 控制最大内存\n",
    "    nvidia.com/gpu: 20  # 最大GPU数量限制\n",
    "  \n",
    "  # 激进的成本优化策略\n",
    "  disruption:\n",
    "    consolidationPolicy: WhenEmpty\n",
    "    consolidateAfter: 30s        # 30秒空闲即缩容\n",
    "    expireAfter: 1440h          # 60天强制替换（更激进）\n",
    "    \n",
    "---\n",
    "apiVersion: karpenter.k8s.aws/v1\n",
    "kind: EC2NodeClass\n",
    "metadata:\n",
    "  name: optimized-gpu-nodeclass\n",
    "spec:\n",
    "  role: \"KarpenterGPURole-${CLUSTER_NAME}\"\n",
    "  \n",
    "  # 子网选择 - 自动发现最便宜的可用区\n",
    "  subnetSelectorTerms:\n",
    "  - tags:\n",
    "      karpenter.sh/discovery: ${CLUSTER_NAME}\n",
    "      tier: \"gpu-optimized\"\n",
    "  \n",
    "  # 安全组选择\n",
    "  securityGroupSelectorTerms:\n",
    "  - tags:\n",
    "      karpenter.sh/discovery: ${CLUSTER_NAME}\n",
    "  \n",
    "  amiFamily: AL2_x86_64_GPU  # GPU优化的AMI\n",
    "  \n",
    "  # 实例存储优化\n",
    "  instanceStorePolicy: RAID0\n",
    "  \n",
    "  # 详细的标签用于成本跟踪\n",
    "  tags:\n",
    "    Environment: \"production\"\n",
    "    CostCenter: \"ml-inference\"\n",
    "    ManagedBy: \"karpenter\"\n",
    "    AutoScaling: \"enabled\"\n",
    "  \n",
    "  # 优化的用户数据脚本\n",
    "  userData: |\n",
    "    #!/bin/bash\n",
    "    set -e\n",
    "    \n",
    "    # EKS节点初始化\n",
    "    /etc/eks/bootstrap.sh ${CLUSTER_NAME} \\\n",
    "      --container-runtime containerd \\\n",
    "      --kubelet-extra-args '--node-labels=node-type=gpu-optimized,cost-tier=spot-first'\n",
    "    \n",
    "    # GPU驱动和工具安装\n",
    "    yum update -y\n",
    "    yum install -y nvidia-driver-latest-dkms\n",
    "    yum install -y nvidia-docker2\n",
    "    \n",
    "    # GPU监控组件\n",
    "    systemctl enable nvidia-dcgm\n",
    "    systemctl start nvidia-dcgm\n",
    "    \n",
    "    # 性能优化\n",
    "    echo 'net.core.somaxconn = 65535' >> /etc/sysctl.conf\n",
    "    sysctl -p\n",
    "    \n",
    "    # 成本监控脚本\n",
    "    cat > /usr/local/bin/cost-monitor.sh << 'EOF'\n",
    "    #!/bin/bash\n",
    "    # 记录实例使用情况用于成本分析\n",
    "    INSTANCE_ID=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)\n",
    "    INSTANCE_TYPE=$(curl -s http://169.254.169.254/latest/meta-data/instance-type)\n",
    "    echo \"$(date): Instance $INSTANCE_ID ($INSTANCE_TYPE) started\" >> /var/log/cost-tracking.log\n",
    "    EOF\n",
    "    chmod +x /usr/local/bin/cost-monitor.sh\n",
    "    /usr/local/bin/cost-monitor.sh\n",
    "  ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aceb3d3",
   "metadata": {},
   "source": [
    "## 第四步：负载均衡配置 - 智能流量分发\n",
    "\n",
    "核心问题：大模型服务的\"特殊需求\"\n",
    "\n",
    "大模型服务不像普通Web服务，它有两个\"特殊癖好\"：\n",
    "\n",
    "1. 会话保持：用户问\"你叫什么名字\"，模型回答\"我叫ChatGLM\"，下一句用户问\"你几岁了\"，如果请求被分到另一个Pod，模型就不知道前面聊了什么，体验很差。\n",
    "  \n",
    "2. 预热需求：新启动的Pod需要加载模型到显存，这个过程需要2-5分钟。如果立即接收流量，用户会等到怀疑人生。\n",
    "  \n",
    "\n",
    "**解决方案：多层负载均衡 + 智能预热**\n",
    "\n",
    "通用解决方案： Ingress + Service层 + Pod预热机制\n",
    "\n",
    "1. Ingress层配置\n",
    "\n",
    "```yaml\n",
    "# llm-agent-ingress.yaml\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: Ingress\n",
    "metadata:\n",
    "  name: llm-agent-ingress\n",
    "  annotations:\n",
    "    kubernetes.io/ingress.class: nginx\n",
    "    nginx.ingress.kubernetes.io/affinity: \"cookie\"\n",
    "    nginx.ingress.kubernetes.io/affinity-mode: \"persistent\"\n",
    "    nginx.ingress.kubernetes.io/session-cookie-name: \"llm-session\"\n",
    "    nginx.ingress.kubernetes.io/session-cookie-max-age: \"3600\"\n",
    "    nginx.ingress.kubernetes.io/session-cookie-secure: \"true\"  # 生产环境建议启用\n",
    "    nginx.ingress.kubernetes.io/upstream-hash-by: \"$cookie_llm_session\"\n",
    "    # 预热新Pod的配置\n",
    "    nginx.ingress.kubernetes.io/server-snippet: |\n",
    "      location /warmup {\n",
    "        access_log off;\n",
    "        return 200 \"OK\";\n",
    "      }\n",
    "spec:\n",
    "  ingressClassName: nginx  # 推荐使用ingressClassName而非annotation\n",
    "  rules:\n",
    "  - host: llm-api.yourdomain.com\n",
    "    http:\n",
    "      paths:\n",
    "      - path: /\n",
    "        pathType: Prefix\n",
    "        backend:\n",
    "          service:\n",
    "            name: llm-agent-service\n",
    "            port:\n",
    "              number: 80\n",
    "\n",
    "```\n",
    "2. Service层配置\n",
    "\n",
    "```yaml\n",
    "# llm-agent-service.yaml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: llm-agent-service\n",
    "  annotations:\n",
    "    service.beta.kubernetes.io/aws-load-balancer-type: nlb\n",
    "    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: http\n",
    "spec:\n",
    "  type: LoadBalancer\n",
    "  sessionAffinity: ClientIP  # 基于客户端IP的会话保持\n",
    "  sessionAffinityConfig:\n",
    "    clientIP:\n",
    "      timeoutSeconds: 3600   # 1小时会话保持\n",
    "  selector:\n",
    "    app: llm-agent\n",
    "  ports:\n",
    "  - port: 80\n",
    "    targetPort: 8000\n",
    "    protocol: TCP\n",
    "```\n",
    "\n",
    "3. Pod预热机制\n",
    "```yaml\n",
    "# 在Deployment中添加预热InitContainer\n",
    "spec:\n",
    "  template:\n",
    "    spec:\n",
    "      initContainers:\n",
    "      - name: model-warmup\n",
    "        image: your-registry/llm-agent:latest\n",
    "        command: [\"/bin/sh\"]\n",
    "        args:\n",
    "        - -c\n",
    "        - |\n",
    "          echo \"开始模型预热...\"\n",
    "          python warmup.py --model-path /models/your-model --warmup-requests 5\n",
    "          echo \"预热完成\"\n",
    "        resources:\n",
    "          requests:\n",
    "            nvidia.com/gpu: 1\n",
    "        volumeMounts:\n",
    "        - name: model-storage\n",
    "          mountPath: /models\n",
    "```\n",
    "\n",
    "### 负载均衡最佳实践：不同场景不同策略\n",
    "\n",
    "会话保持策略（根据业务场景选择）：\n",
    "\n",
    "- Web应用：使用Cookie会话保持，时长1小时（用户关闭浏览器前保持）\n",
    "- API服务：使用ClientIP会话保持，时长30分钟（API调用相对短暂）\n",
    "- 长连接：使用连接级会话保持（WebSocket推理服务）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798ebe0e",
   "metadata": {},
   "source": [
    "健康检查配置（别让\"半熟\"的Pod接收流量）：\n",
    "```yaml\n",
    "readinessProbe:\n",
    "  httpGet:\n",
    "    path: /health\n",
    "    port: 8000\n",
    "  initialDelaySeconds: 60   # 给模型加载留足时间，别急\n",
    "  periodSeconds: 10\n",
    "  timeoutSeconds: 5\n",
    "  successThreshold: 2       # 连续2次成功才算真的Ready\n",
    "  failureThreshold: 3       # 连续3次失败才算真的挂了\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae9543a",
   "metadata": {},
   "source": [
    "## 实战案例：某医疗AI公司部署的ChatGLM-6B推理服务，日均处理10万次医疗问答请求。\n",
    "\n",
    "### 业务背景：医疗问答的\"高要求\"\n",
    "\n",
    "- 延迟要求：医生问诊时等不起，首Token延迟必须<2秒\n",
    "- 准确性要求：医疗领域容错率极低，服务可用性要求99.9%+\n",
    "- 成本压力：医疗AI利润微薄，GPU成本必须严格控制\n",
    "- 流量特征：工作日8-18点高峰，夜间和周末低谷\n",
    "\n",
    "### 关键改进：\n",
    "- 平均GPU利用率从50%提升到85%\n",
    "- 月度成本从$12万降到$7.2万，节省40%\n",
    "- 服务可用性从95.5%提升到99.8%\n",
    "- 彻底告别半夜被叫醒扩容的噩梦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89af841",
   "metadata": {},
   "source": [
    "### 部署步骤：手把手教你复制成功\n",
    "\n",
    "#### 1. 创建命名空间和资源配额\n",
    "```shell\n",
    "# 创建专用命名空间\n",
    "kubectl create namespace llm-production\n",
    "\n",
    "# 创建GPU资源配额（防止资源滥用）\n",
    "kubectl apply -f - <<EOF\n",
    "apiVersion: v1\n",
    "kind: ResourceQuota\n",
    "metadata:\n",
    "  name: gpu-quota\n",
    "  namespace: llm-production\n",
    "spec:\n",
    "  hard:\n",
    "    requests.nvidia.com/gpu: \"10\"  # 最多10个GPU，根据预算调整\n",
    "    limits.nvidia.com/gpu: \"10\"\n",
    "EOF\n",
    "```\n",
    "\n",
    "#### 2. 一键部署所有组件\n",
    "```shell\n",
    "# 应用所有配置文件（确保顺序正确）\n",
    "kubectl apply -f gpu-metrics-config.yaml -n llm-production\n",
    "kubectl apply -f llm-agent-deployment.yaml -n llm-production\n",
    "kubectl apply -f llm-agent-service.yaml -n llm-production\n",
    "kubectl apply -f llm-agent-hpa.yaml -n llm-production\n",
    "kubectl apply -f llm-agent-vpa.yaml -n llm-production\n",
    "kubectl apply -f llm-agent-ingress.yaml -n llm-production\n",
    "kubectl apply -f gpu-nodepool.yaml  # 这个是集群级别的，不需要namespace\n",
    "```\n",
    "\n",
    "#### 3. 验证部署效果（重要！）\n",
    "```shell\n",
    "# 检查Pod状态（应该都是Running）\n",
    "kubectl get pods -n llm-production -l app=llm-agent\n",
    "\n",
    "# 检查HPA状态（应该能看到GPU指标）\n",
    "kubectl get hpa -n llm-production\n",
    "\n",
    "# 检查节点GPU资源（确认GPU被正确识别）\n",
    "kubectl describe nodes -l node-type=gpu\n",
    "\n",
    "# 测试服务可用性（最关键的一步）\n",
    "curl -X POST https://llm-api.yourdomain.com/chat \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"message\": \"你好，请介绍一下自己\"}' \\\n",
    "  -w \"响应时间: %{time_total}秒\\n\"\n",
    "```\n",
    "\n",
    "如果curl测试返回正常响应且时间<2秒，恭喜你，部署成功了！\n",
    "\n",
    "#### 性能监控Dashboard\n",
    "\n",
    "使用Grafana创建监控面板，关键指标包括：\n",
    "```yaml\n",
    "# 关键监控指标\n",
    "- GPU利用率：avg(DCGM_FI_DEV_GPU_UTIL)\n",
    "- GPU显存使用率：avg(DCGM_FI_DEV_MEM_COPY_UTIL) \n",
    "- Pod副本数：kube_deployment_status_replicas\n",
    "- 请求延迟：histogram_quantile(0.95, http_request_duration_seconds)\n",
    "- 错误率：rate(http_requests_total{status=~\"5..\"}[5m])\n",
    "- 节点数量：count(kube_node_info{node_type=\"gpu\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eab14f2",
   "metadata": {},
   "source": [
    "## 总结：大模型弹性伸缩的\"武功秘籍\"\n",
    "\n",
    "\n",
    "### 核心心法：三层协同，缺一不可\n",
    "\n",
    "**HPA层**：GPU感知的智能扩缩容，让系统有\"眼睛\"看到真实负载\n",
    "\n",
    "**VPA层**：资源配置的智能推荐，让每个Pod都穿\"合身的衣服\"  \n",
    "\n",
    "**CA层**：节点数量的精确控制，让集群成为\"省钱小能手\"\n",
    "\n",
    "**负载均衡**：流量分发的智能调度，让用户体验\"丝般顺滑\"\n",
    "\n",
    "\n",
    "### 最佳实践精华：这些经验值千金\n",
    "\n",
    "1. 扩容触发阈值：GPU利用率70%，别设太低（浪费钱）也别设太高（影响体验）\n",
    "2. 冷却期设置：5-10分钟，防止\"神经质\"扩缩容\n",
    "3. 最小副本数：至少保留2个，完全缩容重启太慢\n",
    "4. 健康检查：大模型启动慢，给足时间（5-10分钟）\n",
    "5. 监控告警：GPU温度、利用率、延迟、错误率，一个都不能少"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
