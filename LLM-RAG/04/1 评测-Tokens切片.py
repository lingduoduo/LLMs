# pip install llama-index llama-index-llms-openai-like llama-index-embeddings-dashscope llama-index-readers-file


import os
from llama_index.core import VectorStoreIndex, Settings, Document
from llama_index.core.node_parser import SentenceWindowNodeParser, SemanticSplitterNodeParser, TokenTextSplitter
from llama_index.core.postprocessor import MetadataReplacementPostProcessor

# å¯¼å…¥ OpenAILike LLM (ç”¨äº DashScope å…¼å®¹æ¨¡å¼ï¼ŒQwen æ¨¡å‹)
from llama_index.llms.openai_like import OpenAILike 
# å¯¼å…¥ DashScopeEmbedding (ç”¨äºé˜¿é‡Œäº‘ DashScope åµŒå…¥æ¨¡å‹)
from llama_index.embeddings.dashscope import DashScopeEmbedding 

# --- é…ç½®é˜¿é‡Œäº‘ DashScope API Key ---
# ç¡®ä¿ä½ çš„ç¯å¢ƒå˜é‡ä¸­è®¾ç½®äº† DASHSCOPE_API_KEY
# æˆ–è€…ç›´æ¥åœ¨è¿™é‡Œè®¾ç½®ï¼ˆä¸æ¨èç”¨äºç”Ÿäº§ç¯å¢ƒï¼‰ï¼š
# os.environ["DASHSCOPE_API_KEY"] = "sk-xxx"

# è·å– API Key
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY")

# åˆå§‹åŒ– LlamaIndex å…¨å±€è®¾ç½®

# 1. é…ç½® LLM (ä½¿ç”¨ DashScope çš„ qwen-plus æ¨¡å‹ï¼Œé€šè¿‡ OpenAILike è°ƒç”¨)
Settings.llm = OpenAILike(
    model="qwen-plus",
    api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
    api_key=DASHSCOPE_API_KEY,
    is_chat_model=True,
    temperature=0.1 # æ·»åŠ æ¸©åº¦å‚æ•°ï¼Œç”¨äºæ§åˆ¶ç”Ÿæˆéšæœºæ€§
) 

# 2. é…ç½®åµŒå…¥æ¨¡å‹ (ä½¿ç”¨ DashScopeEmbedding ç±»)
Settings.embed_model = DashScopeEmbedding(
    model_name="text-embedding-v4", 
    api_key=DASHSCOPE_API_KEY,     
)


def evaluate_splitter(splitter, documents, question, splitter_name):
    """
    è¯„æµ‹ä¸åŒæ–‡æ¡£åˆ‡ç‰‡æ–¹æ³•çš„æ•ˆæœ
    æ‰‹åŠ¨æ‰“å°å¬å›ç»“æœï¼Œæ–¹ä¾¿ç›´æ¥å¯¹æ¯”åˆ‡åˆ†æ•ˆæœã€‚
    """
    print(f"\n{'='*50}")
    print(f"æ­£åœ¨ä½¿ç”¨ {splitter_name} æ–¹æ³•è¿›è¡Œæµ‹è¯•...")
    print(f"{'='*50}\n")

    # æ˜¾ç¤º raw chunks generated by the splitter
    print(f"ã€{splitter_name}ã€‘ç”Ÿæˆçš„åŸå§‹æ–‡æ¡£åˆ‡ç‰‡ (Nodes):")
    raw_nodes = splitter.get_nodes_from_documents(documents)
    for i, node in enumerate(raw_nodes, 1):
        print(f"\n   åˆ‡ç‰‡ {i}:")
        if isinstance(splitter, SentenceWindowNodeParser):
            original_text = node.metadata.get("original_text", node.get_content())
            window_context = node.metadata.get("window", "N/A - çª—å£å†…å®¹æœªç”Ÿæˆ")
            print(f"   æ ¸å¿ƒå†…å®¹: \"{original_text}\"")
            print(f"   å®Œæ•´çª—å£ä¸Šä¸‹æ–‡(ä¾›LLMç”¨): \"{window_context}\"")
        else:
            print(f"   å†…å®¹: \"{node.get_content()}\"")
        # Add metadata for debugging if needed
        # print(f"   å…ƒæ•°æ®: {node.metadata}")
        print("   " + "-" * 40)
    print("\n" + "="*50)

    # # æ„å»ºç´¢å¼•
    # print("æ­£åœ¨å¤„ç†æ–‡æ¡£å¹¶æ„å»ºç´¢å¼•...")
    # nodes = splitter.get_nodes_from_documents(documents) 

    # index = VectorStoreIndex(nodes, embed_model=Settings.embed_model)

    # # åˆ›å»ºæŸ¥è¯¢å¼•æ“
    # print("æ­£åœ¨åˆ›å»ºæŸ¥è¯¢å¼•æ“...")
    # query_engine_params = {
    #     "similarity_top_k": 5,
    #     "streaming": True
    # }

    # # å¦‚æœæ˜¯ Sentence Window åˆ‡ç‰‡ï¼Œæ·»åŠ åå¤„ç†å™¨
    # if isinstance(splitter, SentenceWindowNodeParser):
    #     query_engine_params["node_postprocessors"] = [
    #         MetadataReplacementPostProcessor(target_metadata_key="window")
    #     ]
    #     print("ğŸ’¡ æ£€æµ‹åˆ° Sentence Window åˆ‡ç‰‡ï¼Œå·²æ·»åŠ  MetadataReplacementPostProcessorã€‚")
    
    # query_engine = index.as_query_engine(**query_engine_params)

    # # æ‰§è¡ŒæŸ¥è¯¢
    # print(f"\næµ‹è¯•é—®é¢˜: {question}")
    # print("\næ¨¡å‹å›ç­”:")
    # response = query_engine.query(question)
    # response.print_response_stream()

    # # è¾“å‡ºå¬å›çš„å‚è€ƒç‰‡æ®µ
    # print(f"\n{splitter_name} å¬å›çš„å‚è€ƒç‰‡æ®µ:")
    # if response.source_nodes:
    #     for i, node in enumerate(response.source_nodes, 1):
    #         print(f"\n--- æ–‡æ¡£ç‰‡æ®µ {i} ---")
            
    #         # --- ä¼˜åŒ–åçš„æ‰“å°é€»è¾‘ ---
    #         if isinstance(splitter, SentenceWindowNodeParser):
    #             # å¯¹äºSentence Windowï¼Œæ‰“å°å‡ºçª—å£å†…å®¹å’ŒåŸå§‹ä¸­å¿ƒå¥å­
    #             window_content = node.metadata.get("window", "N/A - Window content missing")
    #             original_text = node.metadata.get("original_text", "N/A - Original text missing")
    #             print(f"ã€å¬å›çš„çª—å£ä¸Šä¸‹æ–‡ã€‘:\n{window_content}")
    #             print(f"\nã€çª—å£ä¸­çš„æ ¸å¿ƒå¥å­ã€‘:\n{original_text}")
    #             # node.get_content() åœ¨è¿™é‡Œé€šå¸¸æ˜¯æ ¸å¿ƒå¥å­
    #             # print(f"\nã€Node Content (é€šå¸¸æ˜¯æ ¸å¿ƒå¥å­)ã€‘:\n{node.get_content()}") 
    #         else:
    #             # å¯¹äºå…¶ä»–åˆ‡ç‰‡å™¨ï¼Œç›´æ¥æ‰“å°èŠ‚ç‚¹å†…å®¹
    #             print(f"ã€å¬å›çš„æ–‡æ¡£å†…å®¹ã€‘:\n{node.get_content()}")
    #         # --- æ‰“å°é€»è¾‘ç»“æŸ ---

    #         print("-" * 50) # åˆ†éš”çº¿

    # else:
    #     print("æœªå¬å›ä»»ä½•æ–‡æ¡£ç‰‡æ®µã€‚è¯·æ£€æŸ¥æ–‡æ¡£å†…å®¹æˆ–åˆ‡ç‰‡ç­–ç•¥ã€‚")

    print(f"\n{splitter_name} æµ‹è¯•å®Œæˆã€‚")
    print(f"{'='*50}\n")

# --- ç¤ºä¾‹æ–‡æ¡£å’Œé—®é¢˜ ---
documents = [
    Document(
        text="""
        LlamaIndex æ˜¯ä¸€ä¸ªç”¨äºæ„å»º LLM åº”ç”¨ç¨‹åºçš„æ•°æ®æ¡†æ¶ã€‚
        å®ƒæä¾›äº†ä¸€å¥—å·¥å…·ï¼Œå¯ä»¥å¸®åŠ©å¼€å‘è€…å°†ç§æœ‰æ•°æ®ä¸å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿æ¥èµ·æ¥ï¼Œ
        å®ç°åŒ…æ‹¬é—®ç­”ã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç­‰åŠŸèƒ½ã€‚
        LlamaIndex æ”¯æŒå¤šç§æ•°æ®æºï¼ŒåŒ…æ‹¬ PDFã€æ•°æ®åº“ã€API ç­‰ã€‚
        å…¶æ ¸å¿ƒæ¦‚å¿µåŒ…æ‹¬æ–‡æ¡£åŠ è½½å™¨ã€èŠ‚ç‚¹è§£æå™¨ã€ç´¢å¼•å’ŒæŸ¥è¯¢å¼•æ“ã€‚

        æ–‡æ¡£åŠ è½½å™¨è´Ÿè´£å°†å„ç§æ ¼å¼å’Œæ¥æºçš„æ•°æ®æ‘„å–åˆ° LlamaIndex ä¸­ã€‚
        èŠ‚ç‚¹è§£æå™¨éšåå°†è¿™äº›åŠ è½½çš„æ–‡æ¡£åˆ†è§£æˆæ›´å°ã€æ›´æ˜“äºç®¡ç†çš„å•å…ƒï¼Œç§°ä¸ºèŠ‚ç‚¹ã€‚
        è¿™äº›èŠ‚ç‚¹é€šå¸¸æ˜¯å¥å­æˆ–æ®µè½ï¼Œå…·ä½“å–å†³äºè§£æç­–ç•¥ã€‚
        ç´¢å¼•æ˜¯æ„å»ºåœ¨è¿™äº›èŠ‚ç‚¹ä¹‹ä¸Šçš„æ•°æ®ç»“æ„ï¼Œæ—¨åœ¨å®ç°é«˜æ•ˆå­˜å‚¨å’Œæ£€ç´¢ï¼Œ
        é€šå¸¸æ¶‰åŠå‘é‡åµŒå…¥ä»¥è¿›è¡Œè¯­ä¹‰æœç´¢ã€‚
        æœ€åï¼ŒæŸ¥è¯¢å¼•æ“ä¿ƒè¿›äº†ä¸ç´¢å¼•æ•°æ®çš„äº¤äº’ï¼Œå…è®¸ç”¨æˆ·æå‡ºé—®é¢˜
        å¹¶åˆ©ç”¨ LLM å’Œæ£€ç´¢åˆ°çš„ä¿¡æ¯åˆæˆç­”æ¡ˆã€‚

        --- ä»¥ä¸‹æ˜¯ä¸ LlamaIndex ä¸»é¢˜ä¸å¤ªç›´æ¥ç›¸å…³çš„å†…å®¹ ---

        æ­¤å¤–ï¼ŒPython ä½œä¸ºä¸€é—¨é€šç”¨ç¼–ç¨‹è¯­è¨€ï¼Œå…¶ç®€æ´æ€§å’Œä¸°å¯Œçš„åº“ç”Ÿæ€ä½¿å…¶åœ¨ AI é¢†åŸŸå¹¿å—æ¬¢è¿ã€‚
        ä¾‹å¦‚ï¼ŒNumPy å’Œ Pandas æ˜¯æ•°æ®å¤„ç†çš„åŸºç¡€ï¼Œå®ƒä»¬æä¾›äº†å¼ºå¤§çš„å·¥å…·ç”¨äºæ•°å€¼æ“ä½œå’Œç»“æ„åŒ–æ•°æ®ã€‚
        Scikit-learn åˆ™æä¾›äº†å…¨é¢çš„æœºå™¨å­¦ä¹ ç®—æ³•å¥—ä»¶ï¼Œé€‚ç”¨äºåˆ†ç±»ã€å›å½’å’Œèšç±»ç­‰ä»»åŠ¡ã€‚
        è¿™äº›å·¥å…·å…±åŒæ„æˆäº†æ•°æ®ç§‘å­¦å®¶å’Œ AI ä»ä¸šè€…çš„å¼ºå¤§å·¥å…·ç®±ï¼Œ
        ä½¿ä»–ä»¬èƒ½å¤Ÿé«˜æ•ˆåœ°å¼€å‘å’Œéƒ¨ç½²å¤æ‚çš„ AI æ¨¡å‹ã€‚

        --- ä»¥ä¸‹æ˜¯å¦ä¸€ä¸ªç›¸å…³ä½†æ¦‚å¿µä¸Šç‹¬ç«‹çš„éƒ¨åˆ† ---

        å¥å­çª—å£åˆ‡ç‰‡æ˜¯ä¸€ç§é«˜çº§çš„åˆ‡ç‰‡ç­–ç•¥ï¼Œå®ƒåœ¨æ¯ä¸ªåˆ‡ç‰‡ä¸­åŒ…å«ä¸€ä¸ªç›®æ ‡å¥å­ï¼Œ
        å¹¶åœ¨å…¶å‰åæ·»åŠ ä¸€å®šæ•°é‡çš„â€œçª—å£â€å¥å­ä½œä¸ºä¸Šä¸‹æ–‡ã€‚
        è¿™ç§æ–¹æ³•æ—¨åœ¨æ£€ç´¢æ—¶ä¸º LLM æä¾›ä¸°å¯Œçš„å±€éƒ¨ä¸Šä¸‹æ–‡ï¼Œä»è€Œæé«˜ç”Ÿæˆç­”æ¡ˆçš„è¿è´¯æ€§ã€‚
        è¯­ä¹‰åˆ‡ç‰‡åˆ™å°è¯•æ ¹æ®æ–‡æœ¬çš„è¯­ä¹‰å†…å®¹æ¥åˆ’åˆ†æ®µè½ï¼Œ
        è€Œä¸æ˜¯ä»…ä»…ä¾é å›ºå®šçš„å­—ç¬¦æ•°æˆ–å¥å­æ•°é‡ã€‚
        å®ƒåˆ©ç”¨åµŒå…¥æ¨¡å‹è®¡ç®—å¥å­æˆ–çŸ­è¯­ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œ
        è¯†åˆ«å‡ºä¸»é¢˜æˆ–å«ä¹‰å‘ç”Ÿè‡ªç„¶è½¬å˜çš„æ–­ç‚¹ã€‚
        è¿™ä¸¤ç§é«˜çº§æ–¹æ³•éƒ½èƒ½æœ‰æ•ˆæå‡ RAG åº”ç”¨çš„å¬å›å’Œç”Ÿæˆè´¨é‡ã€‚
        é€‰æ‹©æ­£ç¡®çš„åˆ‡ç‰‡ç­–ç•¥é€šå¸¸å–å†³äºæ•°æ®çš„å…·ä½“ç‰¹å¾å’Œé¢„æœŸçš„æŸ¥è¯¢ç±»å‹ã€‚
        """
    )
]

question = "LlamaIndex çš„ä¸»è¦åŠŸèƒ½å’Œæ ¸å¿ƒæ¦‚å¿µæ˜¯ä»€ä¹ˆï¼Ÿä»¥åŠä¸¤ç§é«˜çº§åˆ‡ç‰‡ç­–ç•¥çš„åŒºåˆ«ï¼Ÿ"

# --- å¼€å§‹æµ‹è¯•ä¸åŒçš„åˆ‡ç‰‡ç­–ç•¥ ---
# Token åˆ‡ç‰‡ (Character/Token-based)
token_splitter = TokenTextSplitter(
    chunk_size=30, # Small chunk size to demonstrate forced breaks
    chunk_overlap=0 # No overlap for clear distinct chunks
)
evaluate_splitter(token_splitter, documents, question, "Token åˆ‡ç‰‡ (chunk_size=30)")

token_splitter = TokenTextSplitter(
    chunk_size=30, # Small chunk size to demonstrate forced breaks
    chunk_overlap=10 # No overlap for clear distinct chunks
)
evaluate_splitter(token_splitter, documents, question, "Token åˆ‡ç‰‡ (chunk_size=30,chunk_overlap=10 )")


# æ–‡æ¡£åŸæ–‡: [.....å†…å®¹X..... | å†…å®¹Y | .....å†…å®¹Z.....]
#                      â†‘           â†‘
#                      åˆ‡å—Aæœ«å°¾    åˆ‡å—Bèµ·å§‹ç‚¹ (ä»åˆ‡å—Aæœ«å°¾å¾€å‰50ä¸ªå­—ç¬¦)

# åˆ‡å—A: [.....å†…å®¹X..... | å†…å®¹Y] (512ä¸ªå­—ç¬¦)
# åˆ‡å—B:                 [å†…å®¹Y | .....å†…å®¹Z.....] (512ä¸ªå­—ç¬¦)
