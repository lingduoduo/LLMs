import os
from llama_index.core import VectorStoreIndex, Settings, Document
from llama_index.core.node_parser import (
    SentenceWindowNodeParser,
    SemanticSplitterNodeParser,
    TokenTextSplitter,
    SentenceSplitter,
)
from llama_index.core.postprocessor import MetadataReplacementPostProcessor

# Import OpenAILike LLM (DashScope compatible mode for Qwen models)
from llama_index.llms.openai_like import OpenAILike

# Import DashScopeEmbedding (Alibaba Cloud DashScope embedding models)
from llama_index.embeddings.dashscope import DashScopeEmbedding

# Get API key
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY")

# ------------------------------------------------------
# Initialize global LlamaIndex settings
# ------------------------------------------------------

# 1) Configure LLM (use DashScope qwen-plus via OpenAILike)
Settings.llm = OpenAILike(
    model="qwen-plus",
    api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
    api_key=DASHSCOPE_API_KEY,
    is_chat_model=True,
    temperature=0.1,  # Controls randomness of generation
)

# 2) Configure embedding model (DashScopeEmbedding)
Settings.embed_model = DashScopeEmbedding(
    model_name="text-embedding-v4",
    api_key=DASHSCOPE_API_KEY,
)


def evaluate_splitter(splitter, documents, question, splitter_name):
    """
    Evaluate different document chunking strategies.

    This function prints the raw chunks generated by the splitter
    so you can compare splitting quality directly.
    """
    print(f"\n{'=' * 50}")
    print(f"Testing splitter: {splitter_name} ...")
    print(f"{'=' * 50}\n")

    # Show raw chunks generated by the splitter
    print(f"[{splitter_name}] Generated raw document chunks (Nodes):")
    raw_nodes = splitter.get_nodes_from_documents(documents)
    for i, node in enumerate(raw_nodes, 1):
        print(f"\n   Chunk {i}:")
        if isinstance(splitter, SentenceWindowNodeParser):
            # For Sentence Window, show the core sentence and the full window context.
            original_text = node.metadata.get("original_text", node.get_content())
            window_context = node.metadata.get("window", "N/A - window context not generated")
            print(f'   Core sentence: "{original_text}"')
            print(f'   Full window context (for LLM): "{window_context}"')
        else:
            print(f'   Content: "{node.get_content()}"')

        # Uncomment if you want metadata debugging
        # print(f"   Metadata: {node.metadata}")
        print("   " + "-" * 40)

    print("\n" + "=" * 50)

    # --------------------------------------------------
    # Optional: Build index and run retrieval + QA
    # --------------------------------------------------
    # print("Building index...")
    # nodes = splitter.get_nodes_from_documents(documents)
    # index = VectorStoreIndex(nodes, embed_model=Settings.embed_model)
    #
    # print("Creating query engine...")
    # query_engine_params = {"similarity_top_k": 5, "streaming": True}
    #
    # if isinstance(splitter, SentenceWindowNodeParser):
    #     query_engine_params["node_postprocessors"] = [
    #         MetadataReplacementPostProcessor(target_metadata_key="window")
    #     ]
    #     print("Sentence Window detected — MetadataReplacementPostProcessor added.")
    #
    # query_engine = index.as_query_engine(**query_engine_params)
    #
    # print(f"\nTest question: {question}")
    # print("\nModel response:")
    # response = query_engine.query(question)
    # response.print_response_stream()

    print(f"\n{splitter_name} test completed.")
    print(f"{'=' * 50}\n")


# ------------------------------------------------------
# Example document and question
# ------------------------------------------------------
documents = [
    Document(
        text="""
        LlamaIndex is a data framework for building LLM applications.
        It provides a suite of tools to help developers connect private data with
        large language models (LLMs), enabling use cases such as question answering
        and Retrieval-Augmented Generation (RAG).
        LlamaIndex supports multiple data sources, including PDFs, databases, and APIs.
        Its core concepts include document loaders, node parsers, indexes, and query engines.

        Document loaders ingest data from various formats and sources into LlamaIndex.
        Node parsers then break the loaded documents into smaller, more manageable units
        called nodes. These nodes are typically sentences or paragraphs, depending on
        the parsing strategy.
        Indexes are data structures built on top of these nodes to enable efficient
        storage and retrieval, typically involving vector embeddings for semantic search.
        Finally, query engines enable interaction with the indexed data, allowing users
        to ask questions and synthesize answers using both the LLM and retrieved context.

        --- The following content is less directly related to LlamaIndex ---

        Python is a general-purpose programming language widely used in AI due to its
        simplicity and rich ecosystem. For example, NumPy and Pandas are foundational
        libraries for data processing and numerical computing.
        Scikit-learn provides a comprehensive suite of machine learning algorithms for
        classification, regression, and clustering tasks.
        Together, these tools form a powerful toolbox for data scientists and AI practitioners,
        enabling efficient development and deployment of complex AI models.

        --- Another related but conceptually independent section ---

        Sentence Window chunking is an advanced chunking strategy that includes a target sentence
        in each chunk and adds a certain number of “window” sentences before and after it
        as context.
        This approach provides richer local context for the LLM during retrieval, improving
        the coherence of generated answers.
        Semantic chunking attempts to split text based on semantic meaning rather than fixed
        character counts or sentence counts.
        It uses embeddings to compute semantic similarity between sentences or phrases and
        detects natural breakpoints where the topic or meaning shifts.
        Both advanced methods can improve retrieval and generation quality in RAG applications.
        Choosing the right chunking strategy typically depends on the characteristics of your
        data and the expected query types.
        """
    )
]

question = (
    "What are the main functionalities and core concepts of LlamaIndex, "
    "and what is the difference between the two advanced chunking strategies?"
)

# ------------------------------------------------------
# Test different chunking strategies
# ------------------------------------------------------

# Sentence Window chunking
sentence_window_splitter = SentenceWindowNodeParser.from_defaults(
    window_size=3,
    window_metadata_key="window",
    original_text_metadata_key="original_text",
)
evaluate_splitter(sentence_window_splitter, documents, question, "Sentence Window")
