import os
from llama_index.core import VectorStoreIndex, Settings, Document
from llama_index.core.node_parser import SentenceWindowNodeParser, SemanticSplitterNodeParser, TokenTextSplitter, SentenceSplitter
from llama_index.core.postprocessor import MetadataReplacementPostProcessor

# 导入 OpenAILike LLM (用于 DashScope 兼容模式，Qwen 模型)
from llama_index.llms.openai_like import OpenAILike 
# 导入 DashScopeEmbedding (用于阿里云 DashScope 嵌入模型)
from llama_index.embeddings.dashscope import DashScopeEmbedding 

# 获取 API Key
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY")

# 初始化 LlamaIndex 全局设置

# 1. 配置 LLM (使用 DashScope 的 qwen-plus 模型，通过 OpenAILike 调用)
Settings.llm = OpenAILike(
    model="qwen-plus",
    api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
    api_key=DASHSCOPE_API_KEY,
    is_chat_model=True,
    temperature=0.1 # 添加温度参数，用于控制生成随机性
) 

# 2. 配置嵌入模型 (使用 DashScopeEmbedding 类)
Settings.embed_model = DashScopeEmbedding(
    model_name="text-embedding-v4", 
    api_key=DASHSCOPE_API_KEY,     
)

def evaluate_splitter(splitter, documents, question, splitter_name):
    """
    评测不同文档切片方法的效果
    手动打印召回结果，方便直接对比切分效果。
    """
    print(f"\n{'='*50}")
    print(f"正在使用 {splitter_name} 方法进行测试...")
    print(f"{'='*50}\n")

    # 显示 raw chunks generated by the splitter
    print(f"【{splitter_name}】生成的原始文档切片 (Nodes):")
    raw_nodes = splitter.get_nodes_from_documents(documents)
    for i, node in enumerate(raw_nodes, 1):
        print(f"\n   切片 {i}:")
        if isinstance(splitter, SentenceWindowNodeParser):
            # For Sentence Window, show the original text that forms the 'core' of the chunk
            # and indicate what its full window context would be.
            original_text = node.metadata.get("original_text", node.get_content())
            window_context = node.metadata.get("window", "N/A - 窗口内容未生成")
            print(f"   核心内容: \"{original_text}\"")
            print(f"   完整窗口上下文(供LLM用): \"{window_context}\"")
        else:
            print(f"   内容: \"{node.get_content()}\"")
        # Add metadata for debugging if needed
        # print(f"   元数据: {node.metadata}")
        print("   " + "-" * 40)
    print("\n" + "="*50)

    # # 构建索引
    # print("正在处理文档并构建索引...")
    # nodes = splitter.get_nodes_from_documents(documents) 

    # index = VectorStoreIndex(nodes, embed_model=Settings.embed_model)

    # # 创建查询引擎
    # print("正在创建查询引擎...")
    # query_engine_params = {
    #     "similarity_top_k": 5,
    #     "streaming": True
    # }

    # # 如果是 Sentence Window 切片，添加后处理器
    # if isinstance(splitter, SentenceWindowNodeParser):
    #     query_engine_params["node_postprocessors"] = [
    #         MetadataReplacementPostProcessor(target_metadata_key="window")
    #     ]
    #     print("检测到 Sentence Window 切片，已添加 MetadataReplacementPostProcessor。")
    
    # query_engine = index.as_query_engine(**query_engine_params)

    # # 执行查询
    # print(f"\n测试问题: {question}")
    # print("\n模型回答:")
    # response = query_engine.query(question)
    # response.print_response_stream()

    # # 输出召回的参考片段
    # print(f"\n{splitter_name} 召回的参考片段:")
    # if response.source_nodes:
    #     for i, node in enumerate(response.source_nodes, 1):
    #         print(f"\n--- 文档片段 {i} ---")
            
    #         # --- 优化后的打印逻辑 ---
    #         if isinstance(splitter, SentenceWindowNodeParser):
    #             # 对于Sentence Window，打印出窗口内容和原始中心句子
    #             window_content = node.metadata.get("window", "N/A - Window content missing")
    #             original_text = node.metadata.get("original_text", "N/A - Original text missing")
    #             print(f"【召回的窗口上下文】:\n{window_content}")
    #             print(f"\n【窗口中的核心句子】:\n{original_text}")
    #             # node.get_content() 在这里通常是核心句子
    #             # print(f"\n【Node Content (通常是核心句子)】:\n{node.get_content()}") 
    #         else:
    #             # 对于其他切片器，直接打印节点内容
    #             print(f"【召回的文档内容】:\n{node.get_content()}")
    #         # --- 打印逻辑结束 ---

    #         print("-" * 50) # 分隔线

    # else:
    #     print("未召回任何文档片段。请检查文档内容或切片策略。")

    print(f"\n{splitter_name} 测试完成。")
    print(f"{'='*50}\n")

# --- 示例文档和问题 ---
documents = [
    Document(
        text="""
        LlamaIndex 是一个用于构建 LLM 应用程序的数据框架。
        它提供了一套工具，可以帮助开发者将私有数据与大型语言模型（LLMs）连接起来，
        实现包括问答、检索增强生成（RAG）等功能。
        LlamaIndex 支持多种数据源，包括 PDF、数据库、API 等。
        其核心概念包括文档加载器、节点解析器、索引和查询引擎。

        文档加载器负责将各种格式和来源的数据摄取到 LlamaIndex 中。
        节点解析器随后将这些加载的文档分解成更小、更易于管理的单元，称为节点。
        这些节点通常是句子或段落，具体取决于解析策略。
        索引是构建在这些节点之上的数据结构，旨在实现高效存储和检索，
        通常涉及向量嵌入以进行语义搜索。
        最后，查询引擎促进了与索引数据的交互，允许用户提出问题
        并利用 LLM 和检索到的信息合成答案。

        --- 以下是与 LlamaIndex 主题不太直接相关的内容 ---

        此外，Python 作为一门通用编程语言，其简洁性和丰富的库生态使其在 AI 领域广受欢迎。
        例如，NumPy 和 Pandas 是数据处理的基础，它们提供了强大的工具用于数值操作和结构化数据。
        Scikit-learn 则提供了全面的机器学习算法套件，适用于分类、回归和聚类等任务。
        这些工具共同构成了数据科学家和 AI 从业者的强大工具箱，
        使他们能够高效地开发和部署复杂的 AI 模型。

        --- 以下是另一个相关但概念上独立的部分 ---

        句子窗口切片是一种高级的切片策略，它在每个切片中包含一个目标句子，
        并在其前后添加一定数量的“窗口”句子作为上下文。
        这种方法旨在检索时为 LLM 提供丰富的局部上下文，从而提高生成答案的连贯性。
        语义切片则尝试根据文本的语义内容来划分段落，
        而不是仅仅依靠固定的字符数或句子数量。
        它利用嵌入模型计算句子或短语之间的语义相似度，
        识别出主题或含义发生自然转变的断点。
        这两种高级方法都能有效提升 RAG 应用的召回和生成质量。
        选择正确的切片策略通常取决于数据的具体特征和预期的查询类型。
        """
    )
]

question = "LlamaIndex 的主要功能和核心概念是什么？以及两种高级切片策略的区别？"

# --- 开始测试不同的切片策略 ---

# 句子窗口切片 (Sentence Window)
sentence_window_splitter = SentenceWindowNodeParser.from_defaults(
    window_size=3,
    window_metadata_key="window",
    original_text_metadata_key="original_text"
)
evaluate_splitter(sentence_window_splitter, documents, question, "Sentence Window")
