{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ae3b4a2",
   "metadata": {},
   "source": [
    "# 多个模型不知选哪个？设计多模型路由机制智能选择最优生成器\n",
    "\n",
    "在当前大语言模型（LLM）应用快速发展的背景下，企业或平台往往部署了多个不同规模、不同架构甚至不同推理引擎的模型（如 Llama3、Qwen、ChatGLM、Baichuan 等），以满足多样化业务需求。然而，当后端存在 **多个模型实例**时，如何高效、稳定、智能地将用户请求路由到“最合适”的模型上，成为提升系统性能与用户体验的关键挑战。\n",
    "\n",
    "传统的负载均衡策略（如轮询、随机、最少连接）已难以应对 LLM 推理场景中的复杂性：  \n",
    "- 不同请求的 Prompt 长度差异巨大；  \n",
    "- 输出 Token 数具有高度随机性；  \n",
    "- GPU 显存占用和计算负载动态变化；  \n",
    "- 模型间推理速度、成本、精度存在显著差异。\n",
    "\n",
    "为此，我们引入 **LLM 智能路由机制**——一种基于实时指标感知与动态调度的多模型路由方案，实现“智能选模”，最大化集群资源利用率与服务稳定性。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5300e07f",
   "metadata": {},
   "source": [
    "## 一、为什么需要智能路由？传统方案的局限\n",
    "\n",
    "在面对多个模型实例时，常见的做法是使用 Nginx 或 Kubernetes Service 做简单的负载均衡。但这些方法在 LLM 场景下面临以下问题：\n",
    "\n",
    "| 问题 | 描述 |\n",
    "|------|------|\n",
    "| ❌ 负载不均 | 某些实例因处理长文本请求而长时间占用显存，其他实例却空闲。 |\n",
    "| ❌ 响应延迟高 | 将大请求发给已满载的实例，导致排队甚至超时。 |\n",
    "| ❌ 无法感知模型特性 | 不能根据模型能力（如是否支持 prefix caching）进行优化调度。 |\n",
    "| ❌ 容错能力弱 | 实例异常无法及时发现，流量仍被转发导致失败率上升。 |\n",
    "\n",
    "因此，我们需要一个 **能感知后端状态、具备决策能力、支持自动容灾** 的智能路由系统。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cca9ec",
   "metadata": {},
   "source": [
    "## 二、LLM 智能路由架构详解\n",
    "\n",
    "所以，引入 **LLM 智能路由基础组件**，专为解决上述问题而设计。它由三大核心模块构成，形成完整的“感知—决策—执行”闭环：\n",
    "\n",
    "![router](./22/路由架构.png)\n",
    "\n",
    "\n",
    "### 1. LLM Gateway：流量入口与缓冲中枢\n",
    "\n",
    "作为整个系统的前端网关，LLM Gateway 承担着流量接入与初步管理职责。\n",
    "\n",
    "#### 核心功能：\n",
    "-  **多协议支持**：支持 HTTP（含 SSE 流式响应）和 WebSocket，适配各类前端应用。\n",
    "-  **请求缓冲与限流**：当所有后端实例负载较高时，新请求不会立即被拒绝，而是暂存于网关内部队列，等待最佳调度时机。\n",
    "-  **故障转移**：网关通常部署多个实例，单点故障不影响整体服务可用性。\n",
    "\n",
    "\n",
    "### 2. LLM Scheduler：智能调度大脑\n",
    "\n",
    "这是整个智能路由系统的“决策中心”。它不再盲目轮询，而是根据实时反馈的指标数据，选择当前最合适的推理实例。\n",
    "\n",
    "#### 调度策略亮点：\n",
    "-  **基于负载指标动态选择**：\n",
    "  - 等待队列长度（Waiting Queue Length）\n",
    "  - KV Cache 使用率（直接影响并发能力）\n",
    "  - 输入/输出 Token 吞吐量（TPS）\n",
    "  - 实例健康状态\n",
    "-  **支持 Prefix 匹配调度**（需后端开启 prefix-caching）：\n",
    "  - 若请求的 Prompt 前缀与某实例缓存匹配，则优先调度至该实例，大幅降低重复计算开销。\n",
    "-  **加权评分模型**：\n",
    "  - 对各实例打分，综合考虑响应延迟预测、资源余量、历史表现等，选出“性价比最高”的目标。\n",
    "\n",
    "### 3. LLM Agent：实例级“哨兵”\n",
    "\n",
    "每个推理实例旁都部署一个轻量级 LLM Agent，负责本地监控与上报。\n",
    "\n",
    "#### 主要职责：\n",
    "-  **指标采集与上报**：\n",
    "  - 实时采集 waiting 队列长度、KV Cache 占用、GPU 利用率等关键指标。\n",
    "  - 主动推送至 LLM Scheduler，确保调度决策基于最新状态。\n",
    "-  **健康检查与保活**：\n",
    "  - 与 Scheduler 保持心跳，一旦检测到推理进程崩溃或无响应，立即通知 Scheduler 将该实例“摘除”。\n",
    "  - 恢复后自动重新注册，无缝回归服务池。\n",
    "-  **自动识别推理框架**：\n",
    "  - 支持 Blade-LLM、vLLM、SGLang 等主流推理引擎，无需手动配置类型，降低运维复杂度。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c749dc9c",
   "metadata": {},
   "source": [
    "## 三、智能路由工作流程详解\n",
    "\n",
    "以下是用户请求从进入系统到完成推理的完整链路：\n",
    "\n",
    "![route](./22/路由工作流程.png)\n",
    "```mermaid\n",
    "sequenceDiagram\n",
    "    participant User\n",
    "    participant Gateway\n",
    "    participant Scheduler\n",
    "    participant Agent\n",
    "    participant Engine\n",
    "\n",
    "    User->>Gateway: 发送请求（/v1/chat/completions）\n",
    "    Gateway->>Scheduler: 查询最佳实例（携带请求元信息）\n",
    "    Scheduler->>Agent: 获取各实例实时指标\n",
    "    Agent-->>Scheduler: 上报 KV Cache、队列、负载等\n",
    "    Scheduler->>Gateway: 返回最优实例地址\n",
    "    alt 实例可用\n",
    "        Gateway->>Engine: 转发请求\n",
    "        Engine->>User: 返回流式或完整响应\n",
    "    else 实例过载\n",
    "        Gateway->>Gateway: 缓存请求，等待调度窗口\n",
    "    end\n",
    "```\n",
    "\n",
    "### 关键步骤说明：\n",
    "\n",
    "1. **注册阶段**：  \n",
    "   推理服务启动后，LLM Agent 检测到引擎 ready，主动向 LLM Scheduler 注册，并开始周期性上报指标。\n",
    "\n",
    "2. **接入阶段**：  \n",
    "   用户请求通过标准 OpenAI 类接口发送至 LLM Gateway。\n",
    "\n",
    "3. **调度阶段**：  \n",
    "   Gateway 将请求特征（如 prompt 长度、是否流式）发送给 Scheduler，Scheduler 结合实时指标评估所有候选实例，返回最优目标。\n",
    "\n",
    "4. **转发执行**：  \n",
    "   Gateway 将请求转发至指定推理实例，返回结果给用户。\n",
    "\n",
    "5. **异步更新**：  \n",
    "   Agent 持续采集运行数据，确保下一次调度更精准。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea10209",
   "metadata": {},
   "source": [
    "## 四、Failover 容灾机制：保障高可用\n",
    "\n",
    "智能路由不仅追求“最优”，更要保证“不断”。以下是各组件的容错设计：\n",
    "\n",
    "| 组件 | 容错机制 |\n",
    "|------|----------|\n",
    "| **LLM Gateway** | 多实例部署，前端通过 DNS 或 SLB 实现故障转移，单实例宕机不影响整体流量接入。 |\n",
    "| **LLM Scheduler** | 当前为单实例设计（避免脑裂），若宕机，LLM Gateway 自动降级为 **轮询模式**，保障请求不断；恢复后自动切回智能调度。 |\n",
    "| **LLM Agent** | 故障时，Scheduler 在心跳超时后将其关联实例从服务列表中剔除，避免无效调度。 |\n",
    "| **推理引擎** | Agent 可快速感知进程异常（如 OOM、死锁），立即摘除实例；恢复后重新纳入调度池。 |\n",
    "\n",
    "> 💡 这种“优雅降级 + 自动恢复”机制，极大提升了系统的鲁棒性。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e874a7",
   "metadata": {},
   "source": [
    "\n",
    "## 五、实际应用场景举例\n",
    "\n",
    "### 场景 1：长短请求混合场景\n",
    "- 有些用户输入几百字，有些仅几个词。\n",
    "- 智能路由可将短请求优先调度至正在处理长请求的实例（利用其已加载的 KV Cache），提升缓存命中率。\n",
    "- 长请求则分配给负载较低、显存充足的实例，避免阻塞。\n",
    "\n",
    "### 场景 2：多模型异构集群\n",
    "- 同时运行 Qwen-72B（高性能）、Qwen-7B（低成本）、Llama3-8B（通用）等多个模型。\n",
    "- 调度器可根据请求优先级、预算限制、响应时间要求，选择最适合的模型。\n",
    "- 例如：VIP 用户请求 → 调度至 Qwen-72B；普通问答 → 调度至 Qwen-7B。\n",
    "\n",
    "### 场景 3：突发流量洪峰\n",
    "- 流量激增时，部分实例 waiting 队列积压。\n",
    "- 智能路由自动避开高负载节点，将请求导向空闲实例，防止雪崩。\n",
    "- 同时 Gateway 缓冲机制防止直接拒绝用户请求。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efe9aa4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 六、总结：智能路由的价值\n",
    "\n",
    "| 维度 | 传统负载均衡 | LLM 智能路由 |\n",
    "|------|---------------|----------------|\n",
    "| 调度依据 | 固定规则 | 实时指标 + 动态评估 |\n",
    "| 资源利用率 | 低（易出现负载倾斜） | 高（均衡分配） |\n",
    "| 响应延迟 | 波动大 | 更稳定可控 |\n",
    "| 缓存利用 | 无感知 | 支持 prefix 匹配优化 |\n",
    "| 容错能力 | 弱 | 强（自动摘除 + 降级） |\n",
    "| 多模型支持 | 差 | 原生支持 |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
