{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 动态切片策略与重叠机制提升RAG召回率\n",
    "- 本讲重点：探讨主流RAG框架（LangChain, LlamaIndex, Dify）如何结合动态切片与重叠机制提升召回能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 碎片化原因\n",
    "- **连贯性丧失：** 语义中断，大模型理解受损。\n",
    "- **相关性稀释：** 关键信息被“稀释”，检索排名下降。\n",
    "- **信息分散：** 多跳推理信息不全，答案不完整。\n",
    "- **结果：** “垃圾进，垃圾出”，增**“幻觉”**风险。\n",
    "\n",
    "---\n",
    "\n",
    "## 策略：动态切片 + 重叠机制\n",
    "\n",
    "### 动态切片：源头避免碎片化\n",
    "- **定义：** 智能、自适应切分，根据语义、结构、主题定切分点。\n",
    "- **类型：**\n",
    "    - **内容感知型：**\n",
    "        - **语义切片：** 识别语义断点。\n",
    "        - **主题切片：** 按主题切分。\n",
    "        - **优势：** 减少语义割裂，提高连贯性。\n",
    "    - **结构感知型：**\n",
    "        - **布局感知切片：** 利用PDF、HTML结构元素。\n",
    "        - **文档特异性切片：** 针对特定格式（如Markdown）。\n",
    "        - **优势：** 保留文档结构。\n",
    "    - **高级自适应型：**\n",
    "        - **智能体切片：** LLM判断切分边界。\n",
    "        - **优势：** 更灵活智能。\n",
    "\n",
    "### 重叠机制：非动态策略缓冲\n",
    "- **核心：** 相邻区块保留重复内容，形成“滑动窗口”。\n",
    "- **作用：**\n",
    "    - 局部语义保持。\n",
    "    - 缓解信息缺失。\n",
    "    - 提升召回率。\n",
    "- **建议：** 区块大小的 **10%-20%**。\n",
    "\n",
    "---\n",
    "\n",
    "## 协同效果\n",
    "- 动态切片缓解碎片化，重叠机制作补充，增强上下文完整性。\n",
    "\n",
    "---\n",
    "\n",
    "## 最佳实践\n",
    "- **结构化文档：** 优先结构感知切片。\n",
    "- **纯文本：** 推荐语义切片（如LlamaIndex的`SemanticSplitterNodeParser`）。\n",
    "- **尝试：** 父子模式。\n",
    "- **重叠机制：** 必要时启用，控制在 **10%-20%**。\n",
    "- **持续优化：** 建立评估体系。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 固定大小文本切块（递归方法）及其改进\n",
    "\n",
    "### 主要问题\n",
    "- **上下文割裂：** 机械截断文本，破坏语义。\n",
    "- **语义完整性受损：** 区块内句意不完整，影响匹配精度和大模型回答质量。\n",
    "\n",
    "### 改进策略\n",
    "- **引入重叠机制（Overlap）：** 相邻块保留重复内容（如50字符），确保连贯性。\n",
    "- **智能截断：** 尽量在标点符号或段落结束处切分。\n",
    "### 实践工具：LangChain 的 RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain llama-index \n",
    "! pip install llama-index-llms-openai llamaindex-py-client\n",
    "! pip install langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Why does chunking cause context fragmentation?'\n",
      "page_content='1. Loss of Coherence'\n",
      "page_content='When a complete semantic unit is forcibly split,'\n",
      "page_content='lit, the information becomes incomplete'\n",
      "page_content='(the meaning is broken)'\n",
      "page_content='. For example, if an argument is distributed acros'\n",
      "page_content='across two chunks,'\n",
      "page_content='neither chunk alone can accurately convey the ori'\n",
      "page_content='e original meaning'\n",
      "page_content='. This interferes with'\n",
      "page_content='the language model’s understanding and generation'\n",
      "page_content='ation, leading to incomplete or even'\n",
      "page_content='misleading outputs.\n",
      "\n",
      "2. Diluted Relevance'\n",
      "page_content='If a chunk mixes relevant and irrelevant content,'\n",
      "page_content='tent, the key information becomes diluted'\n",
      "page_content='.'\n",
      "page_content='This negatively affects the accuracy of vector re'\n",
      "page_content='or representations and, in turn, lowers'\n",
      "page_content='retrieval ranking performance.'\n",
      "page_content='3. Scattered Information'\n",
      "page_content='For complex questions that require multi-hop reas'\n",
      "page_content='reasoning, relevant information may be'\n",
      "page_content='scattered across multiple chunks'\n",
      "page_content='. If not all of them are retrieved, a RAG system c'\n",
      "page_content='tem cannot'\n",
      "page_content='produce a complete answer.'\n",
      "page_content='When these issues compound, they directly lead to'\n",
      "page_content='ad to a “garbage in, garbage out” effect,'\n",
      "page_content='and may even increase the risk of model hallucina'\n",
      "page_content='ucinations'\n",
      "page_content='.'\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=5,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\", \".\", \"\"],\n",
    ")\n",
    "\n",
    "text = \"\"\"\n",
    "Why does chunking cause context fragmentation?\n",
    "\n",
    "1. Loss of Coherence\n",
    "When a complete semantic unit is forcibly split, the information becomes incomplete\n",
    "(the meaning is broken). For example, if an argument is distributed across two chunks,\n",
    "neither chunk alone can accurately convey the original meaning. This interferes with\n",
    "the language model’s understanding and generation, leading to incomplete or even\n",
    "misleading outputs.\n",
    "\n",
    "2. Diluted Relevance\n",
    "If a chunk mixes relevant and irrelevant content, the key information becomes diluted.\n",
    "This negatively affects the accuracy of vector representations and, in turn, lowers\n",
    "retrieval ranking performance.\n",
    "\n",
    "3. Scattered Information\n",
    "For complex questions that require multi-hop reasoning, relevant information may be\n",
    "scattered across multiple chunks. If not all of them are retrieved, a RAG system cannot\n",
    "produce a complete answer.\n",
    "\n",
    "When these issues compound, they directly lead to a “garbage in, garbage out” effect,\n",
    "and may even increase the risk of model hallucinations.\n",
    "\"\"\"\n",
    "\n",
    "# Text to be processed\n",
    "documents = text_splitter.create_documents([text])\n",
    "\n",
    "for doc in documents:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实践工具：LangChain 的 RecursiveCharacterTextSplitter\n",
    "- **核心优势：** 利用一组优先级分割符递归切分文本，尽可能保留语义完整。\n",
    "- **工作原理：**\n",
    "    - **初步切分：** 用第一个分隔符（如`\\n`）段落划分。\n",
    "    - **检查长度：** 若某段超`chunk_size`，用下一个分隔符（如`。`）切分。\n",
    "    - **递归处理：** 依次尝试剩余分隔符，直到满足长度。\n",
    "    - **合并优化：** 相邻小块合并后仍小于`chunk_size`，则合并。\n",
    "\n",
    "### 切块策略建议\n",
    "- **根据内容类型选择：**\n",
    "    - **逻辑紧密型：** 尽量保持段落完整性。\n",
    "    - **语义独立型：** 可按句子切分。\n",
    "- **结合向量化模型特性：**\n",
    "    - 模型对长文本处理不佳：适当缩短块长。\n",
    "    - 模型擅长短文本：可适度切分但保留关键上下文。\n",
    "- **关注LLM输入限制：** 控制块长，避免超模型最大输入。\n",
    "- **持续实验与优化：** 无普适最佳实践，需测试验证，建立评估体系。\n",
    "\n",
    "---\n",
    "\n",
    "## 表1：切片策略对RAG指标的量化影响（综合视图）\n",
    "\n",
    "| 切片策略   | 关键评估指标 / 发现与结果                                                                                                                                                                             | 来源 / 备注                                         | 适用场景                               |\n",
    "| :--------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------- | :------------------------------------- |\n",
    "| **页面级切片** | **端到端答案准确率**：达到最高的平均准确率 (0.648)，性能最稳定。                                                                                                                                | NVIDIA研究                                          | 结构化、多页商业文档                   |\n",
    "| **语义切片 (Spacy)** | **上下文相关性、答案相关性、忠实度**：处理不同难度查询时，总体表现优于递归等方法。                                                                                                            | Antematter研究                                      | 富含语义信息的非结构化文本             |\n",
    "| **令牌级切片** | **答案不正确率、平均倒数排名(MRR)**：<br>- 300-500个令牌的区块大小是性能“甜点区”，更大区块性能下降。<br>- 检索数量K=4是延迟与性能的最佳平衡点。 | Arize AI研究                                        | 通用文本切片，需关注模型输入限制       |\n",
    "| **递归字符切片** | **上下文相关性**：表现不佳，无法有效处理文本间的上下文相似性。                                                                                                                                  | Antematter研究                                      | 基础策略，但易破坏语义连贯性             |\n",
    "\n",
    "\n",
    "- **核心洞察：** 不存在普适“最佳”切片策略，最优选择取决于**文档类型**和**查询复杂性**。\n",
    "- **趋势：** 先进生产级RAG系统需**动态切片路由器**，根据文档特征选择策略（如PDF用布局解析器，.txt用语义分割器，.py用代码分割器）。\n",
    "\n",
    "---\n",
    "\n",
    "## 重叠机制的作用与代价\n",
    "\n",
    "### 核心作用\n",
    "- **维护指代关系：** 边界处维持局部语义连贯。\n",
    "- **提升检索准确性：** 增强跨区块信息关联，提高召回率和匹配质量。\n",
    "\n",
    "### 成本与挑战\n",
    "- **存储成本增加：** 向量数据库体积膨胀，索引构建时间延长。\n",
    "- **计算开销上升：** 索引规模增大，增加向量搜索负担，延长查询延迟。\n",
    "- **冗余信息传递：** 大量重叠区块会浪费LLM上下文窗口，不增新信息。\n",
    "- **建议：** 重叠设置为 `chunk_size` 的 **10%-20%**，通过实验调整。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 动态切块：LlamaIndex 的 SemanticSplitterNodeParser\n",
    "\n",
    "固定大小的文本切块在RAG系统中常导致 **“上下文碎片化”**，影响检索和生成质量。为解决此问题，LlamaIndex推出了**SemanticSplitterNodeParser**，这是一款通过语义理解实现智能切分的先进工具。\n",
    "\n",
    "### 核心机制\n",
    "SemanticSplitterNodeParser的核心思想是：**根据文本语义变化点进行动态切分**，而非依赖固定字符数或句法结构。\n",
    "\n",
    "其工作流程如下：\n",
    "- **句子级拆分：** 将文档按句子进行划分。\n",
    "- **组块构建：** 连续多个句子组合成一个“句子组”（由`buffer_size`控制）。\n",
    "- **语义嵌入生成：** 使用指定嵌入模型（如OpenAI或HuggingFace模型）为每个句子组生成向量表示。\n",
    "- **语义相似度计算：** 通过余弦距离衡量相邻句子组之间的语义差异。\n",
    "- **动态切分决策：** 当语义差异超过设定的阈值（由`breakpoint_percentile_threshold`控制），则在此处插入切分点。\n",
    "\n",
    "这种方式确保了每个文本块内部**语义连贯、信息完整**，极大提升了后续检索与生成的效果。\n",
    "\n",
    "### 核心参数解析\n",
    "- **`embed_model`** (BaseEmbedding, 必需):\n",
    "    - 用于生成语义向量的嵌入模型。这是语义比较的基础，其质量直接决定切片效果。\n",
    "- **`buffer_size`** (整数, 默认: 1):\n",
    "    - 评估语义相似性时，组合在一起的句子数量。\n",
    "    - 设为1表示逐句比较；大于1则将多个句子视为一个单元，有助于考虑更广泛的上下文。\n",
    "- **`breakpoint_percentile_threshold`** (整数, 默认: 95):\n",
    "    - 确定切分点的余弦距离百分位阈值。\n",
    "    - **敏感度调节参数：** 较低值（如80）意味着对语义变化更敏感，产生更多、更小的区块；较高值（如98）要求语义变化非常显著才切分，产生更少、更大的区块。\n",
    "\n",
    "### 代码实现示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语义切分后得到 1 个节点：\n",
      "--- 节点 1 ---\n",
      "人工智能（AI）正在彻底改变医疗保健行业。AI算法能够通过分析医学影像，比人类放射科医生更早、更准确地诊断出癌症等疾病。此外，AI在药物研发中也发挥着关键作用，能够预测化合物的有效性，从而大大缩短新药上市的时间。话锋一转，我们来谈谈金融科技（FinTech）。移动支付已经成为全球主流，数字钱包和非接触式支付改变了人们的消费习惯。区块链技术则为跨境支付和资产代币化提供了去中心化的解决方案，有望重塑整个金融体系的底层架构。\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.schema import Document\n",
    "import os\n",
    "\n",
    "# Set API key\n",
    "# os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Example text containing multiple topics\n",
    "multi_theme_text = (\n",
    "    \"Artificial intelligence (AI) is fundamentally transforming the healthcare industry. \"\n",
    "    \"AI algorithms can analyze medical images and diagnose diseases such as cancer earlier \"\n",
    "    \"and more accurately than human radiologists. \"\n",
    "    \"In addition, AI plays a critical role in drug discovery by predicting the effectiveness \"\n",
    "    \"of chemical compounds, significantly reducing the time required to bring new drugs to market. \"\n",
    "    \"Shifting gears, let us talk about financial technology (FinTech). \"\n",
    "    \"Mobile payments have become mainstream worldwide, with digital wallets and contactless \"\n",
    "    \"payments reshaping consumer behavior. \"\n",
    "    \"Blockchain technology provides decentralized solutions for cross-border payments and \"\n",
    "    \"asset tokenization, with the potential to reshape the underlying infrastructure of the \"\n",
    "    \"entire financial system.\"\n",
    ")\n",
    "\n",
    "# Create a document object\n",
    "document = Document(text=multi_theme_text)\n",
    "\n",
    "# Initialize embedding model\n",
    "embed_model = OpenAIEmbedding()\n",
    "\n",
    "# Initialize semantic splitter\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1,\n",
    "    breakpoint_percentile_threshold=90,\n",
    "    embed_model=embed_model,\n",
    ")\n",
    "\n",
    "# Perform semantic splitting\n",
    "nodes = splitter.get_nodes_from_documents([document])\n",
    "\n",
    "# Print results\n",
    "print(f\"Semantic splitting produced {len(nodes)} nodes:\")\n",
    "for i, node in enumerate(nodes):\n",
    "    print(f\"--- Node {i + 1} ---\")\n",
    "    print(node.get_content())\n",
    "    print(\"-\" * 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 实践建议\n",
    "\n",
    "### 1. 嵌入模型选择\n",
    "- **通用文本：** 选`OpenAI`的`text-embedding-ada-002`。\n",
    "- **专业领域：** 用**领域预训练模型**（如`BioBERT`）或**微调模型**。\n",
    "- **注意：** 嵌入模型质量直接影响切分效果。\n",
    "\n",
    "### 2. `buffer_size`设置\n",
    "- **`buffer_size=1`：** 逐句比较，适合语义边界明显文本。\n",
    "- **`buffer_size>1`：** 适合语义渐变的长段落。\n",
    "\n",
    "### 3. 切分敏感度调节\n",
    "- **高敏感度（低阈值，如80）：** 适用于精细切分，如多跳问答。\n",
    "- **低敏感度（高阈值，如98）：** 适用于保留完整段落。\n",
    "\n",
    "---\n",
    "\n",
    "`SemanticSplitterNodeParser`是LlamaIndex中实现语义切片的核心，通过语义理解避免上下文断裂。\n",
    "\n",
    "### 推荐策略\n",
    "- **优先**高质量、领域适配的**嵌入模型**。\n",
    "- **合理设置**`buffer_size`和`breakpoint_percentile_threshold`。\n",
    "- **借助可视化**工具优化。\n",
    "\n",
    "**效果：** 通过语义驱动的动态切片，显著提升RAG系统召回率与生成质量，尤其适用于复杂文本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 超越句子：使用命题（Propositions）进行原子化检索\n",
    "\n",
    "传统切片难满足RAG高精度与完整上下文需求。LlamaIndex引入**“命题”（Propositions）原子化检索**。\n",
    "- 体现“由小到大”思想：细粒度检索提高准确率，结合父区块提供完整上下文。\n",
    "- 目标：协同提升检索精度与生成质量。\n",
    "\n",
    "### 什么是“命题”？\n",
    "- **定义：** 文本中**原子化的、自包含的事实单元**（《Dense X Retrieval》论文）。\n",
    "- **特征：**\n",
    "    - **不可再分：** 不能拆解为更小语义单位。\n",
    "    - **独立表达：** 脱离上下文独立解释事实/概念。\n",
    "    - **自然语言形式：** 简洁明了，无需额外信息。\n",
    "- **示例：** “埃菲尔铁塔位于巴黎，建于1889年。”可提取为：\n",
    "    - “埃菲尔铁塔位于巴黎”\n",
    "    - “埃菲尔铁塔建于1889年”\n",
    "- **作用：** 高精确命题作检索单元，命中后追溯原始父区块，提供LLM完整上下文，确保生成内容准确完整。\n",
    "\n",
    "### LlamaIndex 中的命题化实现方案\n",
    "\n",
    "#### 1. TopicNodeParser：LLM驱动的主题一致性重组器（代价高）\n",
    "- **功能：**\n",
    "    - 利用LLM将段落分解为多个命题。\n",
    "    - 根据主题一致性重组命题，形成新语义区块。\n",
    "- **参数：** `similarity_method`（LLM或嵌入模型判断主题相关性）。\n",
    "- **场景：** 检索精度要求极高、允许高计算成本的场景。\n",
    "- **优势：** 精准捕捉语义边界，深度解析复杂文本。\n",
    "- **代价：** 依赖大模型，推理成本高，适合离线处理。\n",
    "\n",
    "#### 2. DenseXRetrievalPack：开箱即用的命题化解决方案\n",
    "- **核心流程：**\n",
    "    - 自动为知识库节点提取命题。\n",
    "    - 构建专门针对命题的检索器。\n",
    "    - 返回与命题相关的原始父区块用于生成。\n",
    "- **优势：** 快速部署命题化检索，无需手动编写提示词或训练。\n",
    "- **架构：** 结合LLM提取命题 + 向量索引构建 + 递归检索。\n",
    "\n",
    "### 命题提取的实现细节\n",
    "- LlamaIndex和Langchain均有实现。\n",
    "- LlamaIndex使用论文提供的提示词生成命题。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提取出的命题：\n",
      "['The Eiffel Tower is located in Paris.', 'The Eiffel Tower was built in 1889.']\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.packs.dense_x_retrieval import DenseXRetrievalPack\n",
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "from llama_index.core.llama_pack import download_llama_pack\n",
    "import os\n",
    "import nest_asyncio\n",
    "import json\n",
    "\n",
    "# Apply nest_asyncio to support async calls (e.g., in notebooks)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "PROPOSITIONS_PROMPT = PromptTemplate(\n",
    "    \"\"\"Decompose the \"Content\" into clear and simple propositions, ensuring they are interpretable out of\n",
    "context.\n",
    "1. Split compound sentence into simple sentences. Maintain the original phrasing from the input\n",
    "whenever possible.\n",
    "2. For any named entity that is accompanied by additional descriptive information, separate this\n",
    "information into its own distinct proposition.\n",
    "3. Decontextualize the proposition by adding necessary modifier to nouns or entire sentences\n",
    "and replacing pronouns (e.g., \"it\", \"he\", \"she\", \"they\", \"this\", \"that\") with the full name of the\n",
    "entities they refer to.\n",
    "4. Present the results as a list of strings, formatted in JSON.\n",
    "\n",
    "Input: Title: ¯Eostre. Section: Theories and interpretations, Connection to Easter Hares. Content:\n",
    "The earliest evidence for the Easter Hare (Osterhase) was recorded in south-west Germany in\n",
    "1678 by the professor of medicine Georg Franck von Franckenau, but it remained unknown in\n",
    "other parts of Germany until the 18th century. Scholar Richard Sermon writes that \"hares were\n",
    "frequently seen in gardens in spring, and thus may have served as a convenient explanation for the\n",
    "origin of the colored eggs hidden there for children. Alternatively, there is a European tradition\n",
    "that hares laid eggs, since a hare’s scratch or form and a lapwing’s nest look very similar, and\n",
    "both occur on grassland and are first seen in the spring. In the nineteenth century the influence\n",
    "of Easter cards, toys, and books was to make the Easter Hare/Rabbit popular throughout Europe.\n",
    "German immigrants then exported the custom to Britain and America where it evolved into the\n",
    "Easter Bunny.\"\n",
    "Output: [ \"The earliest evidence for the Easter Hare was recorded in south-west Germany in\n",
    "1678 by Georg Franck von Franckenau.\", \"Georg Franck von Franckenau was a professor of\n",
    "medicine.\", \"The evidence for the Easter Hare remained unknown in other parts of Germany until\n",
    "the 18th century.\", \"Richard Sermon was a scholar.\", \"Richard Sermon writes a hypothesis about\n",
    "the possible explanation for the connection between hares and the tradition during Easter\", \"Hares\n",
    "were frequently seen in gardens in spring.\", \"Hares may have served as a convenient explanation\n",
    "for the origin of the colored eggs hidden in gardens for children.\", \"There is a European tradition\n",
    "that hares laid eggs.\", \"A hare’s scratch or form and a lapwing’s nest look very similar.\", \"Both\n",
    "hares and lapwing’s nests occur on grassland and are first seen in the spring.\", \"In the nineteenth\n",
    "century the influence of Easter cards, toys, and books was to make the Easter Hare/Rabbit popular\n",
    "throughout Europe.\", \"German immigrants exported the custom of the Easter Hare/Rabbit to\n",
    "Britain and America.\", \"The custom of the Easter Hare/Rabbit evolved into the Easter Bunny in\n",
    "Britain and America.\" ]\n",
    "\n",
    "Input: {node_text}\n",
    "Output:\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "def safe_json_loads(text: str) -> list:\n",
    "    \"\"\"Safely parse a JSON list from LLM output (handles ```json fenced blocks).\"\"\"\n",
    "    # Strip whitespace and invisible characters\n",
    "    text = text.strip()\n",
    "\n",
    "    # If it starts with ```json, remove the code fence marker\n",
    "    if text.startswith(\"```json\"):\n",
    "        text = text[7:]\n",
    "    if text.endswith(\"```\"):\n",
    "        text = text[:-3]\n",
    "\n",
    "    # Strip again\n",
    "    text = text.strip()\n",
    "\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSONDecodeError at position {e.pos}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def extract_propositions(text: str, llm: OpenAI) -> list[str]:\n",
    "    \"\"\"\n",
    "    Use an LLM to extract propositions from the input text.\n",
    "    \"\"\"\n",
    "    # Build the full prompt\n",
    "    prompt = PROPOSITIONS_PROMPT.format(node_text=text)\n",
    "\n",
    "    # Call the LLM\n",
    "    response = llm.complete(prompt).text.strip()\n",
    "\n",
    "    # Parse the response as a JSON list\n",
    "    try:\n",
    "        propositions = safe_json_loads(response)\n",
    "    except Exception:\n",
    "        print(\"Failed to parse JSON. Raw response:\", response)\n",
    "        return []\n",
    "\n",
    "    return propositions\n",
    "\n",
    "\n",
    "# Initialize the LLM + embedding model\n",
    "llm = OpenAI(model=\"gpt-4o\", temperature=0.1, max_tokens=750)\n",
    "embed_model = OpenAIEmbedding(embed_batch_size=128)\n",
    "\n",
    "# Example text for testing extract_propositions\n",
    "test_text = \"The Eiffel Tower is located in Paris and was built in 1889.\"\n",
    "\n",
    "# Run and print results\n",
    "propositions = extract_propositions(test_text, llm)\n",
    "print(\"Extracted propositions:\")\n",
    "print(propositions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.90s/it]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:02<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Eiffel Tower was built in 1889.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "from llama_index.core.llama_pack import download_llama_pack\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_KEY\"\n",
    "\n",
    "# DenseXRetrievalPack = download_llama_pack(\n",
    "#     \"DenseXRetrievalPack\", \"./dense_pack\"\n",
    "# )\n",
    "\n",
    "# If you have already downloaded DenseXRetrievalPack, you can import it directly.\n",
    "from llama_index.packs.dense_x_retrieval import DenseXRetrievalPack\n",
    "\n",
    "# Load documents\n",
    "dir_path = \"/Users/wilson/rag50_test\"\n",
    "documents = SimpleDirectoryReader(dir_path).load_data()\n",
    "\n",
    "# Use LLM to extract propositions from every document/node\n",
    "\n",
    "dense_pack = DenseXRetrievalPack(documents)\n",
    "\n",
    "response = dense_pack.run(\"When the Eiffel Tower was built?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 内部原理详解\n",
    "\n",
    "`DenseXRetrievalPack` 的核心逻辑遵循**“由小到大”（small-to-big）**的检索思想：\n",
    "\n",
    "### 步骤一：基础切块（Nodes）\n",
    "- 使用 `SentenceSplitter` 将文档划分为**基本文本块（nodes）**，作为命题提取的**基础单元**。\n",
    "\n",
    "### 步骤二：命题提取（Sub-Nodes）\n",
    "- 调用 **LLM** 和**预定义Prompt**，异步提取每个 `node` 中的**命题**，转化为**子节点（sub-nodes）**。\n",
    "- 同时保留 `sub-node` 与原始 `node` 的**映射关系**。\n",
    "\n",
    "### 步骤三：构建混合索引\n",
    "- 将原始 `nodes` 与 `sub-nodes` 一起构建**向量索引（VectorStoreIndex）**。\n",
    "- 支持**命题级检索**（高精度）与**区块级生成**（完整上下文）。\n",
    "\n",
    "### 步骤四：递归检索机制\n",
    "- 使用 `RecursiveRetriever` 进行检索：\n",
    "    - **优先检索子命题：** 确保高精度匹配。\n",
    "    - **未找到则回溯父区块：** 保证信息完整性。\n",
    "\n",
    "这种索引结构兼顾了**检索效率**与**上下文完整性**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 命题化检索的价值\n",
    "\n",
    "命题化检索是RAG领域的前沿方向：\n",
    "- **突破传统限制：** 不再局限于段落或句子作为最小单位。\n",
    "- **双重提升：** 通过精细化切片和上下文增强机制，同时提升**检索精度**与**生成质量**。\n",
    "- **适用场景：** 特别适合**知识密集型任务**，如多跳问答、法律文书分析、科研文献检索等。\n",
    "\n",
    "LlamaIndex提供了从`SentenceSplitter` → `SemanticSplitter` → `TopicNodeParser` → `DenseXRetrievalPack`的**“复杂度阶梯”**，让开发者可以根据项目需求和性能预算灵活选择策略，并能随业务增长平滑升级RAG架构。\n",
    "\n",
    "---\n",
    "\n",
    "接下来，我们将探讨商业RAG软件如何解决这些问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Dify 实践：可视化配置下的智能切片策略\n",
    "\n",
    "- **Dify：** 以用户体验为核心的LLM应用平台。\n",
    "- **切片模式：**\n",
    "    - **通用模式：** 扁平化文本块，适用于简单文本。\n",
    "    - **父子模式：** 推荐新用户，实现“由小到大”层次化检索。\n",
    "- **注意：** 知识库创建后，切片模式不可更改。\n",
    "\n",
    "### 父子模式：平衡检索精度与上下文完整性\n",
    "- **工作机制：**\n",
    "    - **分层切分：** 文档先切为“父区块”，再细分“子区块”。\n",
    "    - **精确检索：** 用子区块匹配，提升准确率。\n",
    "    - **上下文增强：** 找到子区块后，提取其父区块作为上下文。\n",
    "- **可控性强：** 可分别设置父子区块切分规则。\n",
    "- **优势特性：** 结构化解耦重叠需求，子区块无需手动设置重叠。\n",
    "\n",
    "---\n",
    "\n",
    "## RAGFlow 实践：深度文档理解驱动的切片方案\n",
    "\n",
    "- **RAGFlow：** 企业级复杂格式文档RAG引擎，强调 **“布局感知”** 切片。\n",
    "- **核心设计哲学：**\n",
    "    - 传统切片忽略文档视觉结构，RAGFlow融合CV与NLP实现 **“深度文档理解”** 。\n",
    "    - **目标：** 高保真还原结构，避免格式错误导致的上下文断裂。\n",
    "    - **原则：** “高质量输入，高质量输出”。\n",
    "\n",
    "### 模板化切片：布局感知的标准化路径\n",
    "- **机制：** 根据文档类型预设优化方案，自动应用解析逻辑。\n",
    "\n",
    "| 模板名称       | 适用场景          | 特点                                          |\n",
    "| :------------- | :---------------- | :-------------------------------------------- |\n",
    "| General        | 简单文本文档      | 标准连续切片                                  |\n",
    "| Paper          | 学术论文          | 识别标题、摘要、正文、参考文献               |\n",
    "| Book           | 长篇幅书籍        | 保留章节结构，支持跨页连贯性                  |\n",
    "| Laws           | 法律条文          | 处理编号条款、嵌套结构                        |\n",
    "| Table          | 含有表格的文档    | 解析表格结构并保留表头信息                    |\n",
    "| Resume         | 简历文件          | 提取姓名、联系方式、工作经历等                |\n",
    "| One            | 短文档或全局上下文 | 整个文档视为一个整体                          |\n",
    "| Picture        | 图片中含文字内容  | OCR结合NLP分析图像内文本                      |\n",
    "\n",
    "- **核心价值：** 自动应用优化规则，尤其在PDF等复杂版式中显著优势。\n",
    "- **用户操作：**\n",
    "    - **可视化审查：** 查看文档快照及区块内容。\n",
    "    - **手动微调：** 编辑区块内容，添加关键词。\n",
    "- **设计理念：** **自动化 + 人工协同**。\n",
    "\n",
    "---\n",
    "\n",
    "## 总结与最佳实践建议：实现动态切片与重叠机制的高效协同\n",
    "\n",
    "### 1. 从结构入手：优先使用结构感知策略\n",
    "- **场景：** 明确格式文档（HTML, PDF等）。\n",
    "- **做法：** RAGFlow模板化、LlamaIndex的HTMLNodeParser/TableNodeParser。\n",
    "- **优势：** 保留原始语义与结构，提升检索质量。\n",
    "\n",
    "### 2. 对纯文本，走向语义：采用语义驱动切片\n",
    "- **场景：** 非结构化文本（新闻、报告等）。\n",
    "- **做法：** LlamaIndex的`SemanticSplitterNodeParser`。\n",
    "- **优势：** 提升上下文连贯性，精准检索复杂语义。\n",
    "\n",
    "### 3. 拥抱“由小到大”架构：探索层次化检索模式\n",
    "- **场景：** 需兼顾精度与完整性任务（多跳问答）。\n",
    "- **做法：** Dify父子模式、LlamaIndex的`SentenceWindowNodeParser`、命题化检索。\n",
    "- **优势：** 小块精度，大块完整性；规避重叠依赖。\n",
    "\n",
    "### 4. 将重叠视为战术补充：适度使用而非依赖\n",
    "- **场景：** 固定/递归切片时的辅助手段。\n",
    "- **做法：** 10%-20%重叠比例，避免冗余。\n",
    "- **优势：** 简单策略下提升上下文，成本可控。\n",
    "\n",
    "### 5. 构建评估体系：持续验证与迭代优化\n",
    "- **场景：** 任何RAG应用上线及运行中。\n",
    "- **做法：** 定义指标（召回率、准确性），A/B测试，结合人工反馈。\n",
    "- **优势：** 避免盲目选择，持续优化，适应需求。\n",
    "\n",
    "---\n",
    "\n",
    "- **切片作用：** 为混合检索、重排、查询转换、多阶段检索等高级技术提供高质量数据“弹药”。\n",
    "- **目标：** 确保切片策略的输出能为整个RAG系统奠定坚实基础。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
