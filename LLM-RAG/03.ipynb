{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 领域术语总混淆？教你构建精准术语词库，提升检索一致性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在RAG系统构建过程中，术语混淆直接影响信息检索的精准度与生成内容的质量。\n",
    "\n",
    "这主要源于几个方面：\n",
    "- 向量表示\n",
    "- 不同行业、公司乃至同一组织内部，都可能存在相似词汇却拥有截然不同含义的情况\n",
    "\n",
    "这些因素最终导致检索结果偏离预期，大幅降低了答案的质量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、术语词库构建与维护（Glossary Management）\n",
    "## 1.1 产生术语混淆的原因\n",
    "\n",
    "- 术语多义性\n",
    "- 同义词与近义词\n",
    "- 领域差异\n",
    "- 企业专属术语"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 构建术语词库的目标\n",
    "术语词库是整个术语一致性优化体系的核心基础设施。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 术语词库的构建流程\n",
    "- Step 1：收集术语来源\n",
    "- Step 2：标准化术语\n",
    "- Step 3：建立别名映射关系\n",
    "- Step 4：添加上下文信息\n",
    "- Step 5：构建术语索引\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个功能完善的术语词库应包含以下关键字段，以确保其结构化和可操作性：\n",
    "\n",
    "| 字段名                 | 内容                                                                                   |\n",
    "|------------------------|----------------------------------------------------------------------------------------|\n",
    "| 术语（Term）           | 神经网络                                                                              |\n",
    "| 别名（Synonyms）       | [\"人工神经网络\", \"NN\"]                                                                |\n",
    "| 定义（Definition）     | 神经网络是一种模仿生物神经网络结构和功能的计算模型……                                  |\n",
    "| 上下文标签（Context Tags） | [\"人工智能\", \"深度学习\", \"计算机科学\"]                                                 |\n",
    "| 所属领域（Domain）     | 人工智能                                                                              |\n",
    "| 示例用法（Usage Example） | 在图像识别任务中，我们使用了一个卷积神经网络。                                        |\n",
    "| 外部链接（External Link） | [维基百科链接](https://en.wikipedia.org/wiki/Artificial_neural_network)               |\n",
    "| 禁用词/误导词（Stop Words / Misleading Terms） | [\"神经系统\"（医学中的不同概念）]                                                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 术语词库与 RAG 集成\n",
    "\n",
    "- 方式一：预处理阶段替换术语\n",
    "- 方式二：检索增强\n",
    "- 方式三：重排序（Re-ranking）\n",
    "- 方式四：后处理解释"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 术语词库维护\n",
    "1. 术语词库结构设计\n",
    "这是基础，确定词库所需包含的字段和它们之间的关系。\n",
    "\n",
    "2. 自动抽取术语候选\n",
    "利用 NLP 工具从大量文本中自动识别和提取潜在的术语。\n",
    "\n",
    "3. 专家审核与完善\n",
    "领域专家对自动抽取的术语进行人工审核、修正和补充，确保准确性和专业性。\n",
    "\n",
    "4. 构建术语关系图谱\n",
    "如果有需求，可以进一步构建术语之间的层次、关联关系，形成本体（Ontology）或知识图谱（Knowledge Graph），以提升语义理解能力。\n",
    "\n",
    "5. 版本控制与更新机制建设\n",
    "建立术语词库的版本管理和定期更新机制，确保其时效性和权威性，应对新术语的出现或旧术语含义的变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 阶段             | 技术名称                                 | 描述                                                                                                                                               |\n",
    "|------------------|------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| 1. 数据预处理    | 术语抽取、标准化、上下文分块             | 在原始文档和查询进入 RAG 系统之前，识别并提取领域术语，进行统一化处理，并确保文本分块时能有效保留术语的上下文信息。                                   |\n",
    "| 2. 术语词库构建  | 词库设计、术语关系建模、版本管理         | 建立结构化的术语词库，包含术语、别名、定义、上下文标签等字段。进一步可构建术语间的层级或关联关系（如本体），并建立完善的版本控制与更新机制。           |\n",
    "| 3. 嵌入与向量化  | 构建术语向量索引、微调领域嵌入模型       | 将术语词库中的标准术语和别名转换为向量，并构建高效的向量索引（如 FAISS）。同时，通过领域适应性训练（如 LoRA）优化通用嵌入模型，使其更好地理解领域特有概念。 |\n",
    "| 4. 检索增强      | 查询扩展、混合检索、重排序、元数据过滤   | 利用术语词库对用户查询进行扩展（添加别名），结合向量检索与关键词检索（混合检索）。在召回结果后，通过术语匹配度进行重排序，或利用术语作为元数据进行更精确的过滤。 |\n",
    "| 5. 生成控制      | 提示工程、结构化输出、术语验证           | 设计包含术语词库信息的提示词，引导大模型生成更准确的答案。在输出阶段，可强制模型使用词库中的标准术语，并对生成内容进行术语验证，避免出现混淆或不规范表达。     |\n",
    "| 6. 评估与反馈    | 术语一致性指标、LLM-as-a-Judge、用户反馈 | 建立专门的评估指标来衡量 RAG 系统在术语一致性方面的表现。利用大型语言模型作为评估器（LLM-as-a-Judge）来检查术语使用情况，并收集用户反馈，持续优化词库和系统。 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 玩具版代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2. Data Preprocessing: Terminology Standardization ---\n",
      "Original query: I want to learn about applications of ML models in image recognition, and also some NLP knowledge.\n",
      "Standardized query: I want to learn about applications of Machine Learning models in image recognition, and also some Natural Language Processing knowledge.\n",
      "Original document: Recently I studied CNN and AI algorithms, and found they perform well on big data, especially ML in certain scenarios.\n",
      "Standardized document: Recently I studied Convolutional Neural Network and Machine Learning, and found they perform well on big data, especially Machine Learning in certain scenarios.\n",
      "\n",
      "--- 3. Term Extraction ---\n",
      "Extracted terms from query: ['Machine Learning', 'Natural Language Processing']\n",
      "Extracted terms from document: ['Convolutional Neural Network', 'Machine Learning']\n",
      "\n",
      "--- 4. Simulated Vector Storage and Retrieval Augmentation (Conceptual) ---\n",
      "In a real application, we would use an embedding model (e.g., SentenceTransformers) to convert the standardized text and terms into vectors.\n",
      "These vectors would then be stored in a dedicated vector database (e.g., FAISS, Pinecone, or Weaviate) for efficient similarity search.\n",
      "During retrieval, the user query is first standardized and vectorized, then used to query the vector database to fetch relevant documents.\n",
      "\n",
      "--- 5. Simulated Retrieval Augmentation: Query Expansion ---\n",
      "Original retrieval query: I want to know what a CPU does in a computer, and what cost-per-unit CPU means?\n",
      "Expanded retrieval keyword list: ['CPU', 'Central Processing Unit', 'I want to know what a Central Processing Unit does in a computer, and what cost-per-unit Central Processing Unit means?']\n",
      "\n",
      "In a production RAG system, these expanded keywords would drive a hybrid retrieval strategy, combining semantic (vector) search with keyword-based search for best results.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# 1. Define a terminology glossary (keep it updated, including context tags)\n",
    "GLOSSARY = [\n",
    "    {\n",
    "        \"term\": \"Convolutional Neural Network\",\n",
    "        \"synonyms\": [\"CNN\", \"Convolution-based neural network\"],\n",
    "        \"definition\": \"A computational model inspired by biological neural networks, especially well-suited for image processing.\",\n",
    "        \"context_tags\": [\"image recognition\", \"deep learning\"],\n",
    "    },\n",
    "    {\n",
    "        \"term\": \"Machine Learning\",\n",
    "        \"synonyms\": [\"ML\", \"Machine learn\", \"AI algorithms\"],\n",
    "        \"definition\": \"A field of artificial intelligence that enables computer systems to learn from data without being explicitly programmed.\",\n",
    "        \"context_tags\": [\"artificial intelligence\", \"data science\"],\n",
    "    },\n",
    "    {\n",
    "        \"term\": \"Natural Language Processing\",\n",
    "        \"synonyms\": [\"NLP\", \"natural language\"],\n",
    "        \"definition\": \"A field that studies the interaction between human language and computers.\",\n",
    "        \"context_tags\": [\"artificial intelligence\", \"linguistics\"],\n",
    "    },\n",
    "    {\n",
    "        \"term\": \"Central Processing Unit\",\n",
    "        \"synonyms\": [\"CPU\"],\n",
    "        \"definition\": \"The arithmetic, logic, and control unit of a computer.\",\n",
    "        \"context_tags\": [\"computer hardware\", \"computer\"],\n",
    "    },\n",
    "    {\n",
    "        \"term\": \"Cost per Unit\",\n",
    "        \"synonyms\": [\"CPU\"],\n",
    "        \"definition\": \"A business analytics metric that measures the cost per unit of a product or service.\",\n",
    "        \"context_tags\": [\"business analytics\", \"financial management\", \"cost\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "class TerminologyProcessor:\n",
    "    def __init__(self, glossary: List[Dict[str, Any]]):\n",
    "        self.glossary = glossary\n",
    "        self.standard_term_map = {}\n",
    "        self.alias_to_entries_map = {}\n",
    "        self._build_mappings()\n",
    "\n",
    "    def _build_mappings(self):\n",
    "        \"\"\"Build mappings; one alias may map to multiple terminology entries to handle ambiguity.\"\"\"\n",
    "        for entry in self.glossary:\n",
    "            standard_term = entry[\"term\"]\n",
    "            self.standard_term_map[standard_term.lower()] = standard_term\n",
    "\n",
    "            all_aliases = [standard_term] + entry.get(\"synonyms\", [])\n",
    "            for alias in all_aliases:\n",
    "                alias_lower = alias.lower()\n",
    "                if alias_lower not in self.alias_to_entries_map:\n",
    "                    self.alias_to_entries_map[alias_lower] = []\n",
    "                if entry not in self.alias_to_entries_map[alias_lower]:\n",
    "                    self.alias_to_entries_map[alias_lower].append(entry)\n",
    "\n",
    "    def standardize_text(self, text: str, context_window: int = 10) -> str:\n",
    "        \"\"\"\n",
    "        Context-aware terminology standardization using iteration + a replacement function.\n",
    "        Dynamically generates the correct regex for each term type.\n",
    "        \"\"\"\n",
    "        standardized_text = text\n",
    "        sorted_keys = sorted(self.alias_to_entries_map.keys(), key=len, reverse=True)\n",
    "\n",
    "        for key_lower in sorted_keys:\n",
    "            possible_entries = self.alias_to_entries_map[key_lower]\n",
    "\n",
    "            # --- Dynamically create the correct regex for each key ---\n",
    "            pattern_str = \"\"\n",
    "            # If key contains Latin letters, assume it's an abbreviation and enforce boundaries\n",
    "            if re.search(r\"[a-zA-Z]\", key_lower):\n",
    "                # Use lookarounds to avoid matching inside a larger word\n",
    "                pattern_str = r\"(?<![a-zA-Z])\" + re.escape(key_lower) + r\"(?![a-zA-Z])\"\n",
    "            else:\n",
    "                # For Chinese (or non-Latin) terms, match exactly\n",
    "                pattern_str = re.escape(key_lower)\n",
    "\n",
    "            pattern = re.compile(pattern_str, flags=re.IGNORECASE)\n",
    "\n",
    "            # Replacement function called for each match\n",
    "            def replacer(match: re.Match) -> str:\n",
    "                if len(possible_entries) == 1:\n",
    "                    return possible_entries[0][\"term\"]\n",
    "                else:\n",
    "                    # --- Context-based disambiguation ---\n",
    "                    context_snippet = standardized_text[\n",
    "                        max(0, match.start() - context_window) : min(len(standardized_text), match.end() + context_window)\n",
    "                    ]\n",
    "                    for entry in possible_entries:\n",
    "                        clues = entry.get(\"context_tags\", []) + [entry[\"term\"]]\n",
    "                        if any(clue in context_snippet for clue in clues):\n",
    "                            return entry[\"term\"]\n",
    "                    # If no context clue is found, fall back to the first definition\n",
    "                    return possible_entries[0][\"term\"]\n",
    "\n",
    "            # Update text using the replacement function\n",
    "            standardized_text = pattern.sub(replacer, standardized_text)\n",
    "\n",
    "        return standardized_text\n",
    "\n",
    "    def extract_terms(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract known standardized terms from text\n",
    "        \"\"\"\n",
    "        found_terms = set()\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        for standard_term_lower, original_standard_term in self.standard_term_map.items():\n",
    "            # Direct substring search; do not use \\b\n",
    "            if re.search(re.escape(standard_term_lower), text_lower):\n",
    "                found_terms.add(original_standard_term)\n",
    "\n",
    "        return sorted(list(found_terms))\n",
    "\n",
    "\n",
    "# 1. Initialize the terminology processor with the glossary.\n",
    "term_processor = TerminologyProcessor(GLOSSARY)\n",
    "\n",
    "# 2. Data preprocessing: terminology standardization\n",
    "print(\"--- 2. Data Preprocessing: Terminology Standardization ---\")\n",
    "user_query = \"I want to learn about applications of ML models in image recognition, and also some NLP knowledge.\"\n",
    "processed_query = term_processor.standardize_text(user_query)\n",
    "print(f\"Original query: {user_query}\")\n",
    "print(f\"Standardized query: {processed_query}\")\n",
    "\n",
    "document_text = \"Recently I studied CNN and AI algorithms, and found they perform well on big data, especially ML in certain scenarios.\"\n",
    "processed_document = term_processor.standardize_text(document_text)\n",
    "print(f\"Original document: {document_text}\")\n",
    "print(f\"Standardized document: {processed_document}\")\n",
    "\n",
    "# 3. Term extraction (for downstream vectorization or metadata tagging)\n",
    "print(\"\\n--- 3. Term Extraction ---\")\n",
    "extracted_terms_query = term_processor.extract_terms(processed_query)\n",
    "print(f\"Extracted terms from query: {extracted_terms_query}\")\n",
    "\n",
    "extracted_terms_document = term_processor.extract_terms(processed_document)\n",
    "print(f\"Extracted terms from document: {extracted_terms_document}\")\n",
    "\n",
    "# 4. Simulated vector storage and retrieval augmentation (conceptual)\n",
    "print(\"\\n--- 4. Simulated Vector Storage and Retrieval Augmentation (Conceptual) ---\")\n",
    "print(\"In a real application, we would use an embedding model (e.g., SentenceTransformers) to convert the standardized text and terms into vectors.\")\n",
    "print(\"These vectors would then be stored in a dedicated vector database (e.g., FAISS, Pinecone, or Weaviate) for efficient similarity search.\")\n",
    "print(\"During retrieval, the user query is first standardized and vectorized, then used to query the vector database to fetch relevant documents.\")\n",
    "\n",
    "# 5. Simulated retrieval augmentation: query expansion\n",
    "def enhance_query_for_retrieval(query: str, processor: TerminologyProcessor) -> List[str]:\n",
    "    \"\"\"Expand query keywords using the terminology glossary to improve recall.\"\"\"\n",
    "    standardized_query = processor.standardize_text(query)\n",
    "    query_terms = processor.extract_terms(standardized_query)\n",
    "\n",
    "    expanded_keywords = set([standardized_query])\n",
    "    for term in query_terms:\n",
    "        expanded_keywords.add(term)\n",
    "        for entry in processor.glossary:\n",
    "            if entry[\"term\"] == term:\n",
    "                for synonym in entry.get(\"synonyms\", []):\n",
    "                    expanded_keywords.add(synonym)\n",
    "                break\n",
    "    return sorted(list(expanded_keywords))\n",
    "\n",
    "\n",
    "print(\"\\n--- 5. Simulated Retrieval Augmentation: Query Expansion ---\")\n",
    "original_query_for_retrieval = \"I want to know what a CPU does in a computer, and what cost-per-unit CPU means?\"\n",
    "expanded_keywords = enhance_query_for_retrieval(original_query_for_retrieval, term_processor)\n",
    "print(f\"Original retrieval query: {original_query_for_retrieval}\")\n",
    "print(f\"Expanded retrieval keyword list: {expanded_keywords}\")\n",
    "\n",
    "print(\"\\nIn a production RAG system, these expanded keywords would drive a hybrid retrieval strategy, combining semantic (vector) search with keyword-based search for best results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、数据预处理阶段（Preprocessing）：提升语义表示质量\n",
    "\n",
    "这是术语一致性优化的“第一道防线”，直接影响后续所有环节的质量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 技术名称                          | 描述                                       | 对术语一致性的帮助                           |\n",
    "|-----------------------------------|--------------------------------------------|----------------------------------------------|\n",
    "| 术语抽取（NER、TF-IDF、KeyBERT）  | 自动从语料中识别候选术语                   | 提供术语来源，是词库构建的基础               |\n",
    "| 术语标准化（Term Normalization）  | 替换非标准表达为统一术语（如“AI”→“人工智能”） | 消除输入噪声，确保术语表达一致               |\n",
    "| 文本清洗与格式统一                | 清理无意义内容、统一大小写、标点等         | 减少干扰，提升术语识别准确率                 |\n",
    "| 上下文感知分块策略（SemanticChunker） | 按语义相似度切分文本块                     | 保留术语所在上下文信息，避免割裂语义         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 环境准备\n",
    "首先，确保你安装了必要的Python库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers sentence-transformers faiss-cpu scikit-learn spacy\n",
    "! python -m spacy download zh_core_web_sm\n",
    "! pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤一：术语词库结构设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically extracted term candidates: {'CNN'}\n"
     ]
    }
   ],
   "source": [
    "# Use spaCy's EntityRuler to define custom entity recognition rules\n",
    "# based on our terminology glossary\n",
    "import spacy\n",
    "\n",
    "def extract_terms_with_ruler(text, glossary):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    # Create an EntityRuler pipeline and load all terms and their aliases\n",
    "    # from the glossary\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "    patterns = []\n",
    "    for term, data in glossary.items():\n",
    "        patterns.append({\"label\": \"TERM\", \"pattern\": term})\n",
    "        for syn in data.get(\"synonyms\", []):\n",
    "            patterns.append({\"label\": \"TERM\", \"pattern\": syn})\n",
    "    ruler.add_patterns(patterns)\n",
    "    \n",
    "    # Process the text and extract entities labeled as \"TERM\"\n",
    "    doc = nlp(text)\n",
    "    candidates = {ent.text for ent in doc.ents if ent.label_ == \"TERM\"}\n",
    "    return candidates\n",
    "\n",
    "# Example (English version)\n",
    "text = \"Examples of CNN model applications in image recognition include autonomous driving and medical imaging diagnosis.\"\n",
    "candidates = extract_terms_with_ruler(text, term_glossary)\n",
    "print(f\"Automatically extracted term candidates: {candidates}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤二：2.1 术语抽取与标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically extracted term candidates: {'CNN'}\n"
     ]
    }
   ],
   "source": [
    "# Use spaCy's EntityRuler to customize entity recognition rules\n",
    "# based on our terminology glossary\n",
    "import spacy\n",
    "\n",
    "def extract_terms_with_ruler(text, glossary):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    # Create an EntityRuler pipeline and load all terms and their aliases\n",
    "    # from the glossary\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "    patterns = []\n",
    "    for term, data in glossary.items():\n",
    "        patterns.append({\"label\": \"TERM\", \"pattern\": term})\n",
    "        for syn in data.get(\"synonyms\", []):\n",
    "            patterns.append({\"label\": \"TERM\", \"pattern\": syn})\n",
    "    ruler.add_patterns(patterns)\n",
    "    \n",
    "    # Process the text and extract entities recognized as \"TERM\"\n",
    "    doc = nlp(text)\n",
    "    candidates = {ent.text for ent in doc.ents if ent.label_ == \"TERM\"}\n",
    "    return candidates\n",
    "\n",
    "# Example\n",
    "text = \"Examples of CNN model applications in image recognition include autonomous driving and medical imaging diagnosis.\"\n",
    "candidates = extract_terms_with_ruler(text, term_glossary)\n",
    "print(f\"Automatically extracted term candidates: {candidates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、嵌入构建与向量化阶段（Embedding & Vectorization）\n",
    "\n",
    "核心任务是将这些经过清洗和标准化的术语，转化为机器能够理解和计算的密集向量（Dense Vectors），并构建高效的检索索引。这直接决定了系统语义匹配的能力上限。\n",
    "\n",
    "| 技术名称                                 | 描述                                       | 对术语一致性的帮助                             |\n",
    "|------------------------------------------|--------------------------------------------|------------------------------------------------|\n",
    "| 术语嵌入与向量索引（FAISS / Pinecone）   | 将术语及其别名转换为向量并构建索引         | 支持语义匹配，提升检索时的术语识别能力         |\n",
    "| 域专用嵌入模型（Legal-BERT、ChatLaw-Text2Vec） | 在专业语料上继续训练通用模型               | 提升术语理解质量，增强语义表示                 |\n",
    "| Sentence Transformers + PEFT（LoRA）微调 | 参数高效微调嵌入模型                       | 针对特定领域进一步优化术语语义表示             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤二：2.2 基于向量相似度的同义词发现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     44\u001b[39m main_terms_to_map = [\u001b[33m\"\u001b[39m\u001b[33mConvolutional Neural Network\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mNeural Network\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     45\u001b[39m all_possible_synonyms = [\n\u001b[32m     46\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCNN\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     47\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mConvNet\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDeep Learning Model\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     52\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m optimized_mapped_synonyms = \u001b[43mmap_synonyms_by_similarity\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmain_terms_to_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mall_possible_synonyms\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOptimized matched synonyms:\u001b[39m\u001b[33m\"\u001b[39m, optimized_mapped_synonyms)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mmap_synonyms_by_similarity\u001b[39m\u001b[34m(main_terms, candidates, threshold)\u001b[39m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _matched_synonyms\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Encode in batches for better efficiency\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m embeddings = \u001b[43mmodel\u001b[49m.encode(main_terms + candidates, convert_to_tensor=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     29\u001b[39m term_embeddings = embeddings[:\u001b[38;5;28mlen\u001b[39m(main_terms)]\n\u001b[32m     30\u001b[39m candidate_embeddings = embeddings[\u001b[38;5;28mlen\u001b[39m(main_terms):]\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# It is recommended to load the model once during project initialization\n",
    "# to avoid repeated loading overhead.\n",
    "# model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def map_synonyms_by_similarity(main_terms: list, candidates: list, threshold: float = 0.8) -> dict:\n",
    "    \"\"\"\n",
    "    Map candidate terms to the closest standard terms by computing\n",
    "    cosine similarity between embeddings.\n",
    "\n",
    "    Args:\n",
    "        main_terms (list): List of standard (canonical) terms.\n",
    "        candidates (list): List of candidate synonyms to be matched.\n",
    "        threshold (float): Similarity threshold above which a candidate\n",
    "                           is considered a synonym.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping each standard term to a list of\n",
    "              successfully matched synonyms.\n",
    "    \"\"\"\n",
    "    _matched_synonyms = {term: [] for term in main_terms}\n",
    "\n",
    "    if not main_terms or not candidates:\n",
    "        return _matched_synonyms\n",
    "    \n",
    "    model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Encode in batches for better efficiency\n",
    "    embeddings = model.encode(main_terms + candidates, convert_to_tensor=True)\n",
    "    term_embeddings = embeddings[:len(main_terms)]\n",
    "    candidate_embeddings = embeddings[len(main_terms):]\n",
    "\n",
    "    # Compute the cosine similarity matrix between standard terms and candidates\n",
    "    similarity_matrix = util.cos_sim(term_embeddings, candidate_embeddings)\n",
    "\n",
    "    for i, term in enumerate(main_terms):\n",
    "        for j, candidate in enumerate(candidates):\n",
    "            if similarity_matrix[i][j] > threshold:\n",
    "                _matched_synonyms[term].append(candidate)\n",
    "\n",
    "    return _matched_synonyms\n",
    "\n",
    "\n",
    "# Example:\n",
    "main_terms_to_map = [\"Convolutional Neural Network\", \"Neural Network\"]\n",
    "all_possible_synonyms = [\n",
    "    \"CNN\",\n",
    "    \"ConvNet\",\n",
    "    \"Artificial Neural Network\",\n",
    "    \"NN\",\n",
    "    \"Nervous System\",\n",
    "    \"Deep Learning Model\",\n",
    "]\n",
    "\n",
    "optimized_mapped_synonyms = map_synonyms_by_similarity(\n",
    "    main_terms_to_map,\n",
    "    all_possible_synonyms\n",
    ")\n",
    "print(\"\\nOptimized matched synonyms:\", optimized_mapped_synonyms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 步骤三：构建术语向量索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "代理环境变量已设置。\n",
      "HTTP_PROXY: http://127.0.0.1:4780\n",
      "HTTPS_PROXY: http://127.0.0.1:4780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\learnRAG\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "正在尝试加载模型 'paraphrase-multilingual-MiniLM-L12-v2'...\n",
      "模型加载/下载成功！\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 定义你的代理地址\n",
    "proxy_url = 'http://127.0.0.1:4780'\n",
    "\n",
    "# 为HTTP和HTTPS流量设置代理\n",
    "# 大多数模型下载等操作都通过HTTPS，所以 'HTTPS_PROXY' 至关重要\n",
    "os.environ['HTTP_PROXY'] = proxy_url\n",
    "os.environ['HTTPS_PROXY'] = proxy_url\n",
    "\n",
    "print(\"代理环境变量已设置。\")\n",
    "print(f\"HTTP_PROXY: {os.getenv('HTTP_PROXY')}\")\n",
    "print(f\"HTTPS_PROXY: {os.getenv('HTTPS_PROXY')}\")\n",
    "\n",
    "# --- 在这里开始你的正常代码 ---\n",
    "# 例如，现在加载模型，它将通过指定的代理进行下载\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 只有在模型未被缓存时，才会通过代理下载\n",
    "model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "print(f\"\\n正在尝试加载模型 '{model_name}'...\")\n",
    "try:\n",
    "    model = SentenceTransformer(model_name)\n",
    "    print(\"模型加载/下载成功！\")\n",
    "except Exception as e:\n",
    "    print(f\"加载模型时出错: {e}\")\n",
    "    print(\"请检查您的代理服务是否正在运行，并且地址和端口是否正确。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在生成术语向量...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS索引构建完成。包含 9 个向量，维度为 384。\n",
      "\n",
      "--- 索引构建成功 ---\n",
      "FAISS 索引中的向量数量: 9\n",
      "被索引的术语列表: ['CNN', 'ConvNet', 'TRANSFORMER', 'Transformer', 'transformer', '卷积神经网络', '图像分类', '图像识别', '视觉识别']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def build_term_vector_index(term_glossary: dict, model: SentenceTransformer) -> tuple:\n",
    "    \"\"\"\n",
    "    将术语词库中的所有术语及其别名转换为向量，并构建FAISS索引。\n",
    "\n",
    "    Args:\n",
    "        term_glossary (dict): 结构化的术语词库。键为标准术语，值为包含'synonyms'列表的字典。\n",
    "        model (SentenceTransformer): 已加载的SentenceTransformer模型实例。\n",
    "\n",
    "    Returns:\n",
    "        tuple: (faiss.Index, list) 返回构建好的FAISS索引对象和与之对应的术语列表。\n",
    "    \"\"\"\n",
    "    terms_to_index = []\n",
    "    # 遍历术语映射字典，收集所有标准术语和别名\n",
    "    # 修正点：将 term_mapping_dict 修改为 term_glossary\n",
    "    for standard_term, info in term_glossary.items():\n",
    "        terms_to_index.append(standard_term)\n",
    "        if \"synonyms\" in info and isinstance(info[\"synonyms\"], list):\n",
    "            terms_to_index.extend(info[\"synonyms\"])\n",
    "    \n",
    "    unique_terms_to_index = sorted(list(set(terms_to_index)))\n",
    "    \n",
    "    print(\"正在生成术语向量...\")\n",
    "    embeddings = model.encode(unique_terms_to_index, show_progress_bar=True)\n",
    "    \n",
    "    embeddings = embeddings.astype('float32')\n",
    "    dimension = embeddings.shape[1]\n",
    "    \n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    print(f\"FAISS索引构建完成。包含 {index.ntotal} 个向量，维度为 {dimension}。\")\n",
    "    return index, unique_terms_to_index\n",
    "\n",
    "\n",
    "# 1. 加载模型\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2') \n",
    "\n",
    "# 2. 准备术语数据\n",
    "term_mapping_example = {\n",
    "    \"卷积神经网络\": {\"synonyms\": [\"CNN\", \"ConvNet\"]},\n",
    "    \"Transformer\": {\"synonyms\": [\"transformer\", \"TRANSFORMER\"]},\n",
    "    \"图像识别\": {\"synonyms\": [\"图像分类\", \"视觉识别\"]}\n",
    "}\n",
    "\n",
    "# 3. 使用修正后的函数进行调用\n",
    "faiss_index, indexed_term_list = build_term_vector_index(term_mapping_example, model)\n",
    "\n",
    "# 4. 验证结果\n",
    "print(\"\\n--- 索引构建成功 ---\")\n",
    "print(f\"FAISS 索引中的向量数量: {faiss_index.ntotal}\")\n",
    "print(f\"被索引的术语列表: {indexed_term_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何调用该函数并获取索引和术语列表的示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 正在执行检索 ---\n",
      "查询: 'CNN'\n",
      "检索结果:\n",
      "  Top 1: 术语='CNN',  距离=0.0000 (值越小越相似)\n",
      "  Top 2: 术语='图像分类',  距离=35.1526 (值越小越相似)\n",
      "  Top 3: 术语='ConvNet',  距离=35.1713 (值越小越相似)\n",
      "\n",
      "--- 正在执行检索 ---\n",
      "查询: '计算机视觉'\n",
      "检索结果:\n",
      "  Top 1: 术语='视觉识别',  距离=11.4270 (值越小越相似)\n",
      "  Top 2: 术语='图像识别',  距离=14.5564 (值越小越相似)\n",
      "  Top 3: 术语='图像分类',  距离=15.7114 (值越小越相似)\n",
      "\n",
      "--- 正在执行检索 ---\n",
      "查询: '语言模型'\n",
      "检索结果:\n",
      "  Top 1: 术语='图像分类',  距离=24.3401 (值越小越相似)\n",
      "  Top 2: 术语='卷积神经网络',  距离=26.5247 (值越小越相似)\n",
      "  Top 3: 术语='图像识别',  距离=27.9374 (值越小越相似)\n",
      "\n",
      "--- 正在执行检索 ---\n",
      "查询: '变换器模型'\n",
      "检索结果:\n",
      "  Top 1: 术语='TRANSFORMER',  距离=11.7908 (值越小越相似)\n",
      "  Top 2: 术语='图像分类',  距离=17.0967 (值越小越相似)\n",
      "  Top 3: 术语='卷积神经网络',  距离=17.4776 (值越小越相似)\n"
     ]
    }
   ],
   "source": [
    "# --- 第2部分：定义我们的核心检索函数 ---\n",
    "\n",
    "def search_similar_terms(query_text: str, model: SentenceTransformer, index: faiss.Index, term_list: list, k: int = 5):\n",
    "    \"\"\"\n",
    "    在FAISS索引中检索与查询文本最相似的k个术语。\n",
    "\n",
    "    Args:\n",
    "        query_text (str): 用户输入的查询词。\n",
    "        model (SentenceTransformer): 用于编码查询词的模型。\n",
    "        index (faiss.Index): FAISS索引对象。\n",
    "        term_list (list): 与索引向量顺序一致的术语列表。\n",
    "        k (int): 希望返回的最相似结果的数量。\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- 正在执行检索 ---\")\n",
    "    print(f\"查询: '{query_text}'\")\n",
    "    \n",
    "    # 1. 将查询文本编码为向量\n",
    "    query_vector = model.encode([query_text])\n",
    "    query_vector = query_vector.astype('float32')\n",
    "    \n",
    "    # 2. 在FAISS索引中执行搜索\n",
    "    # index.search返回两个数组：D (distances) 和 I (indices)\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "    \n",
    "    # 3. 解析并打印结果\n",
    "    print(\"检索结果:\")\n",
    "    for i in range(k):\n",
    "        idx = indices[0][i]\n",
    "        dist = distances[0][i]\n",
    "        term = term_list[idx]\n",
    "        \n",
    "        # 对于IndexFlatL2，距离是平方欧氏距离，距离越小代表越相似\n",
    "        print(f\"  Top {i+1}: 术语='{term}',  距离={dist:.4f} (值越小越相似)\")\n",
    "\n",
    "\n",
    "# 4. === 演示效果 ===\n",
    "\n",
    "# **案例一：用别名查询标准术语**\n",
    "# 目标：测试系统能否理解 \"CNN\" 就是 \"卷积神经网络\"。\n",
    "search_similar_terms(query_text=\"CNN\", model=model, index=faiss_index, term_list=indexed_term_list, k=3)\n",
    "\n",
    "# **案例二：语义相近查询（核心优势展示）**\n",
    "# 目标：查询一个不在我们词库中，但意思相近的词 \"计算机视觉\"。\n",
    "# 预期：系统应该能找到 \"图像识别\" 或 \"视觉识别\" 等相关术语。\n",
    "search_similar_terms(query_text=\"计算机视觉\", model=model, index=faiss_index, term_list=indexed_term_list, k=3)\n",
    "\n",
    "# **案例三：用一个更宽泛的词查询**\n",
    "# 目标：查询 \"语言模型\"，看是否能找到更具体的 \"大型语言模型\" 或 \"自然语言处理\"。\n",
    "search_similar_terms(query_text=\"语言模型\", model=model, index=faiss_index, term_list=indexed_term_list, k=3)\n",
    "\n",
    "# **案例四：测试对轻微噪声的容忍度**\n",
    "# 目标：查询一个不存在的、略有差异的词 \"变换器模型\"，看是否能正确匹配到 \"Transformer模型\"。\n",
    "search_similar_terms(query_text=\"变换器模型\", model=model, index=faiss_index, term_list=indexed_term_list, k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、检索增强阶段\n",
    "\n",
    "核心目标是在初步召回（Recall）的基础上，进一步优化检索结果的广度与精度。预处理阶段解决了术语的“标准”问题，而本阶段则聚焦于如何利用这些标准化的知识，在实际检索中发挥最大效用。\n",
    "\n",
    "| 技术名称                             | 描述                                                                                   | 对术语一致性的帮助                                                                 |\n",
    "|--------------------------------------|----------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|\n",
    "| 查询扩展与重写（MultiQueryRetriever） | 利用 LLM 生成多个语义等价的查询变体，合并检索结果。                                     | 自动覆盖用户未提及的同义词或相关表达，极大提升对多样化术语的识别与召回能力。         |\n",
    "| HyDE（假设文档嵌入）                 | 利用 LLM 为查询生成一个“理想答案”的假设性文档，再用该文档的嵌入进行检索。               | 通过生成富含上下文的理想答案，有效缓解原始查询中术语模糊或信息不足的问题，提升检索相关性。 |\n",
    "| 混合检索（BM25 + FAISS）             | 结合关键词检索（如 BM25）与向量检索（如 FAISS）的优势。                                 | 综合利用字面精确匹配和语义相似匹配，确保基础术语不丢失，同时发现语义相关内容。         |\n",
    "| 交叉编码器重排序（BGE-reranker）     | 使用更复杂的交叉编码器模型（如 BGE-reranker）对召回结果进行精细化重排序。               | 通过深度交互分析查询与文档的匹配度，提升对术语匹配度的排序精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain_community\n",
    "! pip install langchain langchain-openai faiss-cpu sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 核心技术一：查询扩展与重写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始查询: CNN是什么？\n",
      "\n",
      "--- MultiQueryRetriever 生成的查询变体 ---\n",
      "查询变体 1: 卷积神经网络的定义是什么？\n",
      "查询变体 2: CNN模型在深度学习中的作用是什么？\n",
      "查询变体 3: 介绍一下卷积神经网络（CNN）。\n",
      "\n",
      "--- 最终检索到的文档内容 ---\n",
      "卷积神经网络（CNN）是深度学习中的一种关键模型，尤其在图像识别领域表现出色。\n",
      "它的核心在于通过卷积层和池化层自动提取图像的局部特征。\n",
      "与CNN不同，Transformer模型最初应用于自然语言处理（NLP）任务，\n",
      "例如机器翻译。如今，它也被成功应用于计算机视觉，称为Vision Transformer。\n",
      "大型语言模型（LLM）是当前AI研究的热点，它基于Transformer架构，\n",
      "能够理解和生成类似人类的文本，展现出强大的推理能力。\n"
     ]
    }
   ],
   "source": [
    "## 查询扩展与重写\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import os\n",
    "\n",
    "# --- 准备工作：设置API Key并创建向量数据库 ---\n",
    "\n",
    "# 设置您的OpenAI API Key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "# 1. 准备示例文档\n",
    "# 我们创建一些包含专业术语的示例文本\n",
    "doc_text = \"\"\"\n",
    "卷积神经网络（CNN）是深度学习中的一种关键模型，尤其在图像识别领域表现出色。\n",
    "它的核心在于通过卷积层和池化层自动提取图像的局部特征。\n",
    "\n",
    "与CNN不同，Transformer模型最初应用于自然语言处理（NLP）任务，\n",
    "例如机器翻译。如今，它也被成功应用于计算机视觉，称为Vision Transformer。\n",
    "\n",
    "大型语言模型（LLM）是当前AI研究的热点，它基于Transformer架构，\n",
    "能够理解和生成类似人类的文本，展现出强大的推理能力。\n",
    "\"\"\"\n",
    "with open(\"sample_tech_doc.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(doc_text)\n",
    "\n",
    "# 2. 加载和切分文档\n",
    "loader = TextLoader(\"sample_tech_doc.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# 3. 创建向量数据库\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# --- MultiQueryRetriever 实现 ---\n",
    "\n",
    "# 4. 初始化LLM和检索器\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(), llm=llm\n",
    ")\n",
    "\n",
    "# 5. 执行查询\n",
    "query = \"CNN是什么？\"\n",
    "retrieved_docs = retriever_from_llm.invoke(query)\n",
    "\n",
    "# --- 效果分析 ---\n",
    "print(f\"原始查询: {query}\")\n",
    "print(\"\\n--- MultiQueryRetriever 生成的查询变体 ---\")\n",
    "# MultiQueryRetriever 内部有日志记录生成的查询，这里我们手动展示其可能生成的查询\n",
    "# 实际使用中可以通过设置 logging.basicConfig(level=logging.INFO) 查看\n",
    "generated_queries = [\n",
    "    \"卷积神经网络的定义是什么？\",\n",
    "    \"CNN模型在深度学习中的作用是什么？\",\n",
    "    \"介绍一下卷积神经网络（CNN）。\"\n",
    "]\n",
    "for i, q in enumerate(generated_queries):\n",
    "    print(f\"查询变体 {i+1}: {q}\")\n",
    "\n",
    "print(\"\\n--- 最终检索到的文档内容 ---\")\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 假设性文档嵌入 (HyDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade langchain langchain-community langchain-openai rank_bm25 faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文档已被切分为 3 个块。\n",
      "\n",
      "正在构建FAISS向量检索器...\n",
      "FAISS检索器构建完成。\n",
      "\n",
      "正在构建BM25关键词检索器...\n",
      "BM25检索器构建完成。\n",
      "\n",
      "正在初始化 MergerRetriever...\n",
      "MergerRetriever 初始化完成。\n",
      "\n",
      "\n",
      "--- 正在执行混合检索 ---\n",
      "查询: 'ViT的技术细节'\n",
      "\n",
      "--- 单独检索结果对比 ---\n",
      "【BM25 关键词检索结果】(共 3 条):\n",
      "  - 第三部分：关于大模型。\n",
      "大型语言模型（LLM）是当前AI研究的热点，它通常基于Transformer...\n",
      "  - 第二部分：关于Transformer。\n",
      "与CNN不同，Transformer模型最初应用于自然语言处...\n",
      "  - 第一部分：关于卷积网络。\n",
      "卷积神经网络（CNN）是深度学习中的一种关键模型，尤其在图像识别领域表现出...\n",
      "\n",
      "【FAISS 向量检索结果】(共 3 条):\n",
      "  - 第二部分：关于Transformer。\n",
      "与CNN不同，Transformer模型最初应用于自然语言处...\n",
      "  - 第一部分：关于卷积网络。\n",
      "卷积神经网络（CNN）是深度学习中的一种关键模型，尤其在图像识别领域表现出...\n",
      "  - 第三部分：关于大模型。\n",
      "大型语言模型（LLM）是当前AI研究的热点，它通常基于Transformer...\n",
      "\n",
      "--- MergerRetriever 混合检索结果 ---\n",
      "【最终混合结果】(共 6 条，已去重):\n",
      "  - 第三部分：关于大模型。\n",
      "大型语言模型（LLM）是当前AI研究的热点，它通常基于Transformer...\n",
      "  - 第二部分：关于Transformer。\n",
      "与CNN不同，Transformer模型最初应用于自然语言处...\n",
      "  - 第二部分：关于Transformer。\n",
      "与CNN不同，Transformer模型最初应用于自然语言处...\n",
      "  - 第一部分：关于卷积网络。\n",
      "卷积神经网络（CNN）是深度学习中的一种关键模型，尤其在图像识别领域表现出...\n",
      "  - 第一部分：关于卷积网络。\n",
      "卷积神经网络（CNN）是深度学习中的一种关键模型，尤其在图像识别领域表现出...\n",
      "  - 第三部分：关于大模型。\n",
      "大型语言模型（LLM）是当前AI研究的热点，它通常基于Transformer...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.retrievers import MergerRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# --- 1. 准备工作：设置API Key并准备数据 ---\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "# 准备示例文档，我们稍微丰富一下内容以便于对比\n",
    "doc_text = \"\"\"\n",
    "第一部分：关于卷积网络。\n",
    "卷积神经网络（CNN）是深度学习中的一种关键模型，尤其在图像识别领域表现出色。\n",
    "它的核心在于通过卷积层和池化层自动提取图像的局部特征。CNN的这个特性让它非常高效。\n",
    "\n",
    "第二部分：关于Transformer。\n",
    "与CNN不同，Transformer模型最初应用于自然语言处理（NLP）任务，\n",
    "例如机器翻译。如今，一种被称为Vision Transformer（ViT）的变体也被成功应用于计算机视觉领域。\n",
    "\n",
    "第三部分：关于大模型。\n",
    "大型语言模型（LLM）是当前AI研究的热点，它通常基于Transformer架构，\n",
    "能够理解和生成类似人类的文本，展现出强大的推理能力。\n",
    "\"\"\"\n",
    "with open(\"hybrid_search_doc.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(doc_text)\n",
    "\n",
    "# 加载和切分文档\n",
    "loader = TextLoader(\"hybrid_search_doc.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=120, chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"文档已被切分为 {len(docs)} 个块。\")\n",
    "\n",
    "\n",
    "# --- 2. 构建两个不同的检索器 ---\n",
    "\n",
    "# **检索器一：FAISS 向量检索器 (用于语义匹配)**\n",
    "print(\"\\n正在构建FAISS向量检索器...\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "faiss_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "print(\"FAISS检索器构建完成。\")\n",
    "\n",
    "\n",
    "# **检索器二：BM25 关键词检索器 (用于精确匹配)**\n",
    "print(\"\\n正在构建BM25关键词检索器...\")\n",
    "# BM25Retriever可以直接从文档列表初始化，它不需要嵌入模型\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "bm25_retriever.k = 3\n",
    "print(\"BM25检索器构建完成。\")\n",
    "\n",
    "\n",
    "# --- 3. 使用 MergerRetriever 合并 ---\n",
    "\n",
    "print(\"\\n正在初始化 MergerRetriever...\")\n",
    "# 创建一个检索器列表\n",
    "retriever_list = [bm25_retriever, faiss_retriever]\n",
    "\n",
    "# 初始化 MergerRetriever\n",
    "# 它会自动处理并行检索和结果去重\n",
    "merged_retriever = MergerRetriever(retrievers=retriever_list)\n",
    "print(\"MergerRetriever 初始化完成。\")\n",
    "\n",
    "\n",
    "# --- 4. 执行查询并对比效果 ---\n",
    "\n",
    "query = \"ViT的技术细节\"\n",
    "print(f\"\\n\\n--- 正在执行混合检索 ---\")\n",
    "print(f\"查询: '{query}'\")\n",
    "\n",
    "\n",
    "# **为了对比，我们先看看每个检索器单独工作的结果**\n",
    "print(\"\\n--- 单独检索结果对比 ---\")\n",
    "bm25_results = bm25_retriever.invoke(query)\n",
    "print(f\"【BM25 关键词检索结果】(共 {len(bm25_results)} 条):\")\n",
    "for doc in bm25_results:\n",
    "    print(f\"  - {doc.page_content[:50]}...\")\n",
    "\n",
    "faiss_results = faiss_retriever.invoke(query)\n",
    "print(f\"\\n【FAISS 向量检索结果】(共 {len(faiss_results)} 条):\")\n",
    "for doc in faiss_results:\n",
    "    print(f\"  - {doc.page_content[:50]}...\")\n",
    "\n",
    "\n",
    "# **现在看看 MergerRetriever 的混合结果**\n",
    "print(\"\\n--- MergerRetriever 混合检索结果 ---\")\n",
    "merged_results = merged_retriever.invoke(query)\n",
    "print(f\"【最终混合结果】(共 {len(merged_results)} 条，已去重):\")\n",
    "for doc in merged_results:\n",
    "    print(f\"  - {doc.page_content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、生成控制与输出验证阶段\n",
    "\n",
    "| 技术名称                  | 描述                                                                 | 对术语一致性的贡献与作用                                                                 |\n",
    "|---------------------------|----------------------------------------------------------------------|------------------------------------------------------------------------------------------|\n",
    "| 提示工程 (Prompt Engineering)       | 在提示中明确指令，引导 LLM 使用标准术语、保持特定风格。                | 最基础的控制手段，直接影响 LLM 的选词倾向，引导其遵循术语规范。                           |\n",
    "| 结构化输出 (Structured Output)     | 强制 LLM 返回符合预定义模式（如 Pydantic 或 JSON Schema）的对象。      | 从根本上杜绝术语的随意使用，确保关键信息以标准、可控的格式输出。                         |\n",
    "| 输出解析与修复 (Output Parsers)    | 使用如 OutputFixingParser 等工具，在 LLM 输出格式错误时自动尝试修复。  | 提升结构化输出的鲁棒性，能自动纠正轻微的术语格式或拼写错误。                               |\n",
    "| 后处理与内容增强                   | 在答案文本中自动高亮术语、添加定义弹窗或引用链接。                    | 提升最终答案的可读性和专业性，为用户提供即时的术语解释和来源追溯。                        |\n",
    "| LLM 即评委 (LLM-as-a-Judge)        | 使用另一个 LLM 实例，根据预设标准（如术语一致性）对生成结果进行打分评估。 | 提供一种可扩展的、自动化的输出质量与术语合规性评估方案。                                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 生成时控制：结构化输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 正在执行结构化输出链 ---\n",
      "\n",
      "--- LLM返回的结构化对象 ---\n",
      "answer='卷积神经网络（Convolutional Neural Network, CNN）是一种深度学习模型，专门用于处理具有网格结构的数据，例如图像。CNN通过使用卷积层来提取输入数据的特征，通常包括卷积层、池化层和全连接层。卷积层通过卷积运算提取局部特征，池化层用于减少特征图的尺寸，从而降低计算复杂度。\\n\\n主要应用领域包括：\\n\\n1. **图像识别**：CNN在图像分类任务中表现出色，能够识别和分类图像中的对象。\\n2. **目标检测**：用于在图像中定位和识别多个对象。\\n3. **图像分割**：将图像划分为不同的区域或对象。\\n4. **自然语言处理**：在文本分类和情感分析等任务中也有应用。\\n5. **医学影像分析**：用于分析医学图像，如X光片和MRI扫描。\\n6. **自动驾驶**：用于识别道路标志、行人和其他车辆。' standard_terms_used=['卷积神经网络', '图像识别', '目标检测', '图像分割', '自然语言处理', '医学影像分析', '自动驾驶']\n",
      "\n",
      "--- 对结果的分析 ---\n",
      "回答内容: 卷积神经网络（Convolutional Neural Network, CNN）是一种深度学习模型，专门用于处理具有网格结构的数据，例如图像。CNN通过使用卷积层来提取输入数据的特征，通常包括卷积层、池化层和全连接层。卷积层通过卷积运算提取局部特征，池化层用于减少特征图的尺寸，从而降低计算复杂度。\n",
      "\n",
      "主要应用领域包括：\n",
      "\n",
      "1. **图像识别**：CNN在图像分类任务中表现出色，能够识别和分类图像中的对象。\n",
      "2. **目标检测**：用于在图像中定位和识别多个对象。\n",
      "3. **图像分割**：将图像划分为不同的区域或对象。\n",
      "4. **自然语言处理**：在文本分类和情感分析等任务中也有应用。\n",
      "5. **医学影像分析**：用于分析医学图像，如X光片和MRI扫描。\n",
      "6. **自动驾驶**：用于识别道路标志、行人和其他车辆。\n",
      "模型确认使用的标准术语: ['卷积神经网络', '图像识别', '目标检测', '图像分割', '自然语言处理', '医学影像分析', '自动驾驶']\n",
      "验证通过：答案中正确地使用了标准术语'卷积神经网络'。\n",
      "\n",
      "==================================================\n",
      "--- 开始执行内容增强 ---\n",
      "\n",
      "--- 内容增强后的最终输出 (在支持HTML的Markdown渲染器中查看) ---\n",
      "<abbr title=\"一种特殊的<abbr title=\"机器学习的一个分支，它基于人工神经网络。\">深度学习</abbr>模型，擅长处理图像数据。\">卷积神经网络</abbr>（Convolutional Neural Network, CNN）是一种<abbr title=\"机器学习的一个分支，它基于人工神经网络。\">深度学习</abbr>模型，专门用于处理具有网格结构的数据，例如图像。CNN通过使用卷积层来提取输入数据的特征，通常包括卷积层、池化层和全连接层。卷积层通过卷积运算提取局部特征，池化层用于减少特征图的尺寸，从而降低计算复杂度。\n",
      "\n",
      "主要应用领域包括：\n",
      "\n",
      "1. **<abbr title=\"计算机视觉中的一个核心任务，旨在识别和分类图像中的对象。\">图像识别</abbr>**：CNN在图像分类任务中表现出色，能够识别和分类图像中的对象。\n",
      "2. **目标检测**：用于在图像中定位和识别多个对象。\n",
      "3. **图像分割**：将图像划分为不同的区域或对象。\n",
      "4. **自然语言处理**：在文本分类和情感分析等任务中也有应用。\n",
      "5. **医学影像分析**：用于分析医学图像，如X光片和MRI扫描。\n",
      "6. **自动驾驶**：用于识别道路标志、行人和其他车辆。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "# 修正点：直接从 pydantic 库导入 BaseModel 和 Field\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# --- 1. 准备工作 ---\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "\n",
    "# --- 2. 定义期望的输出结构 (使用 Pydantic V2) ---\n",
    "class TerminologyInAnswer(BaseModel):\n",
    "    \"\"\"一个包含标准答案和其中使用的技术术语的结构化模型。\"\"\"\n",
    "    answer: str = Field(description=\"对用户问题的详细、准确的回答。\")\n",
    "    standard_terms_used: List[str] = Field(\n",
    "        description=\"在回答中明确使用到的、来自官方词库的标准技术术语列表。\",\n",
    "        example=[\"卷积神经网络\", \"图像识别\"]\n",
    "    )\n",
    "\n",
    "# --- 3. 创建一个结构化输出链 ---\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一个精通AI技术的专家。请根据用户的问题，以结构化的形式提供答案。\"),\n",
    "    (\"human\", \"请解释一下什么是CNN，以及它的主要应用领域。\")\n",
    "])\n",
    "\n",
    "# 使用 .with_structured_output() 将LLM与我们的Pydantic V2模型绑定\n",
    "# 现在它会默认使用最高效的json_schema模式，而不会产生警告\n",
    "structured_llm_chain = prompt | llm.with_structured_output(TerminologyInAnswer)\n",
    "\n",
    "# --- 4. 执行链并查看结果 ---\n",
    "print(\"--- 正在执行结构化输出链 ---\")\n",
    "structured_response = structured_llm_chain.invoke({})\n",
    "\n",
    "print(\"\\n--- LLM返回的结构化对象 ---\")\n",
    "print(structured_response)\n",
    "\n",
    "print(\"\\n--- 对结果的分析 ---\")\n",
    "print(f\"回答内容: {structured_response.answer}\")\n",
    "print(f\"模型确认使用的标准术语: {structured_response.standard_terms_used}\")\n",
    "\n",
    "if \"卷积神经网络\" in structured_response.standard_terms_used:\n",
    "    print(\"验证通过：答案中正确地使用了标准术语'卷积神经网络'。\")\n",
    "\n",
    "\n",
    "# --- 5. 内容增强：自动添加术语解释 ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- 开始执行内容增强 ---\")\n",
    "\n",
    "# 假设我们有这样一个简化的术语词库\n",
    "glossary = {\n",
    "    \"卷积神经网络\": \"一种特殊的深度学习模型，擅长处理图像数据。\",\n",
    "    \"深度学习\": \"机器学习的一个分支，它基于人工神经网络。\",\n",
    "    \"图像识别\": \"计算机视觉中的一个核心任务，旨在识别和分类图像中的对象。\"\n",
    "}\n",
    "\n",
    "# 获取LLM生成的答案文本\n",
    "llm_answer_text = structured_response.answer\n",
    "\n",
    "def enhance_text_with_definitions(text: str, term_glossary: dict) -> str:\n",
    "    \"\"\"\n",
    "    在文本中查找标准术语，并为其添加Markdown格式的悬浮注释。\n",
    "    \"\"\"\n",
    "    enhanced_text = text\n",
    "    for term, definition in term_glossary.items():\n",
    "        # 创建带注释的Markdown/HTML格式\n",
    "        replacement = f'<abbr title=\"{definition}\">{term}</abbr>'\n",
    "        # 替换文本中的术语\n",
    "        enhanced_text = enhanced_text.replace(term, replacement)\n",
    "    return enhanced_text\n",
    "\n",
    "# 执行内容增强\n",
    "final_output = enhance_text_with_definitions(llm_answer_text, glossary)\n",
    "\n",
    "print(\"\\n--- 内容增强后的最终输出 (在支持HTML的Markdown渲染器中查看) ---\")\n",
    "print(final_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 生成后处理：验证与内容增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 内容增强后的输出 ---\n",
      "<abbr title=\"基于海量数据训练的、参数规模巨大的语言模型。\">大型语言模型</abbr>通常基于<abbr title=\"一种基于自注意力机制的神经网络结构，在NLP领域取得巨大成功。\">Transformer架构</abbr>，而<abbr title=\"一种特殊的深度学习模型，擅长处理图像数据。\">卷积神经网络</abbr>则在图像处理方面是主流。\n"
     ]
    }
   ],
   "source": [
    "# 假设我们有这样一个简化的术语词库\n",
    "glossary = {\n",
    "    \"卷积神经网络\": \"一种特殊的深度学习模型，擅长处理图像数据。\",\n",
    "    \"Transformer架构\": \"一种基于自注意力机制的神经网络结构，在NLP领域取得巨大成功。\",\n",
    "    \"大型语言模型\": \"基于海量数据训练的、参数规模巨大的语言模型。\"\n",
    "}\n",
    "\n",
    "# 假设这是LLM返回的、已经验证过的答案文本\n",
    "llm_answer_text = \"大型语言模型通常基于Transformer架构，而卷积神经网络则在图像处理方面是主流。\"\n",
    "\n",
    "def enhance_text_with_definitions(text: str, term_glossary: dict) -> str:\n",
    "    \"\"\"\n",
    "    在文本中查找标准术语，并为其添加Markdown格式的悬浮注释。\n",
    "    在支持HTML的Markdown渲染器中，这通常会显示为鼠标悬停提示。\n",
    "    \"\"\"\n",
    "    enhanced_text = text\n",
    "    for term, definition in term_glossary.items():\n",
    "        # 创建带注释的Markdown/HTML格式\n",
    "        replacement = f'<abbr title=\"{definition}\">{term}</abbr>'\n",
    "        # 替换文本中的术语\n",
    "        enhanced_text = enhanced_text.replace(term, replacement)\n",
    "    return enhanced_text\n",
    "\n",
    "# 执行内容增强\n",
    "final_output = enhance_text_with_definitions(llm_answer_text, glossary)\n",
    "\n",
    "print(\"\\n--- 内容增强后的输出 ---\")\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六、评估与反馈机制\n",
    "\n",
    "人工抽样评估难以覆盖海量的生成内容，而“LLM即评委”为此提供了一个高效、可扩展的自动化解决方案。其核心是利用一个LLM的理解和推理能力，来评估另一个LLM（或整个RAG系统）的输出质量。\n",
    "\n",
    "使用LCEL构建评估链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 正在评估【优秀回答】---\n",
      "{'准确性': {'评价': '高', '说明': \"回答中正确使用了'大语言模型'、'Transformer模型'和'卷积神经网络'等标准术语。\"}, '规范性': {'评价': '高', '说明': '回答中避免了使用非官方的、易产生歧义的别名，使用的术语均为官方标准术语。'}, '全面性': {'评价': '高', '说明': '回答在必要时使用了最能表达其意的标准术语，清晰地传达了大语言模型与Transformer模型的关系，以及卷积神经网络的应用领域。'}}\n",
      "\n",
      "--- 正在评估【待改进回答】---\n",
      "{'准确性': {'评价': '不准确', '问题': \"使用了不正确的术语'卷基神经网络'，应为'卷积神经网络'。\"}, '规范性': {'评价': '不规范', '问题': \"使用了非官方的术语'大模型'，应为'大语言模型'或其别名'LLM'。\"}, '全面性': {'评价': '不全面', '问题': \"未使用最能表达其意的标准术语'Transformer模型'，而是使用了非标准的'变换器架构'。\"}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# --- 1. 准备工作 ---\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "evaluator_llm = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "\n",
    "# --- 2. 定义评估结果的结构化模型 ---\n",
    "class TerminologyEvaluation(BaseModel):\n",
    "    \"\"\"用于评估术语一致性的结构化模型。\"\"\"\n",
    "    consistency_score: int = Field(description=\"一个从1到5的分数，5代表完全一致，1代表严重不一致。\")\n",
    "    is_consistent: bool = Field(description=\"布尔值，是否整体上符合术语规范。\")\n",
    "    reasoning: str = Field(description=\"对评分的详细解释，指出做得好的地方和存在的问题。\")\n",
    "    suggestions_for_improvement: List[str] = Field(description=\"为改进答案中的术语使用提出的具体建议。\")\n",
    "\n",
    "# --- 3. 构建评估链 ---\n",
    "# 创建一个专门的评估提示\n",
    "evaluation_prompt_template = \"\"\"\n",
    "你是一个严格的AI技术文档质量评估员，你的核心任务是评估一段回答在术语使用上的一致性和准确性。\n",
    "\n",
    "**评估标准:**\n",
    "1.  **准确性**: 是否正确地使用了标准术语？\n",
    "2.  **规范性**: 是否避免了使用非官方的、易产生歧义的别名？\n",
    "3.  **全面性**: 是否在必要时使用了最能表达其意的标准术语？\n",
    "\n",
    "**权威术语词库 (部分):**\n",
    "- 卷积神经网络 (别名: CNN)\n",
    "- Transformer模型 (别名: Transformer)\n",
    "- 大语言模型 (别名: LLM)\n",
    "\n",
    "**待评估的回答:**\n",
    "{answer_text}\n",
    "\n",
    "请根据以上标准和词库，以JSON格式输出你的评估结果。\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(evaluation_prompt_template)\n",
    "parser = JsonOutputParser(pydantic_object=TerminologyEvaluation)\n",
    "\n",
    "# 使用LCEL构建评估链\n",
    "evaluation_chain = prompt | evaluator_llm | parser\n",
    "\n",
    "# --- 4. 执行评估 ---\n",
    "\n",
    "# 案例一：一个使用规范的回答\n",
    "good_answer = \"大语言模型（LLM）是基于Transformer模型构建的，而卷积神经网络（CNN）则在图像领域应用广泛。\"\n",
    "\n",
    "# 案例二：一个使用了非标准术语的回答\n",
    "bad_answer = \"大模型是基于变换器架构的，而卷基神经网络在图片处理上很强。\"\n",
    "\n",
    "print(\"--- 正在评估【优秀回答】---\")\n",
    "good_evaluation_result = evaluation_chain.invoke({\"answer_text\": good_answer})\n",
    "print(good_evaluation_result)\n",
    "\n",
    "\n",
    "print(\"\\n--- 正在评估【待改进回答】---\")\n",
    "bad_evaluation_result = evaluation_chain.invoke({\"answer_text\": bad_answer})\n",
    "print(bad_evaluation_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结：术语一致性优化路线图\n",
    "\n",
    "经过以上各阶段的详细探讨，从数据预处理到最终的评估反馈，我们已经全面构建了保障术语一致性的技术体系。\n",
    "\n",
    "为了更直观地理解各项技术的定位与优先级，我们将整个优化策略总结为以下分级路线图，为不同阶段的RAG系统建设提供实践指引。\n",
    "\n",
    "| 优化层级                | 核心技术与解决方案                                                                 |\n",
    "|-------------------------|------------------------------------------------------------------------------------|\n",
    "| 基础核心 (Foundation)    | 术语词库构建、术语抽取、预处理标准化、术语嵌入与向量索引                             |\n",
    "| 关键增强 (Key Enhancement)| 混合检索 (BM25 + 向量)、查询扩展 (MultiQuery)、假设性文档嵌入 (HyDE)、交叉编码器重排序 |\n",
    "| 辅助优化 (Auxiliary Optimization) | 领域专用嵌入模型微调、上下文感知分块                                               |\n",
    "| 生成控制 (Generation Control)     | 提示工程、结构化输出、输出解析与修复                                               |\n",
    "| 长期保障 (Long-term Assurance)    | LLM即评委 (LLM-as-a-Judge)、用户反馈闭环、日志审计与分析                            |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
