{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 领域术语总混淆？教你构建精准术语词库，提升检索一致性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在RAG系统构建过程中，术语混淆直接影响信息检索的精准度与生成内容的质量。\n",
    "\n",
    "这主要源于几个方面：\n",
    "- 向量表示\n",
    "- 不同行业、公司乃至同一组织内部，都可能存在相似词汇却拥有截然不同含义的情况\n",
    "\n",
    "这些因素最终导致检索结果偏离预期，大幅降低了答案的质量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、术语词库构建与维护（Glossary Management）\n",
    "## 1.1 产生术语混淆的原因\n",
    "\n",
    "- 术语多义性\n",
    "- 同义词与近义词\n",
    "- 领域差异\n",
    "- 企业专属术语"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 构建术语词库的目标\n",
    "术语词库是整个术语一致性优化体系的核心基础设施。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 术语词库的构建流程\n",
    "- Step 1：收集术语来源\n",
    "- Step 2：标准化术语\n",
    "- Step 3：建立别名映射关系\n",
    "- Step 4：添加上下文信息\n",
    "- Step 5：构建术语索引\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个功能完善的术语词库应包含以下关键字段，以确保其结构化和可操作性：\n",
    "\n",
    "| 字段名                 | 内容                                                                                   |\n",
    "|------------------------|----------------------------------------------------------------------------------------|\n",
    "| 术语（Term）           | 神经网络                                                                              |\n",
    "| 别名（Synonyms）       | [\"人工神经网络\", \"NN\"]                                                                |\n",
    "| 定义（Definition）     | 神经网络是一种模仿生物神经网络结构和功能的计算模型……                                  |\n",
    "| 上下文标签（Context Tags） | [\"人工智能\", \"深度学习\", \"计算机科学\"]                                                 |\n",
    "| 所属领域（Domain）     | 人工智能                                                                              |\n",
    "| 示例用法（Usage Example） | 在图像识别任务中，我们使用了一个卷积神经网络。                                        |\n",
    "| 外部链接（External Link） | [维基百科链接](https://en.wikipedia.org/wiki/Artificial_neural_network)               |\n",
    "| 禁用词/误导词（Stop Words / Misleading Terms） | [\"神经系统\"（医学中的不同概念）]                                                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 术语词库与 RAG 集成\n",
    "\n",
    "- 方式一：预处理阶段替换术语\n",
    "- 方式二：检索增强\n",
    "- 方式三：重排序（Re-ranking）\n",
    "- 方式四：后处理解释"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 术语词库维护\n",
    "1. 术语词库结构设计\n",
    "这是基础，确定词库所需包含的字段和它们之间的关系。\n",
    "\n",
    "2. 自动抽取术语候选\n",
    "利用 NLP 工具从大量文本中自动识别和提取潜在的术语。\n",
    "\n",
    "3. 专家审核与完善\n",
    "领域专家对自动抽取的术语进行人工审核、修正和补充，确保准确性和专业性。\n",
    "\n",
    "4. 构建术语关系图谱\n",
    "如果有需求，可以进一步构建术语之间的层次、关联关系，形成本体（Ontology）或知识图谱（Knowledge Graph），以提升语义理解能力。\n",
    "\n",
    "5. 版本控制与更新机制建设\n",
    "建立术语词库的版本管理和定期更新机制，确保其时效性和权威性，应对新术语的出现或旧术语含义的变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 阶段             | 技术名称                                 | 描述                                                                                                                                               |\n",
    "|------------------|------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| 1. 数据预处理    | 术语抽取、标准化、上下文分块             | 在原始文档和查询进入 RAG 系统之前，识别并提取领域术语，进行统一化处理，并确保文本分块时能有效保留术语的上下文信息。                                   |\n",
    "| 2. 术语词库构建  | 词库设计、术语关系建模、版本管理         | 建立结构化的术语词库，包含术语、别名、定义、上下文标签等字段。进一步可构建术语间的层级或关联关系（如本体），并建立完善的版本控制与更新机制。           |\n",
    "| 3. 嵌入与向量化  | 构建术语向量索引、微调领域嵌入模型       | 将术语词库中的标准术语和别名转换为向量，并构建高效的向量索引（如 FAISS）。同时，通过领域适应性训练（如 LoRA）优化通用嵌入模型，使其更好地理解领域特有概念。 |\n",
    "| 4. 检索增强      | 查询扩展、混合检索、重排序、元数据过滤   | 利用术语词库对用户查询进行扩展（添加别名），结合向量检索与关键词检索（混合检索）。在召回结果后，通过术语匹配度进行重排序，或利用术语作为元数据进行更精确的过滤。 |\n",
    "| 5. 生成控制      | 提示工程、结构化输出、术语验证           | 设计包含术语词库信息的提示词，引导大模型生成更准确的答案。在输出阶段，可强制模型使用词库中的标准术语，并对生成内容进行术语验证，避免出现混淆或不规范表达。     |\n",
    "| 6. 评估与反馈    | 术语一致性指标、LLM-as-a-Judge、用户反馈 | 建立专门的评估指标来衡量 RAG 系统在术语一致性方面的表现。利用大型语言模型作为评估器（LLM-as-a-Judge）来检查术语使用情况，并收集用户反馈，持续优化词库和系统。 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 玩具版代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2. Data Preprocessing: Terminology Standardization ---\n",
      "Original query: I want to learn about applications of ML models in image recognition, and also some NLP knowledge.\n",
      "Standardized query: I want to learn about applications of Machine Learning models in image recognition, and also some Natural Language Processing knowledge.\n",
      "Original document: Recently I studied CNN and AI algorithms, and found they perform well on big data, especially ML in certain scenarios.\n",
      "Standardized document: Recently I studied Convolutional Neural Network and Machine Learning, and found they perform well on big data, especially Machine Learning in certain scenarios.\n",
      "\n",
      "--- 3. Term Extraction ---\n",
      "Extracted terms from query: ['Machine Learning', 'Natural Language Processing']\n",
      "Extracted terms from document: ['Convolutional Neural Network', 'Machine Learning']\n",
      "\n",
      "--- 4. Simulated Vector Storage and Retrieval Augmentation (Conceptual) ---\n",
      "In a real application, we would use an embedding model (e.g., SentenceTransformers) to convert the standardized text and terms into vectors.\n",
      "These vectors would then be stored in a dedicated vector database (e.g., FAISS, Pinecone, or Weaviate) for efficient similarity search.\n",
      "During retrieval, the user query is first standardized and vectorized, then used to query the vector database to fetch relevant documents.\n",
      "\n",
      "--- 5. Simulated Retrieval Augmentation: Query Expansion ---\n",
      "Original retrieval query: I want to know what a CPU does in a computer, and what cost-per-unit CPU means?\n",
      "Expanded retrieval keyword list: ['CPU', 'Central Processing Unit', 'I want to know what a Central Processing Unit does in a computer, and what cost-per-unit Central Processing Unit means?']\n",
      "\n",
      "In a production RAG system, these expanded keywords would drive a hybrid retrieval strategy, combining semantic (vector) search with keyword-based search for best results.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# 1. Define a terminology glossary (keep it updated, including context tags)\n",
    "GLOSSARY = [\n",
    "    {\n",
    "        \"term\": \"Convolutional Neural Network\",\n",
    "        \"synonyms\": [\"CNN\", \"Convolution-based neural network\"],\n",
    "        \"definition\": \"A computational model inspired by biological neural networks, especially well-suited for image processing.\",\n",
    "        \"context_tags\": [\"image recognition\", \"deep learning\"],\n",
    "    },\n",
    "    {\n",
    "        \"term\": \"Machine Learning\",\n",
    "        \"synonyms\": [\"ML\", \"Machine learn\", \"AI algorithms\"],\n",
    "        \"definition\": \"A field of artificial intelligence that enables computer systems to learn from data without being explicitly programmed.\",\n",
    "        \"context_tags\": [\"artificial intelligence\", \"data science\"],\n",
    "    },\n",
    "    {\n",
    "        \"term\": \"Natural Language Processing\",\n",
    "        \"synonyms\": [\"NLP\", \"natural language\"],\n",
    "        \"definition\": \"A field that studies the interaction between human language and computers.\",\n",
    "        \"context_tags\": [\"artificial intelligence\", \"linguistics\"],\n",
    "    },\n",
    "    {\n",
    "        \"term\": \"Central Processing Unit\",\n",
    "        \"synonyms\": [\"CPU\"],\n",
    "        \"definition\": \"The arithmetic, logic, and control unit of a computer.\",\n",
    "        \"context_tags\": [\"computer hardware\", \"computer\"],\n",
    "    },\n",
    "    {\n",
    "        \"term\": \"Cost per Unit\",\n",
    "        \"synonyms\": [\"CPU\"],\n",
    "        \"definition\": \"A business analytics metric that measures the cost per unit of a product or service.\",\n",
    "        \"context_tags\": [\"business analytics\", \"financial management\", \"cost\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "class TerminologyProcessor:\n",
    "    def __init__(self, glossary: List[Dict[str, Any]]):\n",
    "        self.glossary = glossary\n",
    "        self.standard_term_map = {}\n",
    "        self.alias_to_entries_map = {}\n",
    "        self._build_mappings()\n",
    "\n",
    "    def _build_mappings(self):\n",
    "        \"\"\"Build mappings; one alias may map to multiple terminology entries to handle ambiguity.\"\"\"\n",
    "        for entry in self.glossary:\n",
    "            standard_term = entry[\"term\"]\n",
    "            self.standard_term_map[standard_term.lower()] = standard_term\n",
    "\n",
    "            all_aliases = [standard_term] + entry.get(\"synonyms\", [])\n",
    "            for alias in all_aliases:\n",
    "                alias_lower = alias.lower()\n",
    "                if alias_lower not in self.alias_to_entries_map:\n",
    "                    self.alias_to_entries_map[alias_lower] = []\n",
    "                if entry not in self.alias_to_entries_map[alias_lower]:\n",
    "                    self.alias_to_entries_map[alias_lower].append(entry)\n",
    "\n",
    "    def standardize_text(self, text: str, context_window: int = 10) -> str:\n",
    "        \"\"\"\n",
    "        Context-aware terminology standardization using iteration + a replacement function.\n",
    "        Dynamically generates the correct regex for each term type.\n",
    "        \"\"\"\n",
    "        standardized_text = text\n",
    "        sorted_keys = sorted(self.alias_to_entries_map.keys(), key=len, reverse=True)\n",
    "\n",
    "        for key_lower in sorted_keys:\n",
    "            possible_entries = self.alias_to_entries_map[key_lower]\n",
    "\n",
    "            # --- Dynamically create the correct regex for each key ---\n",
    "            pattern_str = \"\"\n",
    "            # If key contains Latin letters, assume it's an abbreviation and enforce boundaries\n",
    "            if re.search(r\"[a-zA-Z]\", key_lower):\n",
    "                # Use lookarounds to avoid matching inside a larger word\n",
    "                pattern_str = r\"(?<![a-zA-Z])\" + re.escape(key_lower) + r\"(?![a-zA-Z])\"\n",
    "            else:\n",
    "                # For Chinese (or non-Latin) terms, match exactly\n",
    "                pattern_str = re.escape(key_lower)\n",
    "\n",
    "            pattern = re.compile(pattern_str, flags=re.IGNORECASE)\n",
    "\n",
    "            # Replacement function called for each match\n",
    "            def replacer(match: re.Match) -> str:\n",
    "                if len(possible_entries) == 1:\n",
    "                    return possible_entries[0][\"term\"]\n",
    "                else:\n",
    "                    # --- Context-based disambiguation ---\n",
    "                    context_snippet = standardized_text[\n",
    "                        max(0, match.start() - context_window) : min(len(standardized_text), match.end() + context_window)\n",
    "                    ]\n",
    "                    for entry in possible_entries:\n",
    "                        clues = entry.get(\"context_tags\", []) + [entry[\"term\"]]\n",
    "                        if any(clue in context_snippet for clue in clues):\n",
    "                            return entry[\"term\"]\n",
    "                    # If no context clue is found, fall back to the first definition\n",
    "                    return possible_entries[0][\"term\"]\n",
    "\n",
    "            # Update text using the replacement function\n",
    "            standardized_text = pattern.sub(replacer, standardized_text)\n",
    "\n",
    "        return standardized_text\n",
    "\n",
    "    def extract_terms(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Extract known standardized terms from text\n",
    "        \"\"\"\n",
    "        found_terms = set()\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        for standard_term_lower, original_standard_term in self.standard_term_map.items():\n",
    "            # Direct substring search; do not use \\b\n",
    "            if re.search(re.escape(standard_term_lower), text_lower):\n",
    "                found_terms.add(original_standard_term)\n",
    "\n",
    "        return sorted(list(found_terms))\n",
    "\n",
    "\n",
    "# 1. Initialize the terminology processor with the glossary.\n",
    "term_processor = TerminologyProcessor(GLOSSARY)\n",
    "\n",
    "# 2. Data preprocessing: terminology standardization\n",
    "print(\"--- 2. Data Preprocessing: Terminology Standardization ---\")\n",
    "user_query = \"I want to learn about applications of ML models in image recognition, and also some NLP knowledge.\"\n",
    "processed_query = term_processor.standardize_text(user_query)\n",
    "print(f\"Original query: {user_query}\")\n",
    "print(f\"Standardized query: {processed_query}\")\n",
    "\n",
    "document_text = \"Recently I studied CNN and AI algorithms, and found they perform well on big data, especially ML in certain scenarios.\"\n",
    "processed_document = term_processor.standardize_text(document_text)\n",
    "print(f\"Original document: {document_text}\")\n",
    "print(f\"Standardized document: {processed_document}\")\n",
    "\n",
    "# 3. Term extraction (for downstream vectorization or metadata tagging)\n",
    "print(\"\\n--- 3. Term Extraction ---\")\n",
    "extracted_terms_query = term_processor.extract_terms(processed_query)\n",
    "print(f\"Extracted terms from query: {extracted_terms_query}\")\n",
    "\n",
    "extracted_terms_document = term_processor.extract_terms(processed_document)\n",
    "print(f\"Extracted terms from document: {extracted_terms_document}\")\n",
    "\n",
    "# 4. Simulated vector storage and retrieval augmentation (conceptual)\n",
    "print(\"\\n--- 4. Simulated Vector Storage and Retrieval Augmentation (Conceptual) ---\")\n",
    "print(\"In a real application, we would use an embedding model (e.g., SentenceTransformers) to convert the standardized text and terms into vectors.\")\n",
    "print(\"These vectors would then be stored in a dedicated vector database (e.g., FAISS, Pinecone, or Weaviate) for efficient similarity search.\")\n",
    "print(\"During retrieval, the user query is first standardized and vectorized, then used to query the vector database to fetch relevant documents.\")\n",
    "\n",
    "# 5. Simulated retrieval augmentation: query expansion\n",
    "def enhance_query_for_retrieval(query: str, processor: TerminologyProcessor) -> List[str]:\n",
    "    \"\"\"Expand query keywords using the terminology glossary to improve recall.\"\"\"\n",
    "    standardized_query = processor.standardize_text(query)\n",
    "    query_terms = processor.extract_terms(standardized_query)\n",
    "\n",
    "    expanded_keywords = set([standardized_query])\n",
    "    for term in query_terms:\n",
    "        expanded_keywords.add(term)\n",
    "        for entry in processor.glossary:\n",
    "            if entry[\"term\"] == term:\n",
    "                for synonym in entry.get(\"synonyms\", []):\n",
    "                    expanded_keywords.add(synonym)\n",
    "                break\n",
    "    return sorted(list(expanded_keywords))\n",
    "\n",
    "\n",
    "print(\"\\n--- 5. Simulated Retrieval Augmentation: Query Expansion ---\")\n",
    "original_query_for_retrieval = \"I want to know what a CPU does in a computer, and what cost-per-unit CPU means?\"\n",
    "expanded_keywords = enhance_query_for_retrieval(original_query_for_retrieval, term_processor)\n",
    "print(f\"Original retrieval query: {original_query_for_retrieval}\")\n",
    "print(f\"Expanded retrieval keyword list: {expanded_keywords}\")\n",
    "\n",
    "print(\"\\nIn a production RAG system, these expanded keywords would drive a hybrid retrieval strategy, combining semantic (vector) search with keyword-based search for best results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、数据预处理阶段（Preprocessing）：提升语义表示质量\n",
    "\n",
    "这是术语一致性优化的“第一道防线”，直接影响后续所有环节的质量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 技术名称                          | 描述                                       | 对术语一致性的帮助                           |\n",
    "|-----------------------------------|--------------------------------------------|----------------------------------------------|\n",
    "| 术语抽取（NER、TF-IDF、KeyBERT）  | 自动从语料中识别候选术语                   | 提供术语来源，是词库构建的基础               |\n",
    "| 术语标准化（Term Normalization）  | 替换非标准表达为统一术语（如“AI”→“人工智能”） | 消除输入噪声，确保术语表达一致               |\n",
    "| 文本清洗与格式统一                | 清理无意义内容、统一大小写、标点等         | 减少干扰，提升术语识别准确率                 |\n",
    "| 上下文感知分块策略（SemanticChunker） | 按语义相似度切分文本块                     | 保留术语所在上下文信息，避免割裂语义         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 环境准备\n",
    "首先，确保你安装了必要的Python库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install transformers sentence-transformers faiss-cpu scikit-learn spacy\n",
    "# ! python -m spacy download en_core_web_sm\n",
    "# ! pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤一：术语词库结构设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a structured JSON that defines the core of our knowledge system\n",
    "term_glossary = {\n",
    "    \"Neural Network\": {\n",
    "        \"synonyms\": [\"Artificial Neural Network\", \"NN\"],\n",
    "        \"definition\": \"A computational model that mimics the structure and function of biological neural networks\",\n",
    "        \"context_tags\": [\"Artificial Intelligence\", \"Deep Learning\"],\n",
    "        \"domain\": \"Computer Science\",\n",
    "        \"stop_words\": [\"Nervous System\"]\n",
    "    },\n",
    "    \"Convolutional Neural Network\": {\n",
    "        \"synonyms\": [\"CNN\", \"ConvNet\"],\n",
    "        \"definition\": \"A deep learning model that extracts local features through convolutional layers\",\n",
    "        \"context_tags\": [\"Computer Vision\", \"Image Recognition\"],\n",
    "        \"domain\": \"Artificial Intelligence\",\n",
    "        \"stop_words\": []\n",
    "    },\n",
    "    \"Image Recognition\": { \n",
    "        \"synonyms\": [],\n",
    "        \"definition\": \"The task of enabling computers to identify and interpret the content of images\",\n",
    "        \"context_tags\": [\"Computer Vision\"],\n",
    "        \"domain\": \"Artificial Intelligence\",\n",
    "        \"stop_words\": []\n",
    "    },\n",
    "    \"Autonomous Driving\": { \n",
    "        \"synonyms\": [],\n",
    "        \"definition\": \"Technology that enables vehicles to operate without human intervention\",\n",
    "        \"context_tags\": [\"Artificial Intelligence\", \"Robotics\"],\n",
    "        \"domain\": \"Computer Science\",\n",
    "        \"stop_words\": []\n",
    "    },\n",
    "    \"Medical Imaging Diagnosis\": {\n",
    "        \"synonyms\": [],\n",
    "        \"definition\": \"The use of medical imaging to diagnose diseases\",\n",
    "        \"context_tags\": [\"Healthcare\", \"Image Processing\"],\n",
    "        \"domain\": \"Medicine\",\n",
    "        \"stop_words\": []\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤二：2.1 术语抽取与标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically extracted term candidates: {'CNN'}\n"
     ]
    }
   ],
   "source": [
    "# Use spaCy's EntityRuler to customize entity recognition rules\n",
    "# based on our terminology glossary\n",
    "import spacy\n",
    "\n",
    "def extract_terms_with_ruler(text, glossary):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    # Create an EntityRuler pipeline and load all terms and their aliases\n",
    "    # from the glossary\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "    patterns = []\n",
    "    for term, data in glossary.items():\n",
    "        patterns.append({\"label\": \"TERM\", \"pattern\": term})\n",
    "        for syn in data.get(\"synonyms\", []):\n",
    "            patterns.append({\"label\": \"TERM\", \"pattern\": syn})\n",
    "    ruler.add_patterns(patterns)\n",
    "    \n",
    "    # Process the text and extract entities recognized as \"TERM\"\n",
    "    doc = nlp(text)\n",
    "    candidates = {ent.text for ent in doc.ents if ent.label_ == \"TERM\"}\n",
    "    return candidates\n",
    "\n",
    "# Example\n",
    "text = \"Examples of CNN model applications in image recognition include autonomous driving and medical imaging diagnosis.\"\n",
    "candidates = extract_terms_with_ruler(text, term_glossary)\n",
    "print(f\"Automatically extracted term candidates: {candidates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、嵌入构建与向量化阶段（Embedding & Vectorization）\n",
    "\n",
    "核心任务是将这些经过清洗和标准化的术语，转化为机器能够理解和计算的密集向量（Dense Vectors），并构建高效的检索索引。这直接决定了系统语义匹配的能力上限。\n",
    "\n",
    "| 技术名称                                 | 描述                                       | 对术语一致性的帮助                             |\n",
    "|------------------------------------------|--------------------------------------------|------------------------------------------------|\n",
    "| 术语嵌入与向量索引（FAISS / Pinecone）   | 将术语及其别名转换为向量并构建索引         | 支持语义匹配，提升检索时的术语识别能力         |\n",
    "| 域专用嵌入模型（Legal-BERT、ChatLaw-Text2Vec） | 在专业语料上继续训练通用模型               | 提升术语理解质量，增强语义表示                 |\n",
    "| Sentence Transformers + PEFT（LoRA）微调 | 参数高效微调嵌入模型                       | 针对特定领域进一步优化术语语义表示             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 步骤二：2.2 基于向量相似度的同义词发现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linghuang/miniconda3/envs/llmops/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimized matched synonyms: {'Convolutional Neural Network': [], 'Neural Network': ['Artificial Neural Network']}\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# It is recommended to load the model once during project initialization\n",
    "# to avoid repeated loading overhead.\n",
    "# model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def map_synonyms_by_similarity(main_terms: list, candidates: list, threshold: float = 0.8) -> dict:\n",
    "    \"\"\"\n",
    "    Map candidate terms to the closest standard terms by computing\n",
    "    cosine similarity between embeddings.\n",
    "\n",
    "    Args:\n",
    "        main_terms (list): List of standard (canonical) terms.\n",
    "        candidates (list): List of candidate synonyms to be matched.\n",
    "        threshold (float): Similarity threshold above which a candidate\n",
    "                           is considered a synonym.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping each standard term to a list of\n",
    "              successfully matched synonyms.\n",
    "    \"\"\"\n",
    "    _matched_synonyms = {term: [] for term in main_terms}\n",
    "\n",
    "    if not main_terms or not candidates:\n",
    "        return _matched_synonyms\n",
    "    \n",
    "    model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Encode in batches for better efficiency\n",
    "    embeddings = model.encode(main_terms + candidates, convert_to_tensor=True)\n",
    "    term_embeddings = embeddings[:len(main_terms)]\n",
    "    candidate_embeddings = embeddings[len(main_terms):]\n",
    "\n",
    "    # Compute the cosine similarity matrix between standard terms and candidates\n",
    "    similarity_matrix = util.cos_sim(term_embeddings, candidate_embeddings)\n",
    "\n",
    "    for i, term in enumerate(main_terms):\n",
    "        for j, candidate in enumerate(candidates):\n",
    "            if similarity_matrix[i][j] > threshold:\n",
    "                _matched_synonyms[term].append(candidate)\n",
    "\n",
    "    return _matched_synonyms\n",
    "\n",
    "\n",
    "# Example:\n",
    "main_terms_to_map = [\"Convolutional Neural Network\", \"Neural Network\"]\n",
    "all_possible_synonyms = [\n",
    "    \"CNN\",\n",
    "    \"ConvNet\",\n",
    "    \"Artificial Neural Network\",\n",
    "    \"NN\",\n",
    "    \"Nervous System\",\n",
    "    \"Deep Learning Model\",\n",
    "]\n",
    "\n",
    "optimized_mapped_synonyms = map_synonyms_by_similarity(\n",
    "    main_terms_to_map,\n",
    "    all_possible_synonyms\n",
    ")\n",
    "print(\"\\nOptimized matched synonyms:\", optimized_mapped_synonyms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 步骤三：构建术语向量索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "s = requests.Session()\n",
    "s.trust_env = False\n",
    "print(s.get(\"https://huggingface.co\", timeout=10).status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting to load model 'paraphrase-MiniLM-L6-v2'...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# (Optional but recommended) Explicit cache directory\n",
    "os.environ.setdefault(\"HF_HOME\", os.path.expanduser(\"~/.cache/huggingface\"))\n",
    "os.environ.setdefault(\"TRANSFORMERS_CACHE\", os.path.expanduser(\"~/.cache/huggingface/transformers\"))\n",
    "\n",
    "model_name = 'paraphrase-MiniLM-L6-v2'\n",
    "\n",
    "print(f\"\\nAttempting to load model '{model_name}'...\")\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating term embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built successfully. Vectors: 9, Dimension: 384, Metric: cosine similarity\n",
      "\n",
      "--- Index Build Successful ---\n",
      "Number of vectors in FAISS index: 9\n",
      "Indexed terms: ['CNN', 'ConvNet', 'Convolutional Neural Network', 'Image Classification', 'Image Recognition', 'TRANSFORMER', 'Transformer', 'Visual Recognition', 'transformer']\n",
      "\n",
      "--- Query Tests ---\n",
      "[('CNN', 1.0), ('ConvNet', 0.46894609928131104), ('Convolutional Neural Network', 0.4410366714000702), ('Image Classification', 0.3689947724342346), ('Visual Recognition', 0.3341491222381592)]\n",
      "[('Image Classification', 0.9950429201126099), ('Image Recognition', 0.8483021855354309), ('Visual Recognition', 0.6152772307395935), ('CNN', 0.3614282011985779), ('Convolutional Neural Network', 0.35928869247436523)]\n",
      "[('transformer', 0.8794118165969849), ('TRANSFORMER', 0.7784579992294312), ('Transformer', 0.7107824683189392), ('Image Classification', 0.31479692459106445), ('Image Recognition', 0.289703369140625)]\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "\n",
    "def build_term_vector_index(\n",
    "    term_glossary: Dict[str, dict],\n",
    "    model: SentenceTransformer,\n",
    "    use_cosine: bool = True\n",
    ") -> Tuple[faiss.Index, List[str]]:\n",
    "    \"\"\"\n",
    "    Convert all terms and their synonyms into vector embeddings and build a FAISS index.\n",
    "\n",
    "    Args:\n",
    "        term_glossary (dict):\n",
    "            A structured glossary where keys are canonical terms and values contain\n",
    "            a 'synonyms' list.\n",
    "        model (SentenceTransformer):\n",
    "            A loaded SentenceTransformer model.\n",
    "        use_cosine (bool):\n",
    "            Whether to use cosine similarity (recommended for sentence embeddings).\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            (faiss_index, indexed_terms)\n",
    "            - faiss_index: FAISS index containing all embeddings\n",
    "            - indexed_terms: list of terms aligned with index rows\n",
    "    \"\"\"\n",
    "    terms_to_index: List[str] = []\n",
    "\n",
    "    # Collect canonical terms and synonyms\n",
    "    for canonical_term, info in term_glossary.items():\n",
    "        terms_to_index.append(canonical_term)\n",
    "        synonyms = info.get(\"synonyms\", [])\n",
    "        if isinstance(synonyms, list):\n",
    "            terms_to_index.extend(synonyms)\n",
    "\n",
    "    # Deduplicate while keeping deterministic order\n",
    "    indexed_terms = sorted(set(terms_to_index))\n",
    "    if not indexed_terms:\n",
    "        raise ValueError(\"No terms found in glossary.\")\n",
    "\n",
    "    print(\"Generating term embeddings...\")\n",
    "    embeddings = model.encode(\n",
    "        indexed_terms,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=use_cosine,\n",
    "        show_progress_bar=True\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    dim = embeddings.shape[1]\n",
    "\n",
    "    # Choose FAISS index type\n",
    "    if use_cosine:\n",
    "        # Cosine similarity = inner product on normalized vectors\n",
    "        index = faiss.IndexFlatIP(dim)\n",
    "    else:\n",
    "        index = faiss.IndexFlatL2(dim)\n",
    "\n",
    "    index.add(embeddings)\n",
    "\n",
    "    metric = \"cosine similarity\" if use_cosine else \"L2 distance\"\n",
    "    print(f\"FAISS index built successfully. \"\n",
    "          f\"Vectors: {index.ntotal}, Dimension: {dim}, Metric: {metric}\")\n",
    "\n",
    "    return index, indexed_terms\n",
    "\n",
    "\n",
    "def search_terms(\n",
    "    query: str,\n",
    "    model: SentenceTransformer,\n",
    "    index: faiss.Index,\n",
    "    indexed_terms: List[str],\n",
    "    top_k: int = 5,\n",
    "    use_cosine: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Search the FAISS index for the most similar terms to a query.\n",
    "\n",
    "    Args:\n",
    "        query (str): Input query text.\n",
    "        model (SentenceTransformer): SentenceTransformer model.\n",
    "        index (faiss.Index): FAISS index.\n",
    "        indexed_terms (list): Terms aligned with index rows.\n",
    "        top_k (int): Number of results to return.\n",
    "        use_cosine (bool): Whether cosine similarity is used.\n",
    "\n",
    "    Returns:\n",
    "        list of (term, score) tuples.\n",
    "    \"\"\"\n",
    "    query_embedding = model.encode(\n",
    "        [query],\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=use_cosine\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    scores, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        if idx == -1:\n",
    "            continue\n",
    "        results.append((indexed_terms[idx], float(score)))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1. Load embedding model\n",
    "# ------------------------------------------------------------------\n",
    "model = SentenceTransformer(\n",
    "    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2. Prepare term glossary (ALL IN ENGLISH)\n",
    "# ------------------------------------------------------------------\n",
    "term_glossary_example = {\n",
    "    \"Convolutional Neural Network\": {\n",
    "        \"synonyms\": [\"CNN\", \"ConvNet\"]\n",
    "    },\n",
    "    \"Transformer\": {\n",
    "        \"synonyms\": [\"transformer\", \"TRANSFORMER\"]\n",
    "    },\n",
    "    \"Image Recognition\": {\n",
    "        \"synonyms\": [\"Image Classification\", \"Visual Recognition\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3. Build FAISS index\n",
    "# ------------------------------------------------------------------\n",
    "faiss_index, indexed_term_list = build_term_vector_index(\n",
    "    term_glossary_example,\n",
    "    model,\n",
    "    use_cosine=True\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4. Verify results\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n--- Index Build Successful ---\")\n",
    "print(\"Number of vectors in FAISS index:\", faiss_index.ntotal)\n",
    "print(\"Indexed terms:\", indexed_term_list)\n",
    "\n",
    "print(\"\\n--- Query Tests ---\")\n",
    "print(search_terms(\"CNN\", model, faiss_index, indexed_term_list))\n",
    "print(search_terms(\"image classification\", model, faiss_index, indexed_term_list))\n",
    "print(search_terms(\"transformer model\", model, faiss_index, indexed_term_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何调用该函数并获取索引和术语列表的示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Retrieval ---\n",
      "Query: 'CNN'\n",
      "Results:\n",
      "  Top 1: term='CNN', distance=6.2372 (smaller = more similar)\n",
      "  Top 2: term='ConvNet', distance=2.9249 (smaller = more similar)\n",
      "  Top 3: term='Convolutional Neural Network', distance=2.7508 (smaller = more similar)\n",
      "\n",
      "--- Running Retrieval ---\n",
      "Query: 'Computer Vision'\n",
      "Results:\n",
      "  Top 1: term='Visual Recognition', distance=3.8383 (smaller = more similar)\n",
      "  Top 2: term='Image Recognition', distance=3.2715 (smaller = more similar)\n",
      "  Top 3: term='Image Classification', distance=2.6665 (smaller = more similar)\n",
      "\n",
      "--- Running Retrieval ---\n",
      "Query: 'Language Model'\n",
      "Results:\n",
      "  Top 1: term='TRANSFORMER', distance=1.6813 (smaller = more similar)\n",
      "  Top 2: term='Convolutional Neural Network', distance=1.3615 (smaller = more similar)\n",
      "  Top 3: term='CNN', distance=1.2736 (smaller = more similar)\n",
      "\n",
      "--- Running Retrieval ---\n",
      "Query: 'Transformer model'\n",
      "Results:\n",
      "  Top 1: term='Transformer', distance=5.3101 (smaller = more similar)\n",
      "  Top 2: term='transformer', distance=4.7924 (smaller = more similar)\n",
      "  Top 3: term='TRANSFORMER', distance=3.7632 (smaller = more similar)\n"
     ]
    }
   ],
   "source": [
    "# --- Part 2: Define our core retrieval function ---\n",
    "\n",
    "def search_similar_terms(\n",
    "    query_text: str,\n",
    "    model: SentenceTransformer,\n",
    "    index: faiss.Index,\n",
    "    term_list: list,\n",
    "    k: int = 5\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrieve the top-k most similar terms to a query text from a FAISS index.\n",
    "\n",
    "    Args:\n",
    "        query_text (str): The user input query term/text.\n",
    "        model (SentenceTransformer): The embedding model used to encode the query.\n",
    "        index (faiss.Index): The FAISS index object.\n",
    "        term_list (list): The term list aligned with the order of vectors in the FAISS index.\n",
    "        k (int): The number of most similar results to return.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running Retrieval ---\")\n",
    "    print(f\"Query: '{query_text}'\")\n",
    "\n",
    "    # 1) Encode the query text into an embedding vector\n",
    "    query_vector = model.encode([query_text])\n",
    "    query_vector = query_vector.astype(\"float32\")\n",
    "\n",
    "    # 2) Search in the FAISS index\n",
    "    # index.search returns two arrays: D (distances/scores) and I (indices)\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "\n",
    "    # 3) Parse and print results\n",
    "    print(\"Results:\")\n",
    "    for i in range(k):\n",
    "        idx = int(indices[0][i])\n",
    "        dist = float(distances[0][i])\n",
    "        term = term_list[idx]\n",
    "\n",
    "        # For IndexFlatL2, distance is squared Euclidean distance:\n",
    "        # smaller distance => more similar\n",
    "        print(f\"  Top {i+1}: term='{term}', distance={dist:.4f} (smaller = more similar)\")\n",
    "\n",
    "\n",
    "# 4) === Demo ===\n",
    "\n",
    "# Case 1: Query using a synonym for the canonical term\n",
    "# Goal: Test whether the system understands that \"CNN\" refers to \"Convolutional Neural Network\".\n",
    "search_similar_terms(query_text=\"CNN\", model=model, index=faiss_index, term_list=indexed_term_list, k=3)\n",
    "\n",
    "# Case 2: Semantically similar query (core advantage)\n",
    "# Goal: Query a term not explicitly in the glossary but semantically related: \"Computer Vision\".\n",
    "# Expected: The system should find related terms like \"Image Recognition\" or \"Visual Recognition\".\n",
    "search_similar_terms(query_text=\"Computer Vision\", model=model, index=faiss_index, term_list=indexed_term_list, k=3)\n",
    "\n",
    "# Case 3: Query with a broader term\n",
    "# Goal: Query \"Language Model\" and see whether it matches more specific related terms (if present).\n",
    "search_similar_terms(query_text=\"Language Model\", model=model, index=faiss_index, term_list=indexed_term_list, k=3)\n",
    "\n",
    "# Case 4: Tolerance to minor noise / paraphrases\n",
    "# Goal: Query \"Transformer model\" (a paraphrase) and see if it matches \"Transformer\" / \"transformer\".\n",
    "search_similar_terms(query_text=\"Transformer model\", model=model, index=faiss_index, term_list=indexed_term_list, k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、检索增强阶段\n",
    "\n",
    "核心目标是在初步召回（Recall）的基础上，进一步优化检索结果的广度与精度。预处理阶段解决了术语的“标准”问题，而本阶段则聚焦于如何利用这些标准化的知识，在实际检索中发挥最大效用。\n",
    "\n",
    "| 技术名称                             | 描述                                                                                   | 对术语一致性的帮助                                                                 |\n",
    "|--------------------------------------|----------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|\n",
    "| 查询扩展与重写（MultiQueryRetriever） | 利用 LLM 生成多个语义等价的查询变体，合并检索结果。                                     | 自动覆盖用户未提及的同义词或相关表达，极大提升对多样化术语的识别与召回能力。         |\n",
    "| HyDE（假设文档嵌入）                 | 利用 LLM 为查询生成一个“理想答案”的假设性文档，再用该文档的嵌入进行检索。               | 通过生成富含上下文的理想答案，有效缓解原始查询中术语模糊或信息不足的问题，提升检索相关性。 |\n",
    "| 混合检索（BM25 + FAISS）             | 结合关键词检索（如 BM25）与向量检索（如 FAISS）的优势。                                 | 综合利用字面精确匹配和语义相似匹配，确保基础术语不丢失，同时发现语义相关内容。         |\n",
    "| 交叉编码器重排序（BGE-reranker）     | 使用更复杂的交叉编码器模型（如 BGE-reranker）对召回结果进行精细化重排序。               | 通过深度交互分析查询与文档的匹配度，提升对术语匹配度的排序精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain_community\n",
    "# ! pip install langchain langchain-openai faiss-cpu sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 核心技术一：查询扩展与重写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query: What is a CNN?\n",
      "\n",
      "--- Query Variants Generated by MultiQueryRetriever ---\n",
      "Query variant 1: What is the definition of a Convolutional Neural Network?\n",
      "Query variant 2: What role does the CNN model play in deep learning?\n",
      "Query variant 3: Can you explain Convolutional Neural Networks (CNNs)?\n",
      "\n",
      "--- Final Retrieved Document Content ---\n",
      "Convolutional Neural Networks (CNNs) are a key model in deep learning,\n",
      "Unlike CNNs, Transformer models were originally applied to\n",
      "through convolutional layers and pooling layers.\n",
      "Today, they have also been successfully applied to computer vision,\n",
      "known as Vision Transformers.\n",
      "especially effective in the field of image recognition.\n",
      "Their core idea is to automatically extract local image features\n"
     ]
    }
   ],
   "source": [
    "## Query Expansion and Rewriting\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import os\n",
    "\n",
    "# --- Preparation: Set API Key and Create Vector Database ---\n",
    "\n",
    "import dotenv\n",
    "# Set your OpenAI API Key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# 1. Prepare sample documents\n",
    "# We create some example text containing technical terminology\n",
    "doc_text = \"\"\"\n",
    "Convolutional Neural Networks (CNNs) are a key model in deep learning,\n",
    "especially effective in the field of image recognition.\n",
    "Their core idea is to automatically extract local image features\n",
    "through convolutional layers and pooling layers.\n",
    "\n",
    "Unlike CNNs, Transformer models were originally applied to\n",
    "natural language processing (NLP) tasks such as machine translation.\n",
    "Today, they have also been successfully applied to computer vision,\n",
    "known as Vision Transformers.\n",
    "\n",
    "Large Language Models (LLMs) are a major focus of current AI research.\n",
    "Based on the Transformer architecture, they are capable of\n",
    "understanding and generating human-like text,\n",
    "demonstrating strong reasoning capabilities.\n",
    "\"\"\"\n",
    "\n",
    "with open(\"sample_tech_doc.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(doc_text)\n",
    "\n",
    "# 2. Load and split the document\n",
    "loader = TextLoader(\"sample_tech_doc.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# 3. Create the vector database\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# --- MultiQueryRetriever Implementation ---\n",
    "\n",
    "# 4. Initialize the LLM and retriever\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# 5. Execute a query\n",
    "query = \"What is a CNN?\"\n",
    "retrieved_docs = retriever_from_llm.invoke(query)\n",
    "\n",
    "# --- Result Analysis ---\n",
    "print(f\"Original query: {query}\")\n",
    "\n",
    "print(\"\\n--- Query Variants Generated by MultiQueryRetriever ---\")\n",
    "# MultiQueryRetriever internally logs the generated queries.\n",
    "# In practice, you can inspect them by enabling:\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "generated_queries = [\n",
    "    \"What is the definition of a Convolutional Neural Network?\",\n",
    "    \"What role does the CNN model play in deep learning?\",\n",
    "    \"Can you explain Convolutional Neural Networks (CNNs)?\"\n",
    "]\n",
    "\n",
    "for i, q in enumerate(generated_queries):\n",
    "    print(f\"Query variant {i+1}: {q}\")\n",
    "\n",
    "print(\"\\n--- Final Retrieved Document Content ---\")\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 假设性文档嵌入 (HyDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade langchain langchain-community langchain-openai rank_bm25 faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document has been split into 9 chunks.\n",
      "\n",
      "Building FAISS vector retriever...\n",
      "FAISS retriever built successfully.\n",
      "\n",
      "Building BM25 keyword retriever...\n",
      "BM25 retriever built successfully.\n",
      "\n",
      "Initializing MergerRetriever...\n",
      "MergerRetriever initialized successfully.\n",
      "\n",
      "\n",
      "--- Running Hybrid Retrieval ---\n",
      "Query: 'Technical details of ViT'\n",
      "\n",
      "--- Individual Retriever Results ---\n",
      "[BM25 Keyword Retrieval Results] (total 3):\n",
      "  - Part 3: About Large Models.\n",
      "Large Language Models ...\n",
      "  - Today, a variant called Vision Transformer (ViT) h...\n",
      "  - demonstrating strong reasoning capabilities....\n",
      "\n",
      "[FAISS Vector Retrieval Results] (total 3):\n",
      "  - Today, a variant called Vision Transformer (ViT) h...\n",
      "  - They are typically based on the Transformer archit...\n",
      "  - especially effective in image recognition.\n",
      "Their c...\n",
      "\n",
      "--- MergerRetriever Hybrid Results ---\n",
      "[Final Hybrid Results] (total 6, deduplicated):\n",
      "  - Part 3: About Large Models.\n",
      "Large Language Models ...\n",
      "  - Today, a variant called Vision Transformer (ViT) h...\n",
      "  - Today, a variant called Vision Transformer (ViT) h...\n",
      "  - They are typically based on the Transformer archit...\n",
      "  - demonstrating strong reasoning capabilities....\n",
      "  - especially effective in image recognition.\n",
      "Their c...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.retrievers import MergerRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# --- 1. Preparation: Set API Key and Prepare Data ---\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "# Prepare sample documents (slightly richer content for comparison)\n",
    "doc_text = \"\"\"\n",
    "Part 1: About Convolutional Networks.\n",
    "Convolutional Neural Networks (CNNs) are a key model in deep learning,\n",
    "especially effective in image recognition.\n",
    "Their core idea is to automatically extract local image features\n",
    "through convolutional layers and pooling layers. This makes CNNs very efficient.\n",
    "\n",
    "Part 2: About Transformers.\n",
    "Unlike CNNs, Transformer models were originally used in\n",
    "Natural Language Processing (NLP) tasks such as machine translation.\n",
    "Today, a variant called Vision Transformer (ViT) has also been successfully applied\n",
    "to the field of computer vision.\n",
    "\n",
    "Part 3: About Large Models.\n",
    "Large Language Models (LLMs) are a major focus of modern AI research.\n",
    "They are typically based on the Transformer architecture,\n",
    "and can understand and generate human-like text,\n",
    "demonstrating strong reasoning capabilities.\n",
    "\"\"\"\n",
    "\n",
    "with open(\"hybrid_search_doc.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(doc_text)\n",
    "\n",
    "# Load and split documents\n",
    "loader = TextLoader(\"hybrid_search_doc.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=120, chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"The document has been split into {len(docs)} chunks.\")\n",
    "\n",
    "\n",
    "# --- 2. Build two different retrievers ---\n",
    "\n",
    "# Retriever 1: FAISS vector retriever (for semantic matching)\n",
    "print(\"\\nBuilding FAISS vector retriever...\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "faiss_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "print(\"FAISS retriever built successfully.\")\n",
    "\n",
    "\n",
    "# Retriever 2: BM25 keyword retriever (for exact matching)\n",
    "print(\"\\nBuilding BM25 keyword retriever...\")\n",
    "# BM25Retriever can be initialized directly from documents; it does not require embeddings\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "bm25_retriever.k = 3\n",
    "print(\"BM25 retriever built successfully.\")\n",
    "\n",
    "\n",
    "# --- 3. Merge using MergerRetriever ---\n",
    "\n",
    "print(\"\\nInitializing MergerRetriever...\")\n",
    "retriever_list = [bm25_retriever, faiss_retriever]\n",
    "\n",
    "# MergerRetriever handles parallel retrieval and deduplication\n",
    "merged_retriever = MergerRetriever(retrievers=retriever_list)\n",
    "print(\"MergerRetriever initialized successfully.\")\n",
    "\n",
    "\n",
    "# --- 4. Run a query and compare results ---\n",
    "\n",
    "query = \"Technical details of ViT\"\n",
    "print(f\"\\n\\n--- Running Hybrid Retrieval ---\")\n",
    "print(f\"Query: '{query}'\")\n",
    "\n",
    "\n",
    "# For comparison, inspect each retriever’s results individually first\n",
    "print(\"\\n--- Individual Retriever Results ---\")\n",
    "\n",
    "bm25_results = bm25_retriever.invoke(query)\n",
    "print(f\"[BM25 Keyword Retrieval Results] (total {len(bm25_results)}):\")\n",
    "for doc in bm25_results:\n",
    "    print(f\"  - {doc.page_content[:50]}...\")\n",
    "\n",
    "faiss_results = faiss_retriever.invoke(query)\n",
    "print(f\"\\n[FAISS Vector Retrieval Results] (total {len(faiss_results)}):\")\n",
    "for doc in faiss_results:\n",
    "    print(f\"  - {doc.page_content[:50]}...\")\n",
    "\n",
    "\n",
    "# Now inspect the merged (hybrid) results\n",
    "print(\"\\n--- MergerRetriever Hybrid Results ---\")\n",
    "merged_results = merged_retriever.invoke(query)\n",
    "print(f\"[Final Hybrid Results] (total {len(merged_results)}, deduplicated):\")\n",
    "for doc in merged_results:\n",
    "    print(f\"  - {doc.page_content[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、生成控制与输出验证阶段\n",
    "\n",
    "| 技术名称                  | 描述                                                                 | 对术语一致性的贡献与作用                                                                 |\n",
    "|---------------------------|----------------------------------------------------------------------|------------------------------------------------------------------------------------------|\n",
    "| 提示工程 (Prompt Engineering)       | 在提示中明确指令，引导 LLM 使用标准术语、保持特定风格。                | 最基础的控制手段，直接影响 LLM 的选词倾向，引导其遵循术语规范。                           |\n",
    "| 结构化输出 (Structured Output)     | 强制 LLM 返回符合预定义模式（如 Pydantic 或 JSON Schema）的对象。      | 从根本上杜绝术语的随意使用，确保关键信息以标准、可控的格式输出。                         |\n",
    "| 输出解析与修复 (Output Parsers)    | 使用如 OutputFixingParser 等工具，在 LLM 输出格式错误时自动尝试修复。  | 提升结构化输出的鲁棒性，能自动纠正轻微的术语格式或拼写错误。                               |\n",
    "| 后处理与内容增强                   | 在答案文本中自动高亮术语、添加定义弹窗或引用链接。                    | 提升最终答案的可读性和专业性，为用户提供即时的术语解释和来源追溯。                        |\n",
    "| LLM 即评委 (LLM-as-a-Judge)        | 使用另一个 LLM 实例，根据预设标准（如术语一致性）对生成结果进行打分评估。 | 提供一种可扩展的、自动化的输出质量与术语合规性评估方案。                                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 生成时控制：结构化输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Executing structured output chain ---\n",
      "\n",
      "--- Structured object returned by the LLM ---\n",
      "answer='A Convolutional Neural Network (CNN) is a type of deep learning model specifically designed to process data with a grid-like topology, such as images. CNNs are particularly effective for image recognition and classification tasks due to their ability to automatically and adaptively learn spatial hierarchies of features from input data. The architecture of a CNN typically includes layers such as convolutional layers, pooling layers, and fully connected layers. \\n\\n1. **Convolutional Layers**: These layers apply a convolution operation to the input, passing the result to the next layer. This operation helps in detecting features such as edges, textures, and patterns in the image.\\n\\n2. **Pooling Layers**: These layers reduce the spatial size of the representation, which decreases the number of parameters and computation in the network, and also helps control overfitting.\\n\\n3. **Fully Connected Layers**: These layers are used at the end of the network to make predictions based on the features extracted by the convolutional and pooling layers.\\n\\nMain application areas of CNNs include:\\n\\n- **Image and Video Recognition**: CNNs are widely used in applications such as facial recognition, object detection, and video analysis.\\n- **Medical Image Analysis**: They are used to analyze medical images for tasks like tumor detection and organ segmentation.\\n- **Natural Language Processing (NLP)**: Although less common, CNNs can be applied to text data for tasks like sentence classification and sentiment analysis.\\n- **Autonomous Vehicles**: CNNs are used in self-driving cars for tasks such as pedestrian detection and traffic sign recognition.\\n- **Robotics**: In robotics, CNNs help in visual perception tasks, enabling robots to understand and interact with their environment.' standard_terms_used=['Convolutional Neural Network (CNN)', 'convolutional layers', 'pooling layers', 'fully connected layers', 'image recognition', 'object detection', 'video analysis', 'medical image analysis', 'natural language processing (NLP)', 'autonomous vehicles', 'robotics']\n",
      "\n",
      "--- Result analysis ---\n",
      "Answer content: A Convolutional Neural Network (CNN) is a type of deep learning model specifically designed to process data with a grid-like topology, such as images. CNNs are particularly effective for image recognition and classification tasks due to their ability to automatically and adaptively learn spatial hierarchies of features from input data. The architecture of a CNN typically includes layers such as convolutional layers, pooling layers, and fully connected layers. \n",
      "\n",
      "1. **Convolutional Layers**: These layers apply a convolution operation to the input, passing the result to the next layer. This operation helps in detecting features such as edges, textures, and patterns in the image.\n",
      "\n",
      "2. **Pooling Layers**: These layers reduce the spatial size of the representation, which decreases the number of parameters and computation in the network, and also helps control overfitting.\n",
      "\n",
      "3. **Fully Connected Layers**: These layers are used at the end of the network to make predictions based on the features extracted by the convolutional and pooling layers.\n",
      "\n",
      "Main application areas of CNNs include:\n",
      "\n",
      "- **Image and Video Recognition**: CNNs are widely used in applications such as facial recognition, object detection, and video analysis.\n",
      "- **Medical Image Analysis**: They are used to analyze medical images for tasks like tumor detection and organ segmentation.\n",
      "- **Natural Language Processing (NLP)**: Although less common, CNNs can be applied to text data for tasks like sentence classification and sentiment analysis.\n",
      "- **Autonomous Vehicles**: CNNs are used in self-driving cars for tasks such as pedestrian detection and traffic sign recognition.\n",
      "- **Robotics**: In robotics, CNNs help in visual perception tasks, enabling robots to understand and interact with their environment.\n",
      "Standard terms confirmed by the model: ['Convolutional Neural Network (CNN)', 'convolutional layers', 'pooling layers', 'fully connected layers', 'image recognition', 'object detection', 'video analysis', 'medical image analysis', 'natural language processing (NLP)', 'autonomous vehicles', 'robotics']\n",
      "\n",
      "==================================================\n",
      "--- Starting content enhancement ---\n",
      "\n",
      "--- Final enhanced output (view in an HTML-enabled Markdown renderer) ---\n",
      "A <abbr title=\"A specialized deep learning model that excels at processing image data.\">Convolutional Neural Network</abbr> (CNN) is a type of deep learning model specifically designed to process data with a grid-like topology, such as images. CNNs are particularly effective for image recognition and classification tasks due to their ability to automatically and adaptively learn spatial hierarchies of features from input data. The architecture of a CNN typically includes layers such as convolutional layers, pooling layers, and fully connected layers. \n",
      "\n",
      "1. **Convolutional Layers**: These layers apply a convolution operation to the input, passing the result to the next layer. This operation helps in detecting features such as edges, textures, and patterns in the image.\n",
      "\n",
      "2. **Pooling Layers**: These layers reduce the spatial size of the representation, which decreases the number of parameters and computation in the network, and also helps control overfitting.\n",
      "\n",
      "3. **Fully Connected Layers**: These layers are used at the end of the network to make predictions based on the features extracted by the convolutional and pooling layers.\n",
      "\n",
      "Main application areas of CNNs include:\n",
      "\n",
      "- **Image and Video Recognition**: CNNs are widely used in applications such as facial recognition, object detection, and video analysis.\n",
      "- **Medical Image Analysis**: They are used to analyze medical images for tasks like tumor detection and organ segmentation.\n",
      "- **Natural Language Processing (NLP)**: Although less common, CNNs can be applied to text data for tasks like sentence classification and sentiment analysis.\n",
      "- **Autonomous Vehicles**: CNNs are used in self-driving cars for tasks such as pedestrian detection and traffic sign recognition.\n",
      "- **Robotics**: In robotics, CNNs help in visual perception tasks, enabling robots to understand and interact with their environment.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "# Fix: Import BaseModel and Field directly from the pydantic library\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# --- 1. Preparation ---\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "\n",
    "# --- 2. Define the expected output structure (using Pydantic v2) ---\n",
    "class TerminologyInAnswer(BaseModel):\n",
    "    \"\"\"A structured model containing the main answer and the technical terms used.\"\"\"\n",
    "    answer: str = Field(description=\"A detailed and accurate answer to the user's question.\")\n",
    "    standard_terms_used: List[str] = Field(\n",
    "        description=\"A list of standard technical terms from the official glossary that are explicitly used in the answer.\",\n",
    "        example=[\"Convolutional Neural Network\", \"Image Recognition\"]\n",
    "    )\n",
    "\n",
    "# --- 3. Create a structured output chain ---\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI expert with deep technical knowledge. Please provide a structured answer to the user's question.\"),\n",
    "    (\"human\", \"Please explain what a CNN is and describe its main application areas.\")\n",
    "])\n",
    "\n",
    "# Bind the LLM to the Pydantic v2 model using structured output\n",
    "# This uses the efficient json_schema mode by default and avoids warnings\n",
    "structured_llm_chain = prompt | llm.with_structured_output(TerminologyInAnswer)\n",
    "\n",
    "# --- 4. Execute the chain and inspect the result ---\n",
    "print(\"--- Executing structured output chain ---\")\n",
    "structured_response = structured_llm_chain.invoke({})\n",
    "\n",
    "print(\"\\n--- Structured object returned by the LLM ---\")\n",
    "print(structured_response)\n",
    "\n",
    "print(\"\\n--- Result analysis ---\")\n",
    "print(f\"Answer content: {structured_response.answer}\")\n",
    "print(f\"Standard terms confirmed by the model: {structured_response.standard_terms_used}\")\n",
    "\n",
    "if \"Convolutional Neural Network\" in structured_response.standard_terms_used:\n",
    "    print(\"Validation passed: The answer correctly uses the standard term 'Convolutional Neural Network'.\")\n",
    "\n",
    "# --- 5. Content enhancement: automatically add term definitions ---\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"--- Starting content enhancement ---\")\n",
    "\n",
    "# Assume we have a simplified terminology glossary\n",
    "glossary = {\n",
    "    \"Convolutional Neural Network\": \"A specialized deep learning model that excels at processing image data.\",\n",
    "    \"Deep Learning\": \"A subfield of machine learning based on artificial neural networks.\",\n",
    "    \"Image Recognition\": \"A core task in computer vision that aims to identify and classify objects in images.\"\n",
    "}\n",
    "\n",
    "# Extract the answer text generated by the LLM\n",
    "llm_answer_text = structured_response.answer\n",
    "\n",
    "def enhance_text_with_definitions(text: str, term_glossary: dict) -> str:\n",
    "    \"\"\"\n",
    "    Find standard technical terms in the text and add Markdown/HTML hover annotations for them.\n",
    "    \"\"\"\n",
    "    enhanced_text = text\n",
    "    for term, definition in term_glossary.items():\n",
    "        # Create annotated Markdown/HTML format\n",
    "        replacement = f'<abbr title=\"{definition}\">{term}</abbr>'\n",
    "        # Replace occurrences of the term in the text\n",
    "        enhanced_text = enhanced_text.replace(term, replacement)\n",
    "    return enhanced_text\n",
    "\n",
    "# Perform content enhancement\n",
    "final_output = enhance_text_with_definitions(llm_answer_text, glossary)\n",
    "\n",
    "print(\"\\n--- Final enhanced output (view in an HTML-enabled Markdown renderer) ---\")\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 生成后处理：验证与内容增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Enhanced output ---\n",
      "<abbr title=\"A language model trained on massive datasets with a very large number of parameters.\">Large Language Model</abbr>s are typically based on the <abbr title=\"A neural network architecture based on self-attention that has achieved major success in NLP.\">Transformer Architecture</abbr>, while <abbr title=\"A specialized deep learning model that excels at processing image data.\">Convolutional Neural Network</abbr>s are the dominant approach for image processing.\n"
     ]
    }
   ],
   "source": [
    "# Assume we have a simplified terminology glossary\n",
    "glossary = {\n",
    "    \"Convolutional Neural Network\": \"A specialized deep learning model that excels at processing image data.\",\n",
    "    \"Transformer Architecture\": \"A neural network architecture based on self-attention that has achieved major success in NLP.\",\n",
    "    \"Large Language Model\": \"A language model trained on massive datasets with a very large number of parameters.\"\n",
    "}\n",
    "\n",
    "# Assume this is the LLM-generated, already-validated answer text\n",
    "llm_answer_text = (\n",
    "    \"Large Language Models are typically based on the Transformer Architecture, \"\n",
    "    \"while Convolutional Neural Networks are the dominant approach for image processing.\"\n",
    ")\n",
    "\n",
    "def enhance_text_with_definitions(text: str, term_glossary: dict) -> str:\n",
    "    \"\"\"\n",
    "    Find standard technical terms in the text and add Markdown/HTML hover annotations.\n",
    "    In an HTML-enabled Markdown renderer, this typically appears as a tooltip on hover.\n",
    "    \"\"\"\n",
    "    enhanced_text = text\n",
    "    for term, definition in term_glossary.items():\n",
    "        # Create annotated Markdown/HTML\n",
    "        replacement = f'<abbr title=\"{definition}\">{term}</abbr>'\n",
    "        # Replace occurrences of the term in the text\n",
    "        enhanced_text = enhanced_text.replace(term, replacement)\n",
    "    return enhanced_text\n",
    "\n",
    "# Perform content enhancement\n",
    "final_output = enhance_text_with_definitions(llm_answer_text, glossary)\n",
    "\n",
    "print(\"\\n--- Enhanced output ---\")\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六、评估与反馈机制\n",
    "\n",
    "人工抽样评估难以覆盖海量的生成内容，而“LLM即评委”为此提供了一个高效、可扩展的自动化解决方案。其核心是利用一个LLM的理解和推理能力，来评估另一个LLM（或整个RAG系统）的输出质量。\n",
    "\n",
    "使用LCEL构建评估链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating [Good Answer] ---\n",
      "{'Accuracy': \"The terms 'Large Language Model (LLM)', 'Transformer Model', and 'Convolutional Neural Network (CNN)' are used correctly according to the glossary.\", 'Compliance': 'The answer complies with the glossary by using the official terms and their aliases without introducing unofficial or ambiguous aliases.', 'Completeness': 'The answer uses the most appropriate standard terms as per the glossary, and all necessary terms are included.'}\n",
      "\n",
      "--- Evaluating [Needs Improvement Answer] ---\n",
      "{'Accuracy': {'status': 'Fail', 'issues': [{'term': 'big model', 'description': \"The term 'big model' is not a standard term. The correct term is 'Large Language Model' or 'LLM' if applicable.\"}, {'term': 'transformer-style architecture', 'description': \"The term 'transformer-style architecture' is not a standard term. The correct term is 'Transformer Model' or 'Transformer'.\"}, {'term': 'conv net', 'description': \"The term 'conv net' is not a standard term. The correct term is 'Convolutional Neural Network' or 'CNN'.\"}]}, 'Compliance': {'status': 'Fail', 'issues': [{'term': 'big model', 'description': \"The term 'big model' is an unofficial alias and should be replaced with 'Large Language Model' or 'LLM' if applicable.\"}, {'term': 'transformer-style architecture', 'description': \"The term 'transformer-style architecture' is an unofficial alias and should be replaced with 'Transformer Model' or 'Transformer'.\"}, {'term': 'conv net', 'description': \"The term 'conv net' is an unofficial alias and should be replaced with 'Convolutional Neural Network' or 'CNN'.\"}]}, 'Completeness': {'status': 'Fail', 'issues': [{'term': 'big model', 'description': \"The term 'big model' is not the most appropriate standard term. Use 'Large Language Model' or 'LLM' if applicable.\"}, {'term': 'transformer-style architecture', 'description': \"The term 'transformer-style architecture' is not the most appropriate standard term. Use 'Transformer Model' or 'Transformer'.\"}, {'term': 'conv net', 'description': \"The term 'conv net' is not the most appropriate standard term. Use 'Convolutional Neural Network' or 'CNN'.\"}]}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# --- 1. Preparation ---\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "evaluator_llm = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
    "\n",
    "# --- 2. Define a structured evaluation model ---\n",
    "class TerminologyEvaluation(BaseModel):\n",
    "    \"\"\"A structured model for evaluating terminology consistency.\"\"\"\n",
    "    consistency_score: int = Field(description=\"A score from 1 to 5, where 5 means fully consistent and 1 means severely inconsistent.\")\n",
    "    is_consistent: bool = Field(description=\"Boolean indicating whether the answer is overall compliant with terminology standards.\")\n",
    "    reasoning: str = Field(description=\"A detailed explanation of the score, highlighting strengths and issues.\")\n",
    "    suggestions_for_improvement: List[str] = Field(description=\"Concrete suggestions to improve terminology usage in the answer.\")\n",
    "\n",
    "# --- 3. Build the evaluation chain ---\n",
    "# Create a dedicated evaluation prompt\n",
    "evaluation_prompt_template = \"\"\"\n",
    "You are a strict technical documentation quality evaluator for AI content. Your core task is to evaluate the consistency and correctness of terminology usage in a given answer.\n",
    "\n",
    "**Evaluation Criteria:**\n",
    "1.  **Accuracy**: Are standard terms used correctly?\n",
    "2.  **Compliance**: Does the answer avoid unofficial or ambiguous aliases?\n",
    "3.  **Completeness**: Does the answer use the most appropriate standard terms when needed?\n",
    "\n",
    "**Authoritative Terminology Glossary (partial):**\n",
    "- Convolutional Neural Network (alias: CNN)\n",
    "- Transformer Model (alias: Transformer)\n",
    "- Large Language Model (alias: LLM)\n",
    "\n",
    "**Answer to Evaluate:**\n",
    "{answer_text}\n",
    "\n",
    "Based on the criteria and glossary above, output your evaluation result in JSON format.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(evaluation_prompt_template)\n",
    "parser = JsonOutputParser(pydantic_object=TerminologyEvaluation)\n",
    "\n",
    "# Build the evaluation chain using LCEL\n",
    "evaluation_chain = prompt | evaluator_llm | parser\n",
    "\n",
    "# --- 4. Run evaluations ---\n",
    "\n",
    "# Case 1: A terminology-compliant answer\n",
    "good_answer = \"A Large Language Model (LLM) is built on a Transformer Model, while a Convolutional Neural Network (CNN) is widely used in image-related domains.\"\n",
    "\n",
    "# Case 2: An answer using non-standard terminology\n",
    "bad_answer = \"A big model is based on a transformer-style architecture, and a conv net is very strong at picture processing.\"\n",
    "\n",
    "print(\"--- Evaluating [Good Answer] ---\")\n",
    "good_evaluation_result = evaluation_chain.invoke({\"answer_text\": good_answer})\n",
    "print(good_evaluation_result)\n",
    "\n",
    "print(\"\\n--- Evaluating [Needs Improvement Answer] ---\")\n",
    "bad_evaluation_result = evaluation_chain.invoke({\"answer_text\": bad_answer})\n",
    "print(bad_evaluation_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结：术语一致性优化路线图\n",
    "\n",
    "经过以上各阶段的详细探讨，从数据预处理到最终的评估反馈，我们已经全面构建了保障术语一致性的技术体系。\n",
    "\n",
    "为了更直观地理解各项技术的定位与优先级，我们将整个优化策略总结为以下分级路线图，为不同阶段的RAG系统建设提供实践指引。\n",
    "\n",
    "| Optimization Layer | Core Techniques & Solutions |\n",
    "|--------------------|-----------------------------|\n",
    "| Foundation | Terminology glossary construction, term extraction, preprocessing standardization, term embeddings and vector indexing |\n",
    "| Key Enhancement | Hybrid retrieval (BM25 + vectors), query expansion (MultiQuery), hypothetical document embeddings (HyDE), cross-encoder re-ranking |\n",
    "| Auxiliary Optimization | Domain-specific embedding fine-tuning, context-aware chunking |\n",
    "| Generation Control | Prompt engineering, structured outputs, output parsing and repair |\n",
    "| Long-term Assurance | LLM-as-a-Judge, user feedback loops, logging, auditing, and analytics |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llmops)",
   "language": "python",
   "name": "llmops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
