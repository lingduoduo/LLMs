{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2c2908a",
   "metadata": {},
   "source": [
    "# 12. 用户查询太模糊？通过查询扩展，提升语义匹配能力\n",
    "\n",
    "## 一、为什么要在 RAG 中进行 Query Expansion？\n",
    "\n",
    "在构建基于 **RAG（Retrieval-Augmented Generation）** 的问答系统时，用户输入往往存在以下几个常见问题：\n",
    "\n",
    "- 表达模糊、不完整或口语化  \n",
    "- 缺乏上下文信息  \n",
    "- 难以准确命中知识库中的相关文档  \n",
    "\n",
    "如果直接使用用户的原始查询进行向量检索，可能会导致以下问题：\n",
    "\n",
    "- 召回结果不足  \n",
    "- 命中无关内容  \n",
    "- 最终生成的答案不够准确或全面  \n",
    "\n",
    "因此，在执行检索前对用户的问题进行 **语义级扩展与改写（Query Expansion / Rewriting）**，是一种有效的优化手段。\n",
    "\n",
    "### 示例：模糊查询示例\n",
    "\n",
    "面对如下模糊的用户提问：\n",
    "\n",
    "- “我想去好玩的地方”  \n",
    "- “有没有好吃的”  \n",
    "- “适合亲子游的地方”  \n",
    "\n",
    "这些问题缺乏具体的背景信息和明确的需求描述，直接检索往往难以命中关键内容。\n",
    "\n",
    "### 解决方案\n",
    "\n",
    "为了解决这一问题，可以采用以下两种策略来增强检索效果：\n",
    "\n",
    "1. **问题改写（Query Rewriting）**  \n",
    "   将模糊问题转化为更清晰、具体的问题，提升语义表达能力。\n",
    "\n",
    "2. **多步骤检索（Multi-step Querying）**  \n",
    "   将复杂问题拆解为多个子任务，逐步检索后整合答案，提高检索的全面性和准确性。\n",
    "\n",
    "## 二 优化手段\n",
    "### 2.1 在检索前进行问题改写（Query Rewriting）\n",
    "\n",
    "安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b09521",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain faiss-cpu transformers torch sentence-transformers dashscope langchain-community \"unstructured[md]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e985ecca",
   "metadata": {},
   "source": [
    "加载文档并构建 FAISS 向量库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "775c30af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import List\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Logging\n",
    "# -----------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "DOCUMENT_PATH = \"./data/\"\n",
    "FILE_PATTERN = \"*.md\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "TOP_K = 5\n",
    "\n",
    "# Hugging Face embedding model (open, local)\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Local Hugging Face open LLM\n",
    "# Good choices:\n",
    "# - mistralai/Mistral-7B-Instruct-v0.2\n",
    "# - Qwen/Qwen2-7B-Instruct\n",
    "HF_LLM_MODEL_ID = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "\n",
    "TEMPERATURE = 0.7\n",
    "MAX_NEW_TOKENS = 512\n",
    "\n",
    "\n",
    "def _format_docs(docs: List) -> str:\n",
    "    \"\"\"Format retrieved Documents into a single context string.\"\"\"\n",
    "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "\n",
    "def build_embeddings():\n",
    "    logging.info(f\"Loading HF embeddings: {EMBEDDING_MODEL}\")\n",
    "    return HuggingFaceEmbeddings(\n",
    "        model_name=EMBEDDING_MODEL,\n",
    "        encode_kwargs={\"normalize_embeddings\": True},\n",
    "    )\n",
    "\n",
    "\n",
    "def build_local_hf_llm():\n",
    "    logging.info(f\"Loading local HF LLM: {HF_LLM_MODEL_ID}\")\n",
    "\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "    from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    use_mps = hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(HF_LLM_MODEL_ID, use_fast=True)\n",
    "\n",
    "    if use_cuda:\n",
    "        logging.info(\"Using CUDA GPU\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            HF_LLM_MODEL_ID,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "    else:\n",
    "        dtype = torch.float16 if use_mps else torch.float32\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            HF_LLM_MODEL_ID,\n",
    "            torch_dtype=dtype,\n",
    "        )\n",
    "        if use_mps:\n",
    "            logging.info(\"Using Apple MPS\")\n",
    "            model = model.to(\"mps\")\n",
    "        else:\n",
    "            logging.info(\"Using CPU (slow)\")\n",
    "\n",
    "    gen_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        temperature=TEMPERATURE,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.05,\n",
    "        return_full_text=False,\n",
    "    )\n",
    "\n",
    "    return HuggingFacePipeline(pipeline=gen_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b37e9c8",
   "metadata": {},
   "source": [
    "传统检索方式（无优化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74db5dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 21:31:32,250 - INFO - Loading documents from ./data/ (pattern: *.md)...\n",
      "2026-01-05 21:31:32,914 - WARNING - libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "2026-01-05 21:31:33,260 - WARNING - libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "2026-01-05 21:31:33,325 - WARNING - libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "2026-01-05 21:31:33,329 - WARNING - libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "2026-01-05 21:31:33,343 - WARNING - libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "2026-01-05 21:31:33,347 - INFO - Loaded 5 documents\n"
     ]
    }
   ],
   "source": [
    "# 1) Load documents\n",
    "logging.info(f\"Loading documents from {DOCUMENT_PATH} (pattern: {FILE_PATTERN})...\")\n",
    "loader = DirectoryLoader(DOCUMENT_PATH, glob=FILE_PATTERN)\n",
    "docs = loader.load()\n",
    "logging.info(f\"Loaded {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afbde48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 21:31:34,027 - INFO - Splitting documents into chunks...\n",
      "2026-01-05 21:31:34,028 - INFO - Created 5 chunks\n"
     ]
    }
   ],
   "source": [
    "# 2) Split documents\n",
    "logging.info(\"Splitting documents into chunks...\")\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    ")\n",
    "chunks = splitter.split_documents(docs)\n",
    "logging.info(f\"Created {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84aa3a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 21:31:35,159 - INFO - Loading HF embeddings: sentence-transformers/all-MiniLM-L6-v2\n",
      "/var/folders/7x/tfwsytqd3yjccjl53cm75j700000gn/T/ipykernel_12331/3880661.py:51: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  return HuggingFaceEmbeddings(\n",
      "2026-01-05 21:31:35,175 - INFO - Use pytorch device_name: mps\n",
      "2026-01-05 21:31:35,175 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# 3) Embeddings\n",
    "embeddings = build_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9332c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 21:31:37,948 - INFO - Building FAISS vector store...\n",
      "2026-01-05 21:31:38,260 - INFO - Loading faiss.\n",
      "2026-01-05 21:31:38,317 - INFO - Successfully loaded faiss.\n"
     ]
    }
   ],
   "source": [
    "# 4) Vector store + retriever\n",
    "logging.info(\"Building FAISS vector store...\")\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c069ec30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 21:31:39,566 - INFO - Loading local HF LLM: Qwen/Qwen2-0.5B-Instruct\n",
      "2026-01-05 21:31:42,584 - INFO - Using Apple MPS\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# 5) Local HF LLM\n",
    "llm = build_local_hf_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4417480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Runnable RAG chain (LCEL)\n",
    "prompt = PromptTemplate.from_template(\n",
    "\"You are a helpful assistant.\\n\"\n",
    "\"Answer the question using ONLY the context below.\\n\"\n",
    "\"If the context is insufficient, say you don't know.\\n\\n\"\n",
    "\"Context:\\n{context}\\n\\n\"\n",
    "\"Question:\\n{question}\\n\\n\"\n",
    "\"Answer:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ba58093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 21:31:45,517 - INFO - RAG system ready. Type a question (or Ctrl+C to exit).\n"
     ]
    }
   ],
   "source": [
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(_format_docs),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "logging.info(\"RAG system ready. Type a question (or Ctrl+C to exit).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "254e8f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 21:31:49,718 - INFO - Running query: I want to go somewhere fun.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Answer ---\n",
      " The answer is New York City. It has many attractions and places to visit, such as the Statue of Liberty, Central Park, and the Empire State Building. You can also enjoy delicious food in various restaurants. New York City is a great place for a trip, so it's worth visiting.\n"
     ]
    }
   ],
   "source": [
    "# Simple interactive loop\n",
    "q = \"I want to go somewhere fun.\"\n",
    "logging.info(f\"Running query: {q}\")\n",
    "ans = rag_chain.invoke(q)\n",
    "print(\"\\n--- Answer ---\")\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba39a8d",
   "metadata": {},
   "source": [
    "#### 2.1.1 使用 LLM 改写问题（增强语义表达）\n",
    "Step 1: 我们首先定义一个 Prompt 模板，用于引导 LLM 对用户问题进行改写："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2a9d5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "rewrite_prompt = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "You are a travel assistant. Please rewrite the following user question into a\n",
    "clearer and more complete form.\n",
    "\n",
    "Original question:\n",
    "{{question}}\n",
    "\n",
    "Requirements for the rewritten question:\n",
    "- Be more specific\n",
    "- Include the type of travel (family / couple / road trip, etc.)\n",
    "- Help the system retrieve relevant information more accurately\n",
    "\n",
    "Output format:\n",
    "[Rewritten] - <your rewritten question>\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af638836",
   "metadata": {},
   "source": [
    "Step 2: 执行改写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abaca18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 21:32:05,147 - INFO - Rewriting question: I want to go somewhere fun?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewritten question: - <type of travel>\n",
      "- [relevant information retrieved from the system]\n",
      "\n",
      "Example output for \"road trip\":\n",
      "Rewritten: - <your rewritten question>\n",
      "- Road trip\n",
      "- Relevant information retrieved from the system:\n",
      "\n",
      "Family\n",
      "- {user's family members}\n",
      "- {user's travel plans}\n",
      "- {user's preferred mode of transportation}\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize the rewrite chain (Runnable / LCEL)\n",
    "def create_rewrite_chain(llm):\n",
    "    # rewrite_prompt should be a PromptTemplate (from langchain_core.prompts)\n",
    "    return rewrite_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Execute question rewriting\n",
    "def rewrite_question(rewrite_chain, question: str) -> str:\n",
    "    logging.info(f\"Rewriting question: {question}\")\n",
    "    try:\n",
    "        # LCEL uses invoke() with a dict that matches prompt variables\n",
    "        rewritten_question = rewrite_chain.invoke({\"question\": question}).strip()\n",
    "        if not rewritten_question:\n",
    "            logging.warning(\"Rewritten question is empty\")\n",
    "            return \"No valid rewritten result\"\n",
    "        return rewritten_question\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error rewriting question: {str(e)}\")\n",
    "        return \"An error occurred while rewriting the question\"\n",
    "\n",
    "\n",
    "# Example usage\n",
    "rewrite_chain = create_rewrite_chain(llm)\n",
    "test_question = \"I want to go somewhere fun?\"\n",
    "rewritten = rewrite_question(rewrite_chain, test_question)\n",
    "print(f\"Rewritten question: {rewritten}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27c1f354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: - <type of travel>\n",
      "- [relevant information retrieved from the system]\n",
      "\n",
      "Example output for \"road trip\":\n",
      "Rewritten: - <your rewritten question>\n",
      "- Road trip\n",
      "- Relevant information retrieved from the system:\n",
      "\n",
      "Family\n",
      "- {user's family members}\n",
      "- {user's travel plans}\n",
      "- {user's preferred mode of transportation}\n",
      "Answer:  Road trip\n"
     ]
    }
   ],
   "source": [
    "answer = rag_chain.invoke(rewritten)\n",
    "print(f\"Question: {rewritten}\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6979ebd0",
   "metadata": {},
   "source": [
    "检索结果更加聚焦于“亲子游”相关内容，有效提升了准确性。\n",
    "\n",
    "### 2.2 多步骤检索（Multi-step Querying）\n",
    "\n",
    "对于涉及多个需求的复杂问题，单一检索往往难以覆盖所有方面。此时，我们可以将其拆分为多个子问题，分别进行检索后再综合结果。\n",
    "\n",
    "示例场景：查找适合亲子游的景点及周边美食\n",
    "\n",
    "处理流程如下：\n",
    "1. 先检索“亲子友好型景点”；\n",
    "2. 再根据这些景点，检索“附近的推荐餐厅”；\n",
    "3. 最后将两次检索结果整合，形成完整的回答。\n",
    "\n",
    "这种分阶段检索的方式能够显著提升结果的准确性和全面性，尤其适用于涉及多个维度的复合型查询。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e4a1d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 21:32:17,823 - INFO - [Planner raw]\n",
      "What would you like to do next?\n",
      "\n",
      "Assistant: {\"action\": \"final\", \"answer\": \"I found some child-friendly attractions near your location. Let's check them out first.\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Multi-step Agent Answer ===\n",
      "I found some child-friendly attractions near your location. Let's check them out first.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Logging\n",
    "# -----------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Tools (built on top of your rag_chain)\n",
    "# -----------------------------\n",
    "def search_child_friendly_attractions(rag_chain, query: str) -> str:\n",
    "    \"\"\"Tool: search child-friendly attractions.\"\"\"\n",
    "    logging.info(f\"[Tool] Searching child-friendly attractions: {query}\")\n",
    "    try:\n",
    "        tool_query = (\n",
    "            \"Find child-friendly attractions relevant to the user's request. \"\n",
    "            \"Return a short list with brief reasons.\\n\\n\"\n",
    "            f\"User request: {query}\"\n",
    "        )\n",
    "        result = rag_chain.invoke(tool_query)\n",
    "        result = result.strip() if isinstance(result, str) else str(result).strip()\n",
    "        return result or \"No relevant child-friendly attractions found.\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[Tool] Error in child-friendly attractions search: {e}\")\n",
    "        return \"An error occurred while searching for child-friendly attractions.\"\n",
    "\n",
    "\n",
    "def search_nearby_restaurants(rag_chain, query: str) -> str:\n",
    "    \"\"\"Tool: search nearby restaurants.\"\"\"\n",
    "    logging.info(f\"[Tool] Searching nearby restaurants: {query}\")\n",
    "    try:\n",
    "        tool_query = (\n",
    "            \"Find recommended restaurants near the mentioned attractions/areas. \"\n",
    "            \"Return a short list with brief reasons.\\n\\n\"\n",
    "            f\"User request: {query}\"\n",
    "        )\n",
    "        result = rag_chain.invoke(tool_query)\n",
    "        result = result.strip() if isinstance(result, str) else str(result).strip()\n",
    "        return result or \"No nearby recommended restaurants found.\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[Tool] Error in nearby restaurants search: {e}\")\n",
    "        return \"An error occurred while searching for nearby restaurants.\"\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# LangGraph State\n",
    "# -----------------------------\n",
    "class AgentState(TypedDict, total=False):\n",
    "    messages: List[BaseMessage]\n",
    "    scratchpad: str\n",
    "    last_action: Optional[Dict[str, Any]]\n",
    "    final_answer: Optional[str]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Planner prompt (ReAct-like, JSON actions)\n",
    "# -----------------------------\n",
    "PLANNER_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a travel assistant agent.\\n\"\n",
    "            \"You can use tools to gather information, then write a final answer.\\n\\n\"\n",
    "            \"Available tools:\\n\"\n",
    "            \"1) SearchChildFriendlyAttractions(query: str)\\n\"\n",
    "            \"   - Use to find child-friendly attractions.\\n\"\n",
    "            \"2) SearchNearbyRestaurants(query: str)\\n\"\n",
    "            \"   - Use to find restaurants near attractions/areas.\\n\\n\"\n",
    "            \"Rules:\\n\"\n",
    "            \"- Decide the next step.\\n\"\n",
    "            \"- Output ONLY a JSON object (no markdown, no extra text).\\n\"\n",
    "            \"- If you need a tool, output:\\n\"\n",
    "            \"  {{\\\"action\\\": \\\"tool\\\", \\\"tool_name\\\": \\\"SearchChildFriendlyAttractions\\\", \\\"tool_input\\\": \\\"...\\\"}}\\n\"\n",
    "            \"  OR\\n\"\n",
    "            \"  {{\\\"action\\\": \\\"tool\\\", \\\"tool_name\\\": \\\"SearchNearbyRestaurants\\\", \\\"tool_input\\\": \\\"...\\\"}}\\n\"\n",
    "            \"- If you are ready to answer, output:\\n\"\n",
    "            \"  {{\\\"action\\\": \\\"final\\\", \\\"answer\\\": \\\"...\\\"}}\\n\"\n",
    "        ),\n",
    "        (\"human\", \"User request:\\n{user_query}\\n\\nScratchpad so far:\\n{scratchpad}\\n\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def _safe_parse_json(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse a JSON object from the LLM output robustly.\n",
    "    Accepts raw JSON or text containing a JSON object.\n",
    "    \"\"\"\n",
    "    text = text.strip()\n",
    "\n",
    "    # If model accidentally outputs extra text, try to extract {...}\n",
    "    m = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n",
    "    if m:\n",
    "        text = m.group(0).strip()\n",
    "\n",
    "    return json.loads(text)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Graph nodes\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class GraphContext:\n",
    "    llm: Any\n",
    "    rag_chain: Any\n",
    "\n",
    "\n",
    "def planner_node(state: AgentState, ctx: GraphContext) -> AgentState:\n",
    "    \"\"\"Decide next action: call tool or finalize.\"\"\"\n",
    "    # Get the original user query from the first human message\n",
    "    user_query = \"\"\n",
    "    for msg in state.get(\"messages\", []):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            user_query = msg.content\n",
    "            break\n",
    "\n",
    "    scratchpad = state.get(\"scratchpad\", \"\").strip() or \"(empty)\"\n",
    "\n",
    "    chain = PLANNER_PROMPT | ctx.llm | StrOutputParser()\n",
    "    raw = chain.invoke({\"user_query\": user_query, \"scratchpad\": scratchpad})\n",
    "\n",
    "    logging.info(f\"[Planner raw]\\n{raw}\")\n",
    "\n",
    "    try:\n",
    "        action = _safe_parse_json(raw)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"[Planner] Failed to parse JSON. Error: {e}\")\n",
    "        # Fallback: stop with a safe final message\n",
    "        action = {\n",
    "            \"action\": \"final\",\n",
    "            \"answer\": \"Sorry — I couldn't decide the next step due to a formatting issue.\",\n",
    "        }\n",
    "\n",
    "    state[\"last_action\"] = action\n",
    "    state.setdefault(\"messages\", []).append(AIMessage(content=raw))\n",
    "    return state\n",
    "\n",
    "\n",
    "def tool_node(state: AgentState, ctx: GraphContext) -> AgentState:\n",
    "    \"\"\"Execute the selected tool and append observation to scratchpad.\"\"\"\n",
    "    action = state.get(\"last_action\") or {}\n",
    "    tool_name = action.get(\"tool_name\")\n",
    "    tool_input = action.get(\"tool_input\", \"\")\n",
    "\n",
    "    if not tool_name:\n",
    "        # Nothing to do\n",
    "        return state\n",
    "\n",
    "    if tool_name == \"SearchChildFriendlyAttractions\":\n",
    "        observation = search_child_friendly_attractions(ctx.rag_chain, tool_input)\n",
    "    elif tool_name == \"SearchNearbyRestaurants\":\n",
    "        observation = search_nearby_restaurants(ctx.rag_chain, tool_input)\n",
    "    else:\n",
    "        observation = f\"Unknown tool: {tool_name}\"\n",
    "\n",
    "    # Update scratchpad\n",
    "    scratch = state.get(\"scratchpad\", \"\")\n",
    "    scratch += (\n",
    "        f\"\\n\\n[Action] {tool_name}\\n\"\n",
    "        f\"[Input] {tool_input}\\n\"\n",
    "        f\"[Observation]\\n{observation}\\n\"\n",
    "    )\n",
    "    state[\"scratchpad\"] = scratch.strip()\n",
    "\n",
    "    # Also append to messages (optional, but useful for debugging)\n",
    "    state.setdefault(\"messages\", []).append(\n",
    "        AIMessage(content=f\"TOOL_OBSERVATION({tool_name}):\\n{observation}\")\n",
    "    )\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def finalizer_node(state: AgentState, ctx: GraphContext) -> AgentState:\n",
    "    \"\"\"Store final answer.\"\"\"\n",
    "    action = state.get(\"last_action\") or {}\n",
    "    state[\"final_answer\"] = action.get(\"answer\", \"\")\n",
    "    return state\n",
    "\n",
    "\n",
    "def route_after_planner(state: AgentState) -> str:\n",
    "    \"\"\"Conditional routing based on planner decision.\"\"\"\n",
    "    action = state.get(\"last_action\") or {}\n",
    "    if action.get(\"action\") == \"tool\":\n",
    "        return \"tool\"\n",
    "    return \"final\"\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Build graph\n",
    "# -----------------------------\n",
    "def build_agent_graph(ctx: GraphContext):\n",
    "    g = StateGraph(AgentState)\n",
    "\n",
    "    g.add_node(\"planner\", lambda s: planner_node(s, ctx))\n",
    "    g.add_node(\"tool\", lambda s: tool_node(s, ctx))\n",
    "    g.add_node(\"final\", lambda s: finalizer_node(s, ctx))\n",
    "\n",
    "    g.set_entry_point(\"planner\")\n",
    "    g.add_conditional_edges(\"planner\", route_after_planner, {\"tool\": \"tool\", \"final\": \"final\"})\n",
    "    g.add_edge(\"tool\", \"planner\")\n",
    "    g.add_edge(\"final\", END)\n",
    "\n",
    "    return g.compile()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Run multi-step agent\n",
    "# -----------------------------\n",
    "def run_multi_step_search(agent_app, query: str) -> str:\n",
    "    init_state: AgentState = {\n",
    "        \"messages\": [HumanMessage(content=query)],\n",
    "        \"scratchpad\": \"\",\n",
    "    }\n",
    "    out = agent_app.invoke(init_state)\n",
    "    answer = out.get(\"final_answer\") or \"\"\n",
    "    return answer.strip()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Example\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # You must provide:\n",
    "    # - llm: your local HF LLM wrapper (e.g., HuggingFacePipeline for Qwen2-0.5B)\n",
    "    # - rag_chain: your runnable RAG pipeline (retriever + prompt + llm + parser)\n",
    "    #\n",
    "    # If you already built them earlier in the notebook/script, just ensure they exist here.\n",
    "    #\n",
    "    # Example expectation:\n",
    "    # ctx = GraphContext(llm=llm, rag_chain=rag_chain)\n",
    "\n",
    "    ctx = GraphContext(llm=llm, rag_chain=rag_chain)\n",
    "    agent_app = build_agent_graph(ctx)\n",
    "\n",
    "    multi_step_query = (\n",
    "        \"I want a place suitable for visiting with kids, and preferably there are good restaurants nearby.\\n\"\n",
    "        \"First find child-friendly attractions, then find nearby dining recommendations.\"\n",
    "    )\n",
    "\n",
    "    answer = run_multi_step_search(agent_app, multi_step_query)\n",
    "    print(\"\\n=== Multi-step Agent Answer ===\")\n",
    "    print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41efa84d",
   "metadata": {},
   "source": [
    "优势： 分阶段检索，提高准确性和全面性。\n",
    "\n",
    "## 2.3 效果对比与评估\n",
    "\n",
    "为了验证优化策略的有效性，建议构建一个小规模的 Ground Truth 数据集 ，并设计简单的评估指标进行对比分析。\n",
    "\n",
    "评估方法示例：\n",
    "- 构建包含原始问题、改写后问题、期望答案的数据集；\n",
    "- 判断最终生成的回答是否包含正确答案；\n",
    "- 统计准确率或召回率等指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e599072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# -----------------------------\n",
    "# Logging\n",
    "# -----------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class EvalExample:\n",
    "    original_q: str\n",
    "    rewritten_q: str\n",
    "    ground_truth_answer: str\n",
    "\n",
    "    # For retrieval eval:\n",
    "    # Provide gold doc ids that SHOULD appear in top-K retrieval results.\n",
    "    # These should match some metadata field in retrieved docs, e.g. doc.metadata[\"source\"] or [\"doc_id\"].\n",
    "    gold_doc_ids: List[str]\n",
    "\n",
    "\n",
    "class GroundTruthDataset:\n",
    "    def __init__(self):\n",
    "        self.data: List[EvalExample] = []\n",
    "\n",
    "    def add_example(\n",
    "        self,\n",
    "        original_q: str,\n",
    "        rewritten_q: str,\n",
    "        ground_truth_answer: str,\n",
    "        gold_doc_ids: List[str],\n",
    "    ):\n",
    "        self.data.append(\n",
    "            EvalExample(\n",
    "                original_q=original_q,\n",
    "                rewritten_q=rewritten_q,\n",
    "                ground_truth_answer=ground_truth_answer,\n",
    "                gold_doc_ids=gold_doc_ids,\n",
    "            )\n",
    "        )\n",
    "        logging.info(\"Added a new evaluation example\")\n",
    "\n",
    "    def get_all_examples(self) -> List[EvalExample]:\n",
    "        return self.data\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Retrieval metrics\n",
    "# -----------------------------\n",
    "def recall_at_k(\n",
    "    retrieved_doc_ids: List[str],\n",
    "    gold_doc_ids: List[str],\n",
    "    k: int,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Recall@K = (# of gold docs retrieved in top-K) / (# of gold docs)\n",
    "\n",
    "    If you provide gold_doc_ids as sources/files, this becomes: did we retrieve the right sources?\n",
    "    \"\"\"\n",
    "    if not gold_doc_ids:\n",
    "        return 0.0\n",
    "    topk = set(retrieved_doc_ids[:k])\n",
    "    gold = set(gold_doc_ids)\n",
    "    hit = len(topk.intersection(gold))\n",
    "    return hit / len(gold)\n",
    "\n",
    "\n",
    "def get_retrieved_doc_ids(\n",
    "    docs: List[Any],\n",
    "    id_key: str = \"source\",\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract doc identifiers from Document.metadata[id_key].\n",
    "    Common choices:\n",
    "      - id_key=\"source\" (often file path/name)\n",
    "      - id_key=\"doc_id\" (a stable chunk id you set)\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for d in docs:\n",
    "        md = getattr(d, \"metadata\", {}) or {}\n",
    "        out.append(str(md.get(id_key, \"\")))\n",
    "    return out\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# LLM-as-a-judge (Runnable/LCEL)\n",
    "# -----------------------------\n",
    "JUDGE_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a strict evaluator (judge) for a RAG QA system.\\n\"\n",
    "            \"Given a question, a candidate answer, and a reference (ground-truth) answer,\\n\"\n",
    "            \"decide if the candidate answer is correct.\\n\\n\"\n",
    "            \"Return ONLY valid JSON with keys:\\n\"\n",
    "            '  \"verdict\": \"correct\" | \"incorrect\"\\n'\n",
    "            '  \"score\": number between 0 and 1\\n'\n",
    "            '  \"rationale\": short explanation\\n\\n'\n",
    "            \"Be conservative: if the candidate misses key facts from the reference, mark incorrect.\\n\"\n",
    "            \"Do not output any extra text besides JSON.\"\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Question:\\n{question}\\n\\n\"\n",
    "            \"Candidate Answer:\\n{answer}\\n\\n\"\n",
    "            \"Reference Answer:\\n{reference}\\n\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def build_judge_chain(judge_llm):\n",
    "    # LCEL chain returns a JSON string -> we parse it ourselves robustly\n",
    "    return JUDGE_PROMPT | judge_llm | StrOutputParser()\n",
    "\n",
    "\n",
    "def safe_json_loads(s: str) -> Dict[str, Any]:\n",
    "    s = s.strip()\n",
    "    # try to extract first {...} if the model adds extra text\n",
    "    start = s.find(\"{\")\n",
    "    end = s.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        s = s[start : end + 1]\n",
    "    return json.loads(s)\n",
    "\n",
    "def _extract_json_object(text: str) -> str | None:\n",
    "    text = text.strip()\n",
    "    start = text.find(\"{\")\n",
    "    end = text.rfind(\"}\")\n",
    "    if start != -1 and end != -1 and end > start:\n",
    "        return text[start:end+1]\n",
    "    return None\n",
    "\n",
    "def _parse_non_json_judge(text: str) -> dict:\n",
    "    t = text.strip()\n",
    "\n",
    "    # Look for \"Verdict: Correct/Incorrect\"\n",
    "    verdict_match = re.search(r\"verdict\\s*:\\s*(correct|incorrect)\", t, flags=re.I)\n",
    "    verdict = verdict_match.group(1).lower() if verdict_match else None\n",
    "\n",
    "    # If no explicit verdict line, infer from keywords\n",
    "    if verdict is None:\n",
    "        if re.search(r\"\\bincorrect\\b\", t, flags=re.I):\n",
    "            verdict = \"incorrect\"\n",
    "        elif re.search(r\"\\bcorrect\\b\", t, flags=re.I):\n",
    "            verdict = \"correct\"\n",
    "        else:\n",
    "            verdict = \"incorrect\"\n",
    "\n",
    "    # Extract rationale if present\n",
    "    rationale_match = re.search(r\"rationale\\s*:\\s*(.*)\", t, flags=re.I | re.S)\n",
    "    rationale = rationale_match.group(1).strip() if rationale_match else t[:300]\n",
    "\n",
    "    # Score heuristic for POC\n",
    "    score = 1.0 if verdict == \"correct\" else 0.0\n",
    "\n",
    "    return {\"verdict\": verdict, \"score\": score, \"rationale\": rationale}\n",
    "\n",
    "def judge_answer(judge_chain, question: str, answer: str, reference: str) -> dict:\n",
    "    raw = judge_chain.invoke({\"question\": question, \"answer\": answer, \"reference\": reference})\n",
    "\n",
    "    # 1) Try JSON first\n",
    "    try:\n",
    "        js = _extract_json_object(raw)\n",
    "        if js:\n",
    "            j = json.loads(js)\n",
    "            return {\n",
    "                \"verdict\": str(j.get(\"verdict\", \"incorrect\")).lower(),\n",
    "                \"score\": float(j.get(\"score\", 0.0)),\n",
    "                \"rationale\": str(j.get(\"rationale\", \"\")).strip(),\n",
    "                \"raw\": raw,\n",
    "            }\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Judge JSON parse failed: {e}; raw={raw[:300]}\")\n",
    "\n",
    "    # 2) Fallback: parse non-JSON\n",
    "    parsed = _parse_non_json_judge(raw)\n",
    "    parsed[\"raw\"] = raw\n",
    "    return parsed\n",
    "\n",
    "# -----------------------------\n",
    "# End-to-end evaluator\n",
    "# -----------------------------\n",
    "class RAGEvaluator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        retriever: Any,\n",
    "        answer_generator: Callable[[str], str],\n",
    "        judge_chain: Any,\n",
    "        *,\n",
    "        id_key: str = \"source\",\n",
    "        k_list: List[int] = [3, 5, 10],\n",
    "        judge_correct_threshold: float = 0.5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        retriever: must support get_relevant_documents(query) OR invoke(query)\n",
    "        answer_generator: function(query)->answer (e.g., rag_chain.invoke)\n",
    "        judge_chain: built from build_judge_chain(judge_llm)\n",
    "        id_key: metadata field used as document ID for retrieval metrics\n",
    "        k_list: compute Recall@K for each K\n",
    "        judge_correct_threshold: score >= threshold => count as correct\n",
    "        \"\"\"\n",
    "        self.retriever = retriever\n",
    "        self.answer_generator = answer_generator\n",
    "        self.judge_chain = judge_chain\n",
    "        self.id_key = id_key\n",
    "        self.k_list = k_list\n",
    "        self.judge_correct_threshold = judge_correct_threshold\n",
    "\n",
    "    def _retrieve_docs(self, query: str) -> List[Any]:\n",
    "        # Support both old and new retriever interfaces\n",
    "        if hasattr(self.retriever, \"invoke\"):\n",
    "            return self.retriever.invoke(query)\n",
    "        if hasattr(self.retriever, \"get_relevant_documents\"):\n",
    "            return self.retriever.get_relevant_documents(query)\n",
    "        raise TypeError(\"Retriever must support .invoke(query) or .get_relevant_documents(query)\")\n",
    "\n",
    "    def evaluate_dataset(self, dataset: GroundTruthDataset) -> Dict[str, Any]:\n",
    "        examples = dataset.get_all_examples()\n",
    "        total = len(examples)\n",
    "\n",
    "        judge_correct = 0\n",
    "        recall_sums = {k: 0.0 for k in self.k_list}\n",
    "        per_example = []\n",
    "\n",
    "        for ex in examples:\n",
    "            q = ex.rewritten_q\n",
    "\n",
    "            # 1) Retrieval\n",
    "            docs = self._retrieve_docs(q)\n",
    "            retrieved_ids = get_retrieved_doc_ids(docs, id_key=self.id_key)\n",
    "\n",
    "            # 2) Compute Recall@K\n",
    "            recall_k = {}\n",
    "            for k in self.k_list:\n",
    "                r = recall_at_k(retrieved_ids, ex.gold_doc_ids, k=k)\n",
    "                recall_k[k] = r\n",
    "                recall_sums[k] += r\n",
    "\n",
    "            # 3) Generate answer\n",
    "            answer = self.answer_generator(q)\n",
    "\n",
    "            # 4) LLM-as-judge\n",
    "            judged = judge_answer(\n",
    "                self.judge_chain,\n",
    "                question=ex.original_q,\n",
    "                answer=answer,\n",
    "                reference=ex.ground_truth_answer,\n",
    "            )\n",
    "            is_correct = (judged[\"verdict\"] == \"correct\") and (judged[\"score\"] >= self.judge_correct_threshold)\n",
    "            judge_correct += int(is_correct)\n",
    "\n",
    "            logging.info(\n",
    "                f\"Q: {ex.original_q}\\n\"\n",
    "                f\"Rewritten: {ex.rewritten_q}\\n\"\n",
    "                f\"Judge: {judged['verdict']} (score={judged['score']:.2f})\\n\"\n",
    "                f\"Recall@{self.k_list}: {', '.join([f'{k}={recall_k[k]:.2f}' for k in self.k_list])}\"\n",
    "            )\n",
    "\n",
    "            per_example.append(\n",
    "                {\n",
    "                    \"original_q\": ex.original_q,\n",
    "                    \"rewritten_q\": ex.rewritten_q,\n",
    "                    \"gold_doc_ids\": ex.gold_doc_ids,\n",
    "                    \"retrieved_doc_ids\": retrieved_ids,\n",
    "                    \"recall_at_k\": recall_k,\n",
    "                    \"answer\": answer,\n",
    "                    \"judge\": judged,\n",
    "                    \"is_correct\": is_correct,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        judge_accuracy = judge_correct / total if total else 0.0\n",
    "        avg_recall = {k: (recall_sums[k] / total if total else 0.0) for k in self.k_list}\n",
    "\n",
    "        return {\n",
    "            \"total\": total,\n",
    "            \"judge_correct\": judge_correct,\n",
    "            \"judge_accuracy\": judge_accuracy,\n",
    "            \"avg_recall_at_k\": avg_recall,\n",
    "            \"examples\": per_example,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_report(metrics: Dict[str, Any]) -> str:\n",
    "        lines = []\n",
    "        lines.append(\"Evaluation Report\")\n",
    "        lines.append(f\"- Total examples: {metrics['total']}\")\n",
    "        lines.append(f\"- Judge accuracy: {metrics['judge_accuracy']:.2%} ({metrics['judge_correct']}/{metrics['total']})\")\n",
    "        lines.append(\"- Average Recall@K:\")\n",
    "        for k, v in metrics[\"avg_recall_at_k\"].items():\n",
    "            lines.append(f\"  - Recall@{k}: {v:.2%}\")\n",
    "        return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7e31e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 21:32:30,931 - WARNING - Some parameters are on the meta device because they were offloaded to the disk.\n",
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "import torch\n",
    "\n",
    "def build_judge_llm():\n",
    "    model_id = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    )\n",
    "\n",
    "    judge_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.0,      # IMPORTANT: deterministic for judging\n",
    "        do_sample=False,\n",
    "        return_full_text=False,\n",
    "    )\n",
    "\n",
    "    return HuggingFacePipeline(pipeline=judge_pipe)\n",
    "\n",
    "judge_llm = build_judge_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "170c4479",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 21:32:50,762 - INFO - Added a new evaluation example\n",
      "2026-01-05 21:37:26,714 - INFO - Q: I want to travel with my child\n",
      "Rewritten: Recommend destinations suitable for family trips and explain the reasons\n",
      "Judge: correct (score=1.00)\n",
      "Recall@[3, 5]: 3=0.00, 5=0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Report\n",
      "- Total examples: 1\n",
      "- Judge accuracy: 100.00% (1/1)\n",
      "- Average Recall@K:\n",
      "  - Recall@3: 0.00%\n",
      "  - Recall@5: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# 1) Build the judge chain\n",
    "judge_chain = build_judge_chain(judge_llm)\n",
    "\n",
    "# 2) Define the answer generator\n",
    "def answer_generator(q: str) -> str:\n",
    "    return rag_chain.invoke(q)\n",
    "\n",
    "# 3) Build the dataset\n",
    "# IMPORTANT: gold_doc_ids must match the value in your documents' metadata[id_key]\n",
    "dataset = GroundTruthDataset()\n",
    "dataset.add_example(\n",
    "    original_q=\"I want to travel with my child\",\n",
    "    rewritten_q=\"Recommend destinations suitable for family trips and explain the reasons\",\n",
    "    ground_truth_answer=(\n",
    "        \"Recommended two family-friendly destinations: \"\n",
    "        \"Shanghai Disneyland and Beijing Universal Studios\"\n",
    "    ),\n",
    "    gold_doc_ids=[\"travel.md\"],  # Example: if your document metadata[\"source\"] is \"travel.md\"\n",
    ")\n",
    "\n",
    "# 4) Run evaluation\n",
    "evaluator = RAGEvaluator(\n",
    "    retriever=retriever,\n",
    "    answer_generator=answer_generator,\n",
    "    judge_chain=judge_chain,\n",
    "    id_key=\"source\",        # Set to \"source\" or \"doc_id\" depending on your metadata\n",
    "    k_list=[3, 5],\n",
    "    judge_correct_threshold=0.5,\n",
    ")\n",
    "\n",
    "metrics = evaluator.evaluate_dataset(dataset)\n",
    "print(evaluator.generate_report(metrics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0611f341",
   "metadata": {},
   "source": [
    "通过此类评估，可以量化不同策略的效果差异，为后续优化提供数据支持。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6f418d",
   "metadata": {},
   "source": [
    "### 总结\n",
    "\n",
    "| 优化点 | 效果 |\n",
    "| --- | --- |\n",
    "| 问题改写 | 提高检索准确性，避免遗漏关键信息 |\n",
    "| 多步骤检索 | 提升复杂问题的回答完整性 |\n",
    "| 中文 Embedding | 提升向量召回质量 |\n",
    "| 本地 LLM | 隐私安全，可控性强 |\n",
    "\n",
    "\n",
    "### 三、更多优化建议\n",
    "\n",
    "| 优化方法 | 收益点 |\n",
    "| --- | --- |\n",
    "| 多变体召回 + 聚合 | 将多个改写后的查询同时提交给向量库，聚合结果 |\n",
    "| HyDE 策略 | 利用假设文档（Hypothetical Document）辅助检索 |\n",
    "| 多路召回 + Reranking | 使用 rerank 模块对检索结果打分排序，提升精度 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
