{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f08c3387",
   "metadata": {},
   "source": [
    "# 13. ç¨€ç–æŸ¥è¯¢å¬å›å·®ï¼ŸHyDEä¼ªæ–‡æ¡£ç”ŸæˆæŠ€æœ¯è§£å†³å†·å¯åŠ¨é—®é¢˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82001a47",
   "metadata": {},
   "source": [
    "\n",
    "## ä¸€ã€HyDEï¼šé€šè¿‡å‡æƒ³æ–‡æ¡£å¢å¼ºè¯­ä¹‰åŒ¹é…èƒ½åŠ›\n",
    "\n",
    "### æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "HyDE çš„æ ¸å¿ƒåœ¨äºâ€œå…ˆå‡è®¾ç­”æ¡ˆï¼Œå†ç”¨è¿™ä¸ªå‡è®¾å»åŒ¹é…çœŸå®æ–‡æ¡£â€ã€‚å…·ä½“æµç¨‹å¦‚ä¸‹ï¼š\n",
    "\n",
    "1. ä½¿ç”¨å¤§æ¨¡å‹åŸºäºç”¨æˆ·é—®é¢˜ç”Ÿæˆä¸€æ®µ**å‡æƒ³çš„ç­”æ¡ˆæ–‡æ¡£**ï¼›\n",
    "2. å°†è¯¥å‡æƒ³æ–‡æ¡£ç”¨äºå‘é‡æ£€ç´¢ï¼Œæ‰¾åˆ°æœ€ç›¸ä¼¼çš„çœŸå®æ–‡æ¡£ï¼›\n",
    "3. åŸºäºè¿™äº›çœŸå®æ–‡æ¡£ç”Ÿæˆæœ€ç»ˆå›ç­”ã€‚\n",
    "\n",
    "è¿™å°±åƒä½ åœ¨æ‰¾ä¸€æœ¬å…³äºâ€œäº²å­æ¸¸â€çš„ä¹¦æ—¶ï¼Œè„‘æµ·ä¸­å·²ç»æœ‰ä¸€ä¸ªå¤§è‡´çš„å†…å®¹è½®å»“ï¼Œç„¶åç”¨è¿™ä¸ªè½®å»“å»å›¾ä¹¦é¦†åŒ¹é…æœ€ç›¸å…³çš„ä¹¦ç±ã€‚\n",
    "\n",
    "### å®ç°ä»£ç "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d8d4b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import List, Optional\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9bc74f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# LangChain imports (stable across recent versions)\n",
    "# -----------------------------\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7ce3dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging to show INFO-level messages for easier debugging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "31398812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "def build_qwen_llm(\n",
    "    model_id: str = \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    max_new_tokens: int = 200,\n",
    "    temperature: float = 0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a LangChain LLM backed by a local Hugging Face Transformers pipeline.\n",
    "    Uses a small instruct model: Qwen2-0.5B-Instruct.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Loading HF tokenizer/model: {model_id}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    )\n",
    "\n",
    "    gen_pipe = pipeline(\n",
    "        task=\"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=temperature > 0,\n",
    "        temperature=temperature if temperature > 0 else None,\n",
    "        return_full_text=False,\n",
    "    )\n",
    "    return HuggingFacePipeline(pipeline=gen_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "25a1d7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _llm_to_text(llm, prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Robust helper that works across LangChain versions and HuggingFacePipeline behavior.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        out = llm.invoke(prompt)\n",
    "    except Exception:\n",
    "        out = llm(prompt)\n",
    "\n",
    "    if isinstance(out, str):\n",
    "        return out.strip()\n",
    "    return getattr(out, \"content\", str(out)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6dcf7e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyde_embed_query(\n",
    "    llm,\n",
    "    embeddings,\n",
    "    question: str,\n",
    "    include_original: bool = True,\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Manual HyDE (no HypotheticalDocumentEmbedder needed):\n",
    "      1) LLM generates a hypothetical passage for the question\n",
    "      2) Embed the hypothetical passage\n",
    "      3) Use that embedding as the query vector\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are generating a hypothetical passage to improve document retrieval.\\n\"\n",
    "        \"Write a concise, information-dense passage that could appear in a document\\n\"\n",
    "        \"that answers the question.\\n\"\n",
    "    )\n",
    "    if include_original:\n",
    "        prompt += \"Explicitly reflect the user's original intent.\\n\"\n",
    "    prompt += f\"\\nQuestion: {question}\\nHypothetical passage:\"\n",
    "\n",
    "    hypothetical_passage = _llm_to_text(llm, prompt)\n",
    "    return embeddings.embed_query(hypothetical_passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f83b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyDEQueryEvaluatorLangChainHF:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str = \"./12/data/\",\n",
    "        include_original: bool = True,\n",
    "        top_k: int = 5,\n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 150,\n",
    "        file_glob: str = \"**/*\",\n",
    "        llm_model_id: str = \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "        embedding_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    ):\n",
    "        self.include_original = include_original\n",
    "        self.top_k = top_k\n",
    "\n",
    "        logging.info(f\"Initializing evaluator. data_dir={data_dir}\")\n",
    "\n",
    "        # 1) Load documents\n",
    "        loader = DirectoryLoader(\n",
    "            data_dir,\n",
    "            glob=file_glob,\n",
    "            loader_cls=TextLoader,\n",
    "            loader_kwargs={\"encoding\": \"utf-8\"},\n",
    "            show_progress=True,\n",
    "            use_multithreading=True,\n",
    "        )\n",
    "        raw_docs = loader.load()\n",
    "        logging.info(f\"Loaded {len(raw_docs)} raw documents.\")\n",
    "\n",
    "        # 2) Split documents into chunks\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "        )\n",
    "        self.documents = splitter.split_documents(raw_docs)\n",
    "        logging.info(f\"Split into {len(self.documents)} chunks.\")\n",
    "\n",
    "        # 3) Embeddings (for indexing AND similarity evaluation)\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "        logging.info(f\"Embeddings loaded: {embedding_model_name}\")\n",
    "\n",
    "        # 4) Vector store\n",
    "        self.vectorstore = FAISS.from_documents(self.documents, self.embeddings)\n",
    "        logging.info(\"FAISS vector store built.\")\n",
    "\n",
    "        # 5) LLM (Qwen2-0.5B-Instruct)\n",
    "        self.llm = build_qwen_llm(model_id=llm_model_id, max_new_tokens=200, temperature=0.0)\n",
    "        logging.info(f\"LLM ready: {llm_model_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d4d1f14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Evaluation helpers (no sentence_transformers)\n",
    "# -----------------------------\n",
    "def evaluate_similarity(self, a: str, b: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity using the SAME embedding model used for retrieval.\n",
    "    This avoids sentence_transformers import + extra torch dependency surface area.\n",
    "    \"\"\"\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "    va = self.embeddings.embed_query(a)\n",
    "    vb = self.embeddings.embed_query(b)\n",
    "    return float(cosine_similarity([va], [vb])[0][0])\n",
    "\n",
    "def evaluate_result(self, question: str, retrieved_docs: List, ground_truth: str) -> float:\n",
    "    retrieved_text = \" \".join([d.page_content for d in retrieved_docs])\n",
    "    logging.info(f\"Evaluating retrieval result for question: '{question}'\")\n",
    "    score = self.evaluate_similarity(retrieved_text, ground_truth)\n",
    "    logging.info(f\"Evaluation completed, score: {score:.4f}\")\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "43a1367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# HyDE retrieval + answering\n",
    "# -----------------------------\n",
    "def hyde_retrieve(self, question: str, k: Optional[int] = None):\n",
    "    k = k or self.top_k\n",
    "    query_vec = hyde_embed_query(\n",
    "        llm=self.llm,\n",
    "        embeddings=self.embeddings,\n",
    "        question=question,\n",
    "        include_original=self.include_original,\n",
    "    )\n",
    "    return self.vectorstore.similarity_search_by_vector(query_vec, k=k)\n",
    "\n",
    "def answer_with_context(self, question: str, docs: List) -> str:\n",
    "    context = \"\\n\\n\".join([f\"[Chunk {i+1}]\\n{d.page_content}\" for i, d in enumerate(docs)])\n",
    "    prompt = (\n",
    "        \"Answer the question using ONLY the context below.\\n\"\n",
    "        \"If the context does not contain sufficient information, say you do not have enough evidence.\\n\\n\"\n",
    "        f\"<context>\\n{context}\\n</context>\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    return _llm_to_text(self.llm, prompt)\n",
    "\n",
    "def run_query(self, question: str, ground_truth: str, num_queries: int = 1) -> float:\n",
    "    print(\"\\n--- HyDE Query Demo Started (LangChain + HF/Qwen2) ---\\n\")\n",
    "    print(f\"â“ User Question: {question}\\n\")\n",
    "    print(\"ğŸ¤– The AI is analyzing the question using HyDE...\\n\")\n",
    "\n",
    "    scores = []\n",
    "    for i in range(num_queries):\n",
    "        if num_queries > 1:\n",
    "            print(f\"ğŸ”„ Running query {i + 1}/{num_queries}...\")\n",
    "\n",
    "        retrieved_docs = self.hyde_retrieve(question, k=self.top_k)\n",
    "\n",
    "        if i == 0:\n",
    "            answer = self.answer_with_context(question, retrieved_docs)\n",
    "\n",
    "            print(\"ğŸ’­ AI Answer:\")\n",
    "            print(\"-\" * 40)\n",
    "            print(answer)\n",
    "\n",
    "            print(\"\\nğŸ“š Retrieved Reference Chunks:\")\n",
    "            print(\"-\" * 40)\n",
    "            for idx, d in enumerate(retrieved_docs, 1):\n",
    "                print(f\"\\nDocument Chunk {idx}:\")\n",
    "                print(\"-\" * 30)\n",
    "                print(d.page_content)\n",
    "\n",
    "        score = self.evaluate_result(question, retrieved_docs, ground_truth)\n",
    "        scores.append(score)\n",
    "\n",
    "        if num_queries > 1:\n",
    "            print(f\"âœ… Score for query {i + 1}: {score:.4f}\")\n",
    "\n",
    "    avg = sum(scores) / len(scores)\n",
    "    print(\"\\nğŸ“Š HyDE Query Evaluation Results:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Average similarity score: {avg:.4f}\")\n",
    "    print(\"\\n--- HyDE Query Demo Finished (LangChain + HF/Qwen2) ---\\n\")\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e82cdd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/linghuang/Git/LLMs/LLM-RAG/12/data\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path.cwd().parent / \"12\" / \"data\"\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1a4c4c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 23:58:51,450 - INFO - Initializing evaluator. data_dir=/Users/linghuang/Git/LLMs/LLM-RAG/12/data\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 864.91it/s]\n",
      "2026-01-06 23:58:51,459 - INFO - Loaded 5 raw documents.\n",
      "2026-01-06 23:58:51,459 - INFO - Split into 5 chunks.\n",
      "2026-01-06 23:58:51,462 - INFO - Use pytorch device_name: mps\n",
      "2026-01-06 23:58:51,462 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "2026-01-06 23:58:52,536 - INFO - Embeddings loaded: sentence-transformers/all-MiniLM-L6-v2\n",
      "2026-01-06 23:58:52,585 - INFO - FAISS vector store built.\n",
      "2026-01-06 23:58:52,586 - INFO - Loading HF tokenizer/model: Qwen/Qwen2-0.5B-Instruct\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` requires `accelerate`. You can install it with `pip install accelerate`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m evaluator = \u001b[43mHyDEQueryEvaluatorLangChainHF\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_original\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m150\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm_model_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mQwen/Qwen2-0.5B-Instruct\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_model_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentence-transformers/all-MiniLM-L6-v2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 47\u001b[39m, in \u001b[36mHyDEQueryEvaluatorLangChainHF.__init__\u001b[39m\u001b[34m(self, data_dir, include_original, top_k, chunk_size, chunk_overlap, file_glob, llm_model_id, embedding_model_name)\u001b[39m\n\u001b[32m     44\u001b[39m logging.info(\u001b[33m\"\u001b[39m\u001b[33mFAISS vector store built.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# 5) LLM (Qwen2-0.7B-Instruct)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28mself\u001b[39m.llm = \u001b[43mbuild_qwen_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm_model_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLLM ready: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllm_model_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mbuild_qwen_llm\u001b[39m\u001b[34m(model_id, max_new_tokens, temperature)\u001b[39m\n\u001b[32m     16\u001b[39m logging.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading HF tokenizer/model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m gen_pipe = pipeline(\n\u001b[32m     25\u001b[39m     task=\u001b[33m\"\u001b[39m\u001b[33mtext-generation\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     26\u001b[39m     model=model,\n\u001b[32m   (...)\u001b[39m\u001b[32m     31\u001b[39m     return_full_text=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     32\u001b[39m )\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m HuggingFacePipeline(pipeline=gen_pipe)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm_clean/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm_clean/lib/python3.11/site-packages/transformers/modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/llm_clean/lib/python3.11/site-packages/transformers/modeling_utils.py:4806\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4804\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDeepSpeed Zero-3 is not compatible with passing a `device_map`.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4805\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[32m-> \u001b[39m\u001b[32m4806\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   4807\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mUsing a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4808\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrequires `accelerate`. You can install it with `pip install accelerate`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4809\u001b[39m         )\n\u001b[32m   4811\u001b[39m \u001b[38;5;66;03m# handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation.\u001b[39;00m\n\u001b[32m   4812\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit \u001b[38;5;129;01mor\u001b[39;00m load_in_8bit:\n",
      "\u001b[31mValueError\u001b[39m: Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` requires `accelerate`. You can install it with `pip install accelerate`"
     ]
    }
   ],
   "source": [
    "evaluator = HyDEQueryEvaluatorLangChainHF(\n",
    "    data_dir=path,\n",
    "    include_original=True,\n",
    "    top_k=5,\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=150,\n",
    "    llm_model_id=\"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24aad59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 17:08:35,241 - INFO - æ­£åœ¨åˆå§‹åŒ– HyDEQueryEvaluatorï¼Œæ•°æ®ç›®å½•: ./12/data/\n",
      "2025-07-16 17:08:35,286 - INFO - å·²åŠ è½½ 6 ä»½æ–‡æ¡£ã€‚\n",
      "2025-07-16 17:08:37,306 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-07-16 17:08:37,540 - INFO - å·²åˆ›å»ºæ–‡æ¡£çš„å‘é‡ç´¢å¼•ã€‚\n",
      "2025-07-16 17:08:37,567 - INFO - HyDE æŸ¥è¯¢è½¬æ¢å™¨å·²åˆå§‹åŒ–ï¼Œinclude_original: Trueã€‚\n",
      "2025-07-16 17:08:37,568 - INFO - åŸºç¡€æŸ¥è¯¢å¼•æ“å·²æ„å»ºï¼Œtop_k: 5ã€‚\n",
      "2025-07-16 17:08:37,568 - INFO - HyDE æŸ¥è¯¢å¼•æ“å·²å‡†å¤‡å°±ç»ªã€‚\n",
      "2025-07-16 17:08:37,597 - INFO - Use pytorch device_name: mps\n",
      "2025-07-16 17:08:37,598 - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-MiniLM-L12-v2\n",
      "2025-07-16 17:08:44,008 - INFO - è¯­ä¹‰ç›¸ä¼¼åº¦æ¨¡å‹ 'paraphrase-multilingual-MiniLM-L12-v2' å·²åŠ è½½ã€‚\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- HyDE æŸ¥è¯¢æ¼”ç¤ºå¼€å§‹ ---\n",
      "\n",
      "â“ ç”¨æˆ·é—®é¢˜: é€‚åˆäº²å­æ¸¸çš„åœ°æ–¹æœ‰å“ªäº›ï¼Ÿ\n",
      "\n",
      "ğŸ¤– AI æ­£åœ¨é€šè¿‡ HyDE åˆ†æå¹¶ç”Ÿæˆå›ç­”...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 17:08:48,076 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-07-16 17:08:49,174 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-07-16 17:08:50,073 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’­ AI å›ç­”:\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 17:08:50,787 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä¸Šæµ·è¿ªå£«å°¼ä¹å›­ã€åŒ—äº¬ç¯çƒå½±åŸå’Œå¹¿å·é•¿éš†æ¬¢ä¹ä¸–ç•Œæ˜¯é€‚åˆäº²å­æ¸¸çš„åœ°æ–¹ã€‚"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 17:08:51,258 - INFO - æ­£åœ¨è¯„ä¼°æŸ¥è¯¢ 'é€‚åˆäº²å­æ¸¸çš„åœ°æ–¹æœ‰å“ªäº›ï¼Ÿ' çš„ç»“æœ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“š å‚è€ƒä¾æ® (Source Nodes):\n",
      "----------------------------------------\n",
      "\n",
      "æ–‡æ¡£ç‰‡æ®µ 1:\n",
      "------------------------------\n",
      "# é€‚åˆäº²å­æ¸¸çš„ä¸»é¢˜å…¬å›­æ¨è\n",
      "\n",
      "## æ¨èæ™¯ç‚¹\n",
      "1. ä¸Šæµ·è¿ªå£«å°¼ä¹å›­ï¼ˆä¸Šæµ·ï¼‰\n",
      "   - è®¾æ–½å®Œå–„ï¼Œå„¿ç«¥æ¸¸ä¹åŒºä¸°å¯Œ\n",
      "2. åŒ—äº¬ç¯çƒå½±åŸï¼ˆåŒ—äº¬ï¼‰\n",
      "   - å¤šç§äº’åŠ¨é¡¹ç›®ï¼Œé€‚åˆå…¨å®¶æ¸¸ç©\n",
      "3. å¹¿å·é•¿éš†æ¬¢ä¹ä¸–ç•Œï¼ˆå¹¿å·ï¼‰\n",
      "   - åŠ¨ç‰©å›­+æ¸¸ä¹åœº+æ°´ä¸Šä¹å›­ä¸€ä½“åŒ–\n",
      "\n",
      "## å…±åŒç‰¹ç‚¹\n",
      "- æä¾›å©´å„¿è½¦ç§ŸèµæœåŠ¡\n",
      "- è®¾æœ‰æ¯å©´å®¤\n",
      "- å®‰å…¨æ€§é«˜ï¼Œé€‚åˆä½é¾„å„¿ç«¥\n",
      "\n",
      "## æ—…è¡Œå»ºè®®\n",
      "å»ºè®®é”™å³°å‡ºè¡Œï¼Œé¿å…èŠ‚å‡æ—¥äººæµé«˜å³°ã€‚\n",
      "\n",
      "æ–‡æ¡£ç‰‡æ®µ 2:\n",
      "------------------------------\n",
      "# ä¸Šæµ·è¿ªå£«å°¼ä¹å›­\n",
      "\n",
      "## åœ°å€\n",
      "ä¸Šæµ·å¸‚æµ¦ä¸œæ–°åŒºå·æ²™é•‡\n",
      "\n",
      "## é€‚åˆäººç¾¤\n",
      "äº²å­æ¸¸ã€æƒ…ä¾£ã€å®¶åº­å‡ºæ¸¸\n",
      "\n",
      "## ç®€ä»‹\n",
      "ä¸Šæµ·è¿ªå£«å°¼ä¹å›­æ˜¯ä¸­å›½å¤§é™†é¦–åº§è¿ªå£«å°¼ä¸»é¢˜å…¬å›­ï¼Œæ‹¥æœ‰å¤šä¸ªä¸»é¢˜å›­åŒºï¼ŒåŒ…æ‹¬â€œç±³å¥‡ç«¥è¯ä¸“åˆ—â€ã€â€œåˆ›æé€Ÿå…‰è½®â€ã€â€œé£è·ƒåœ°å¹³çº¿â€ç­‰çƒ­é—¨é¡¹ç›®ã€‚å›­å†…è¿˜æœ‰ä¸°å¯Œçš„è¡¨æ¼”å’Œå¤œé—´ç¯å…‰ç§€ã€‚\n",
      "\n",
      "## äº²å­å‹å¥½è®¾æ–½\n",
      "- å©´å„¿è½¦ç§Ÿèµ\n",
      "- å“ºä¹³å®¤\n",
      "- å„¿ç«¥æ¸¸ä¹åŒº\n",
      "- äº²å­é¤å…\n",
      "\n",
      "## æ¨èç†ç”±\n",
      "é€‚åˆå¸¦å­©å­çš„å®¶åº­ï¼Œé¡¹ç›®ä¸°å¯Œï¼Œæ²‰æµ¸æ„Ÿå¼ºï¼Œæ˜¯äº²å­æ¸¸çš„é¦–é€‰ç›®çš„åœ°ã€‚\n",
      "\n",
      "æ–‡æ¡£ç‰‡æ®µ 3:\n",
      "------------------------------\n",
      "# ä¸åŒåŸå¸‚æœ€ä½³æ—…æ¸¸æ—¶é—´æŒ‡å—\n",
      "\n",
      "## ä¸Šæµ·\n",
      "- æœ€ä½³å­£èŠ‚ï¼šæ˜¥ç§‹å­£ï¼ˆ3æœˆ-5æœˆã€9æœˆ-11æœˆï¼‰\n",
      "- é¿å¼€æ¢…é›¨å­£ï¼ˆ6æœˆ-7æœˆï¼‰å’Œå†¬å­£å¯’å†·æœŸ\n",
      "\n",
      "## åŒ—äº¬\n",
      "- æœ€ä½³å­£èŠ‚ï¼šæ˜¥ç§‹ä¸¤å­£ï¼ˆ4æœˆ-6æœˆã€9æœˆ-10æœˆï¼‰\n",
      "- æ³¨æ„é›¾éœ¾å¤©æ°”ï¼Œå»ºè®®æºå¸¦å£ç½©\n",
      "\n",
      "## æˆéƒ½\n",
      "- æœ€ä½³å­£èŠ‚ï¼šå…¨å¹´çš†å®œï¼Œæ°”å€™æ¸©å’Œ\n",
      "- å†¬å­£å¯å‰å¾€å³¨çœ‰å±±çœ‹é›ªæ™¯\n",
      "\n",
      "## æ¡‚æ—\n",
      "- æœ€ä½³å­£èŠ‚ï¼š4æœˆ-11æœˆ\n",
      "- é¿å¼€å¤å­£æš´é›¨é«˜å³°æœŸ\n",
      "\n",
      "## æ€»ç»“\n",
      "æ ¹æ®å‡ºè¡Œç›®çš„ï¼ˆäº²å­/æƒ…ä¾£/æ‘„å½±ï¼‰é€‰æ‹©åˆé€‚çš„æ—¶é—´æ®µï¼Œèƒ½æ˜¾è‘—æå‡æ—…è¡Œä½“éªŒã€‚\n",
      "\n",
      "æ–‡æ¡£ç‰‡æ®µ 4:\n",
      "------------------------------\n",
      "# ä¸Šæµ·åŸå¸‚æ¦‚è§ˆ\n",
      "\n",
      "## åœ°ç†ä½ç½®\n",
      "ä¸­å›½ä¸œéƒ¨æ²¿æµ·ï¼Œé•¿æ±Ÿå…¥æµ·å£\n",
      "\n",
      "## äººå£\n",
      "çº¦2415ä¸‡äººï¼ˆ2023å¹´ï¼‰\n",
      "\n",
      "## æ—…æ¸¸ç‰¹è‰²\n",
      "- å¤–æ»©å¤œæ™¯\n",
      "- ä¸œæ–¹æ˜ç å¡”\n",
      "- å—äº¬ä¸œè·¯æ­¥è¡Œè¡—\n",
      "- ä¸Šæµ·è¿ªå£«å°¼ä¹å›­\n",
      "\n",
      "## äº¤é€š\n",
      "- åœ°é“ç½‘ç»œå‘è¾¾\n",
      "- æœºåœºï¼šæµ¦ä¸œå›½é™…æœºåœºã€è™¹æ¡¥å›½é™…æœºåœº\n",
      "- å‡ºç§Ÿè½¦/ç½‘çº¦è½¦ä¾¿æ·\n",
      "\n",
      "## æ¨èç©æ³•\n",
      "äº²å­æ¸¸ï¼šè¿ªå£«å°¼ + æµ·æ´‹æ°´æ—é¦†  \n",
      "æƒ…ä¾£æ¸¸ï¼šå¤–æ»©å¤œæ™¯ + æ–°å¤©åœ°æ™šé¤  \n",
      "èƒŒåŒ…å®¢ï¼šæ­¦åº·è·¯éª‘è¡Œ + è¡¡å±±è·¯å°ä¼—å’–å•¡é¦†\n",
      "\n",
      "æ–‡æ¡£ç‰‡æ®µ 5:\n",
      "------------------------------\n",
      "# åŒ—äº¬ç«é”…æ¨è\n",
      "\n",
      "## æ¨èåœ°ç‚¹\n",
      "ç‹åºœäº•ã€å‰é—¨ã€è¥¿å•å•†åœˆ\n",
      "\n",
      "## ç‰¹è‰²ä»‹ç»\n",
      "åŒ—äº¬ç«é”…ä»¥æ¸…æ±¤é“œé”…ä¸ºä¸»ï¼Œæ­é…é²œç¾Šè‚‰ã€éº»é…±è˜¸æ–™ï¼Œæ˜¯å†¬æ—¥é‡Œæœ€å—æ¬¢è¿çš„ä¼ ç»Ÿç¾é£Ÿä¹‹ä¸€ã€‚\n",
      "\n",
      "## æ¨èé¤å…\n",
      "1. å››å­£æ°‘ç¦ï¼ˆæ•…å®«åº—ï¼‰ - ç¯å¢ƒä¼˜é›…ï¼Œå¯è¿œçœºæ•…å®«\n",
      "2. å°åŠæ¢¨æ±¤ - è£…ä¿®æ–‡è‰ºï¼Œé€‚åˆæœ‹å‹èšé¤\n",
      "3. é“¶é”…ç¾Šèå­ç«é”… - å£å‘³åœ°é“ï¼Œä»·æ ¼äº²æ°‘\n",
      "\n",
      "## æ¸©é¦¨æç¤º\n",
      "å»ºè®®æå‰é¢„çº¦ï¼ŒèŠ‚å‡æ—¥äººæµé‡è¾ƒå¤§ã€‚\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c0c981c7e349e4a8194b4eea3c411e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3f56e8baf340319f918c89605b9ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 17:08:51,585 - INFO - è¯„ä¼°å®Œæˆï¼Œå¾—åˆ†: 0.7237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š HyDE æŸ¥è¯¢è¯„ä¼°ç»“æœ:\n",
      "----------------------------------------\n",
      "å¹³å‡ç›¸ä¼¼åº¦å¾—åˆ†: 0.7237\n",
      "\n",
      "--- HyDE æŸ¥è¯¢æ¼”ç¤ºç»“æŸ ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from llama_index.core.indices.query.query_transform.base import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "# é…ç½®æ—¥å¿—è®°å½•ï¼Œæ˜¾ç¤ºä¿¡æ¯çº§åˆ«æ—¥å¿—ï¼Œæ–¹ä¾¿è°ƒè¯•å’Œè§‚å¯Ÿæ¨¡å‹è¡Œä¸º\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class HyDEQueryEvaluator:\n",
    "    def __init__(self, data_dir='./12/data/', include_original=False, top_k=10):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ– HyDE æŸ¥è¯¢è¯„ä¼°å™¨ã€‚\n",
    "        è¿™ä¸ªç±»è´Ÿè´£åŠ è½½æ•°æ®ã€æ„å»ºç´¢å¼•ã€è®¾ç½® HyDE æŸ¥è¯¢è½¬æ¢ä»¥åŠè¿è¡Œè¯„ä¼°ã€‚\n",
    "\n",
    "        :param data_dir: å­˜å‚¨æ–‡æ¡£çš„ç›®å½•è·¯å¾„ã€‚\n",
    "        :param include_original: å¸ƒå°”å€¼ï¼ŒæŒ‡ç¤º HyDE æ˜¯å¦åœ¨ç”Ÿæˆå‡è®¾æ€§æ–‡æ¡£æ—¶åŒ…å«åŸå§‹æŸ¥è¯¢ã€‚\n",
    "                                 è®¾ç½®ä¸º True å¯ä»¥è®©æ¨¡å‹åŒæ—¶è€ƒè™‘åŸå§‹æ„å›¾å’Œå‡è®¾æ€§å†…å®¹ã€‚\n",
    "        :param top_k: ç›¸ä¼¼åº¦æŸ¥è¯¢æ—¶è¿”å›çš„å‰ k ä¸ªæœ€ç›¸å…³ç»“æœã€‚\n",
    "        \"\"\"\n",
    "        logging.info(f\"æ­£åœ¨åˆå§‹åŒ– HyDEQueryEvaluatorï¼Œæ•°æ®ç›®å½•: {data_dir}\")\n",
    "        # 1. åŠ è½½æ–‡æ¡£ï¼šä»æŒ‡å®šç›®å½•è¯»å–æ‰€æœ‰æ–‡æ¡£ã€‚SimpleDirectoryReader ä¼šè‡ªåŠ¨å¤„ç†å¤šç§æ–‡ä»¶ç±»å‹ã€‚\n",
    "        self.documents = SimpleDirectoryReader(data_dir).load_data()\n",
    "        logging.info(f\"å·²åŠ è½½ {len(self.documents)} ä»½æ–‡æ¡£ã€‚\")\n",
    "\n",
    "        # 2. åˆå§‹åŒ–å‘é‡å­˜å‚¨ç´¢å¼•ï¼šå°†æ–‡æ¡£è½¬æ¢ä¸ºå‘é‡å¹¶å­˜å‚¨ï¼Œä»¥ä¾¿è¿›è¡Œé«˜æ•ˆçš„ç›¸ä¼¼åº¦æ£€ç´¢ã€‚\n",
    "        # é»˜è®¤ä½¿ç”¨ OpenAI çš„åµŒå…¥æ¨¡å‹ã€‚\n",
    "        self.sentence_index = VectorStoreIndex.from_documents(self.documents)\n",
    "        logging.info(\"å·²åˆ›å»ºæ–‡æ¡£çš„å‘é‡ç´¢å¼•ã€‚\")\n",
    "\n",
    "        # 3. åˆå§‹åŒ– HyDE æŸ¥è¯¢è½¬æ¢å™¨ï¼šè¿™æ˜¯ HyDE çš„æ ¸å¿ƒéƒ¨åˆ†ï¼Œå®ƒä¼šæ ¹æ®åŸå§‹æŸ¥è¯¢ç”Ÿæˆä¸€ä¸ªå‡è®¾æ€§æ–‡æ¡£ã€‚\n",
    "        # include_original=True è¡¨ç¤ºåœ¨æ£€ç´¢æ—¶ï¼Œä¼šå°†åŸå§‹æŸ¥è¯¢å’Œç”Ÿæˆçš„å‡è®¾æ€§æ–‡æ¡£éƒ½ç”¨äºæŸ¥æ‰¾ã€‚\n",
    "        self.hyde = HyDEQueryTransform(include_original=include_original)\n",
    "        logging.info(f\"HyDE æŸ¥è¯¢è½¬æ¢å™¨å·²åˆå§‹åŒ–ï¼Œinclude_original: {include_original}ã€‚\")\n",
    "\n",
    "        # 4. æ„å»ºåŸºç¡€æŸ¥è¯¢å¼•æ“ï¼šç”¨äºæ‰§è¡Œå®é™…çš„å‘é‡ç›¸ä¼¼åº¦æœç´¢ã€‚\n",
    "        # streaming=True å¯ä»¥åœ¨å“åº”ç”Ÿæˆæ—¶é€æ­¥è¾“å‡ºï¼Œæå‡ç”¨æˆ·ä½“éªŒã€‚\n",
    "        # similarity_top_k å†³å®šäº†æ¯æ¬¡æŸ¥è¯¢è¿”å›å¤šå°‘ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µã€‚\n",
    "        self.query_engine = self.sentence_index.as_query_engine(\n",
    "            streaming=True,\n",
    "            similarity_top_k=top_k\n",
    "        )\n",
    "        logging.info(f\"åŸºç¡€æŸ¥è¯¢å¼•æ“å·²æ„å»ºï¼Œtop_k: {top_k}ã€‚\")\n",
    "\n",
    "        # 5. åŒ…è£…æˆæ”¯æŒ HyDE çš„æŸ¥è¯¢å¼•æ“ï¼šå°†åŸºç¡€æŸ¥è¯¢å¼•æ“ä¸ HyDE è½¬æ¢å™¨ç»“åˆã€‚\n",
    "        # è¿™æ ·ï¼Œæ¯æ¬¡æŸ¥è¯¢éƒ½ä¼šå…ˆé€šè¿‡ HyDE è¿›è¡Œè½¬æ¢ï¼Œç„¶åå†ç”±åŸºç¡€å¼•æ“æ‰§è¡Œã€‚\n",
    "        self.hyde_query_engine = TransformQueryEngine(\n",
    "            self.query_engine,\n",
    "            query_transform=self.hyde\n",
    "        )\n",
    "        logging.info(\"HyDE æŸ¥è¯¢å¼•æ“å·²å‡†å¤‡å°±ç»ªã€‚\")\n",
    "\n",
    "        # 6. åˆå§‹åŒ–è¯­ä¹‰ç›¸ä¼¼åº¦æ¨¡å‹ï¼šç”¨äºè®¡ç®—æŸ¥è¯¢ç»“æœä¸çœŸå®ç­”æ¡ˆä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦ã€‚\n",
    "        # 'paraphrase-multilingual-MiniLM-L12-v2' æ˜¯ä¸€ä¸ªå¤šè¯­è¨€æ¨¡å‹ï¼Œé€‚åˆå¤„ç†ä¸­æ–‡ã€‚\n",
    "        self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "        logging.info(\"è¯­ä¹‰ç›¸ä¼¼åº¦æ¨¡å‹ 'paraphrase-multilingual-MiniLM-L12-v2' å·²åŠ è½½ã€‚\")\n",
    "\n",
    "    def evaluate_similarity(self, response_text, ground_truth):\n",
    "        \"\"\"\n",
    "        ä½¿ç”¨ SentenceTransformer æ¨¡å‹è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚\n",
    "        ç”¨äºé‡åŒ–æ£€ç´¢åˆ°çš„ä¿¡æ¯ä¸çœŸå®ç­”æ¡ˆçš„åŒ¹é…ç¨‹åº¦ã€‚\n",
    "\n",
    "        :param response_text: ä»æ¨¡å‹å“åº”ä¸­æå–çš„æ–‡æœ¬ç‰‡æ®µã€‚\n",
    "        :param ground_truth: é¢„è®¾çš„çœŸå®ç­”æ¡ˆæˆ–æ ‡å‡†å‚è€ƒæ–‡æœ¬ã€‚\n",
    "        :return: ä¸¤ä¸ªæ–‡æœ¬åµŒå…¥å‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦å¾—åˆ†ï¼ŒèŒƒå›´é€šå¸¸åœ¨ -1 åˆ° 1 ä¹‹é—´ï¼ˆ1 è¡¨ç¤ºå®Œå…¨ç›¸åŒï¼‰ã€‚\n",
    "        \"\"\"\n",
    "        # å¦‚æœä»»ä¸€æ–‡æœ¬ä¸ºç©ºï¼Œåˆ™ç›´æ¥è¿”å›0ï¼Œé¿å…ç¼–ç é”™è¯¯\n",
    "        if not response_text or not ground_truth:\n",
    "            return 0.0\n",
    "\n",
    "        # å°†æ–‡æœ¬ç¼–ç ä¸ºå‘é‡ï¼ˆåµŒå…¥ï¼‰ã€‚æ¨¡å‹ä¼šæ•è·æ–‡æœ¬çš„è¯­ä¹‰ä¿¡æ¯ã€‚\n",
    "        response_embedding = self.model.encode([response_text], convert_to_tensor=True)\n",
    "        ground_truth_embedding = self.model.encode([ground_truth], convert_to_tensor=True)\n",
    "\n",
    "        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ã€‚ä½™å¼¦ç›¸ä¼¼åº¦è¡¡é‡ä¸¤ä¸ªå‘é‡æ–¹å‘çš„ç›¸ä¼¼æ€§ï¼Œä¸å‘é‡é•¿åº¦æ— å…³ã€‚\n",
    "        similarity = cosine_similarity(response_embedding.cpu(), ground_truth_embedding.cpu())[0][0]\n",
    "        return float(similarity)\n",
    "\n",
    "    def evaluate_result(self, question, response, ground_truth):\n",
    "        \"\"\"\n",
    "        è¯„ä¼°ä¸€æ¬¡æŸ¥è¯¢ç»“æœçš„è´¨é‡ï¼Œä¸»è¦æ˜¯é€šè¿‡è®¡ç®—æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µä¸çœŸå®ç­”æ¡ˆçš„ç›¸ä¼¼åº¦ã€‚\n",
    "\n",
    "        :param question: ç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢é—®é¢˜ã€‚\n",
    "        :param response: LlamaIndex æŸ¥è¯¢å¼•æ“è¿”å›çš„å“åº”å¯¹è±¡ï¼ŒåŒ…å«æºæ–‡æ¡£èŠ‚ç‚¹ã€‚\n",
    "        :param ground_truth: é—®é¢˜çš„çœŸå®ç­”æ¡ˆã€‚\n",
    "        :return: æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µä¸çœŸå®ç­”æ¡ˆçš„ç›¸ä¼¼åº¦å¾—åˆ†ã€‚\n",
    "        \"\"\"\n",
    "        # ä»å“åº”çš„æºèŠ‚ç‚¹ä¸­æå–æ‰€æœ‰æ–‡æœ¬ï¼Œæ‹¼æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²ã€‚\n",
    "        # è¿™äº›æºèŠ‚ç‚¹æ˜¯ RAG æ¨¡å‹ç”¨æ¥ç”Ÿæˆå›ç­”çš„åŸå§‹ä¿¡æ¯ã€‚\n",
    "        response_text = ' '.join([node.text for node in response.source_nodes])\n",
    "        logging.info(f\"æ­£åœ¨è¯„ä¼°æŸ¥è¯¢ '{question}' çš„ç»“æœ...\")\n",
    "        # è°ƒç”¨è¯­ä¹‰ç›¸ä¼¼åº¦è¯„ä¼°å‡½æ•°\n",
    "        score = self.evaluate_similarity(response_text, ground_truth)\n",
    "        logging.info(f\"è¯„ä¼°å®Œæˆï¼Œå¾—åˆ†: {score:.4f}\")\n",
    "        return score\n",
    "\n",
    "    def run_query(self, question, ground_truth, num_queries=3):\n",
    "        \"\"\"\n",
    "        æ‰§è¡Œå¤šæ¬¡æŸ¥è¯¢å¹¶è¾“å‡ºè¯¦ç»†ç»“æœï¼ŒåŒ…æ‹¬ AI å›ç­”ã€å‚è€ƒæ–‡æ¡£å’Œè¯„ä¼°å¾—åˆ†ã€‚\n",
    "        è¿™ä¸ªæ–¹æ³•ä¸»è¦ç”¨äºæ¼”ç¤º HyDE å¦‚ä½•å½±å“æŸ¥è¯¢ç»“æœå’Œæ£€ç´¢åˆ°çš„ä¿¡æ¯ã€‚\n",
    "\n",
    "        :param question: ç”¨æˆ·è¦æŸ¥è¯¢çš„é—®é¢˜ã€‚\n",
    "        :param ground_truth: è¯¥é—®é¢˜çš„æ ‡å‡†çœŸå®ç­”æ¡ˆï¼Œç”¨äºè¯„ä¼°ã€‚\n",
    "        :param num_queries: æ‰§è¡ŒæŸ¥è¯¢çš„æ¬¡æ•°ï¼Œç”¨äºè§‚å¯Ÿç»“æœçš„ç¨³å®šæ€§ï¼ˆé€šå¸¸ HyDE æ¯æ¬¡ç»“æœä¸ä¼šæœ‰å¤ªå¤§å˜åŒ–ï¼‰ã€‚\n",
    "        \"\"\"\n",
    "        print(f\"\\n--- HyDE æŸ¥è¯¢æ¼”ç¤ºå¼€å§‹ ---\\n\")\n",
    "        print(f\"â“ ç”¨æˆ·é—®é¢˜: {question}\\n\")\n",
    "        print(\"ğŸ¤– AI æ­£åœ¨é€šè¿‡ HyDE åˆ†æå¹¶ç”Ÿæˆå›ç­”...\\n\")\n",
    "\n",
    "        scores = []\n",
    "        for i in range(num_queries):\n",
    "            if num_queries > 1:\n",
    "                print(f\"ğŸ”„ ç¬¬ {i+1}/{num_queries} æ¬¡æŸ¥è¯¢ä¸­...\")\n",
    "\n",
    "            # æ‰§è¡Œ HyDE å¢å¼ºçš„æŸ¥è¯¢ã€‚è¿™ä¸€æ­¥ä¼šå…ˆç”Ÿæˆå‡è®¾æ€§æ–‡æ¡£ï¼Œç„¶åç”¨å®ƒæ¥æ£€ç´¢ã€‚\n",
    "            response = self.hyde_query_engine.query(question)\n",
    "\n",
    "            # é¦–æ¬¡æŸ¥è¯¢æ—¶ï¼Œè¯¦ç»†è¾“å‡º AI çš„å›ç­”å’Œä½œä¸ºå‚è€ƒçš„æºæ–‡æ¡£ç‰‡æ®µã€‚\n",
    "            if i == 0:\n",
    "                print(\"ğŸ’­ AI å›ç­”:\")\n",
    "                print(\"-\" * 40)\n",
    "                # print_response_stream() å¯ä»¥é€æ­¥æ‰“å°å›ç­”ï¼Œæä¾›æ›´å¥½çš„ç”¨æˆ·ä½“éªŒã€‚\n",
    "                response.print_response_stream()\n",
    "\n",
    "                print(\"\\nğŸ“š å‚è€ƒä¾æ® (Source Nodes):\")\n",
    "                print(\"-\" * 40)\n",
    "                # éå†å¹¶æ‰“å°å‡ºæ¨¡å‹ç”¨æ¥ç”Ÿæˆå›ç­”çš„åŸå§‹æ–‡æ¡£ç‰‡æ®µã€‚\n",
    "                for idx, node in enumerate(response.source_nodes, 1):\n",
    "                    print(f\"\\næ–‡æ¡£ç‰‡æ®µ {idx}:\")\n",
    "                    print(\"-\" * 30)\n",
    "                    print(node.text)\n",
    "\n",
    "            # è¯„ä¼°å½“å‰æŸ¥è¯¢çš„ç›¸ä¼¼åº¦å¾—åˆ†ã€‚\n",
    "            score = self.evaluate_result(question, response, ground_truth)\n",
    "            scores.append(score)\n",
    "            if num_queries > 1:\n",
    "                print(f\"âœ… ç¬¬ {i+1} æ¬¡æŸ¥è¯¢å¾—åˆ†: {score:.4f}\")\n",
    "\n",
    "        # è®¡ç®—å¹¶æ‰“å°æ‰€æœ‰æŸ¥è¯¢çš„å¹³å‡å¾—åˆ†ã€‚\n",
    "        average_score = sum(scores) / len(scores)\n",
    "        print(\"\\nğŸ“Š HyDE æŸ¥è¯¢è¯„ä¼°ç»“æœ:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"å¹³å‡ç›¸ä¼¼åº¦å¾—åˆ†: {average_score:.4f}\")\n",
    "        print(f\"\\n--- HyDE æŸ¥è¯¢æ¼”ç¤ºç»“æŸ ---\\n\")\n",
    "        return average_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # åˆå§‹åŒ–è¯„ä¼°å™¨å®ä¾‹ã€‚å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ data_dir, include_original å’Œ top_kã€‚\n",
    "    # data_dir æŒ‡å‘ä½ çš„æ–‡æ¡£å­˜å‚¨ä½ç½®ã€‚\n",
    "    # include_original=True ä¼šè®© HyDE åŒæ—¶è€ƒè™‘åŸå§‹æŸ¥è¯¢å’Œå‡è®¾æ€§æŸ¥è¯¢ã€‚\n",
    "    # top_k=5 è¡¨ç¤ºæ£€ç´¢æœ€ç›¸å…³çš„ 5 ä¸ªæ–‡æ¡£ç‰‡æ®µã€‚\n",
    "    evaluator = HyDEQueryEvaluator(data_dir='./12/data/', include_original=True, top_k=5)\n",
    "\n",
    "    # ç¤ºä¾‹é—®é¢˜ã€‚è¯·ç¡®ä¿ä½ çš„æ–‡æ¡£ä¸­åŒ…å«èƒ½å¤Ÿå›ç­”è¿™ä¸ªé—®é¢˜çš„ç›¸å…³ä¿¡æ¯ã€‚\n",
    "    question = \"é€‚åˆäº²å­æ¸¸çš„åœ°æ–¹æœ‰å“ªäº›ï¼Ÿ\"\n",
    "\n",
    "    # é¢„è®¾çš„çœŸå®ç­”æ¡ˆï¼ˆGround Truthï¼‰ã€‚è¿™æ˜¯ç”¨äºè¯„ä¼°æ¨¡å‹æ£€ç´¢æ•ˆæœçš„åŸºå‡†ã€‚\n",
    "    # è¯·æ ¹æ®ä½ å®é™…æ–‡æ¡£ä¸­çš„å†…å®¹ï¼Œæä¾›ä¸€ä¸ªå‡†ç¡®ä¸”ç®€æ´çš„ç­”æ¡ˆã€‚\n",
    "    ground_truth = \"é€‚åˆäº²å­æ¸¸çš„åœ°æ–¹æœ‰ä¸Šæµ·è¿ªå£«å°¼ä¹å›­ã€åŒ—äº¬ç¯çƒå½±åŸå’Œå¹¿å·é•¿éš†æ¬¢ä¹ä¸–ç•Œç­‰ä¸»é¢˜å…¬å›­ã€‚\"\n",
    "\n",
    "    # æ‰§è¡ŒæŸ¥è¯¢ã€‚num_queries=1 æ›´é€‚åˆæ¼”ç¤ºï¼Œå› ä¸ºå®ƒä¼šæ‰“å°è¯¦ç»†çš„ AI å›ç­”å’Œå‚è€ƒæ–‡æ¡£ã€‚\n",
    "    # å¦‚æœæƒ³è§‚å¯Ÿ HyDE åœ¨å¤šæ¬¡æŸ¥è¯¢ä¸­çš„ç¨³å®šæ€§ï¼Œå¯ä»¥å¢åŠ  num_queriesã€‚\n",
    "    evaluator.run_query(question, ground_truth, num_queries=1) # æ¼”ç¤ºæ—¶é€šå¸¸ä¸€æ¬¡å°±å¤Ÿäº†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e212634c",
   "metadata": {},
   "source": [
    "### 1. æå‡æ£€ç´¢ç›¸å…³æ€§ï¼Œè·å–ç²¾å‡†ç­”æ¡ˆ\n",
    "HyDEé€šè¿‡ç”Ÿæˆä¸é—®é¢˜ç›¸å…³çš„\"å‡è®¾æ€§æ–‡æ¡£\"ï¼Œå¸®åŠ©ç³»ç»Ÿæ›´å‡†ç¡®åœ°ç†è§£ç”¨æˆ·æŸ¥è¯¢æ„å›¾ã€‚åœ¨æœ¬æ¬¡æ¡ˆä¾‹ä¸­ï¼š\n",
    "\n",
    "- åŸå§‹é—®é¢˜ï¼š é€‚åˆäº²å­æ¸¸çš„åœ°æ–¹æœ‰å“ªäº›ï¼Ÿ\n",
    "- HyDEç”Ÿæˆçš„å‡è®¾æ–‡æ¡£å¯èƒ½åŒ…å«ç±»ä¼¼ ä¸»é¢˜å…¬å›­ ã€ å„¿ç«¥è®¾æ–½ ç­‰å…³é”®è¯­ä¹‰\n",
    "- æœ€ç»ˆç³»ç»Ÿç²¾å‡†è¿”å›äº†æ–‡æ¡£ä¸­ ä¸Šæµ·è¿ªå£«å°¼ä¹å›­ã€åŒ—äº¬ç¯çƒå½±åŸã€å¹¿å·é•¿éš†æ¬¢ä¹ä¸–ç•Œ ä¸‰ä¸ªæ ¸å¿ƒç»“æœï¼ˆå®Œå…¨åŒ¹é… `é€‚åˆäº²å­æ¸¸çš„ä¸»é¢˜å…¬å›­æ¨è.md` ä¸­çš„å†…å®¹ï¼‰\n",
    "- é¿å…äº†æ— å…³æ–‡æ¡£å¹²æ‰°ï¼ˆå¦‚è¾“å‡ºä¸­è¢«æˆªæ–­çš„ åŒ—äº¬ç«é”…æ¨è æ–‡æ¡£ç‰‡æ®µæœªå½±å“æœ€ç»ˆç»“æœï¼‰\n",
    "### 2. å¢å¼ºç»“æœç¨³å®šæ€§ï¼Œé™ä½éšæœºè¯¯å·®\n",
    "é€šè¿‡ä¸‰æ¬¡é‡å¤æŸ¥è¯¢æµ‹è¯•ï¼ŒHyDEå±•ç°äº†ä¼˜ç§€çš„ç»“æœä¸€è‡´æ€§ï¼š\n",
    "\n",
    "- ä¸‰æ¬¡æŸ¥è¯¢å¾—åˆ†å‡ä¸º 0.7249 ï¼ˆè¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰\n",
    "- å›ç­”å†…å®¹å®Œå…¨ä¸€è‡´ï¼Œæœªå‡ºç°å› æ£€ç´¢éšæœºæ€§å¯¼è‡´çš„ç­”æ¡ˆæ³¢åŠ¨\n",
    "- è¿™è¡¨æ˜HyDEç”Ÿæˆçš„å‡è®¾æ–‡æ¡£èƒ½å¤Ÿç¨³å®šå¼•å¯¼æ£€ç´¢ç³»ç»Ÿèšç„¦äºé«˜è´¨é‡ç›¸å…³æ–‡æ¡£\n",
    "### 3. ä¼˜åŒ–è¯­ä¹‰åŒ¹é…èƒ½åŠ›ï¼Œè¶…è¶Šå…³é”®è¯æ£€ç´¢\n",
    "ä¼ ç»Ÿå…³é”®è¯æ£€ç´¢å¯èƒ½å—é™äºå­—é¢åŒ¹é…ï¼Œè€ŒHyDEé€šè¿‡ä»¥ä¸‹æœºåˆ¶æå‡è¯­ä¹‰ç†è§£ï¼š\n",
    "\n",
    "- å‡è®¾æ–‡æ¡£ç”Ÿæˆ ï¼šå°†ç®€çŸ­æŸ¥è¯¢ï¼ˆå¦‚ é€‚åˆäº²å­æ¸¸çš„åœ°æ–¹ ï¼‰æ‰©å±•ä¸ºåŒ…å«ä¸°å¯Œè¯­ä¹‰çš„å‡è®¾æ–‡æ¡£\n",
    "- æ·±å±‚è¯­ä¹‰å¯¹é½ ï¼šé€šè¿‡ `HyDEQueryTransform` å®ç°æŸ¥è¯¢ä¸æ–‡æ¡£çš„è¯­ä¹‰çº§åŒ¹é…\n",
    "- ç»“æœ ï¼šå³ä½¿æ–‡æ¡£ä¸­æœªç›´æ¥å‡ºç° äº²å­æ¸¸ å®Œæ•´å…³é”®è¯ï¼ˆå®é™…æ–‡æ¡£æ ‡é¢˜å·²åŒ…å«ï¼‰ï¼Œä»èƒ½ç²¾å‡†å®šä½ç›¸å…³å†…å®¹\n",
    "### 4. æå‡å¤æ‚æŸ¥è¯¢å¤„ç†èƒ½åŠ›\n",
    "å¯¹äºå¼€æ”¾å¼é—®é¢˜ï¼ˆå¦‚æ¨èç±»ã€æ¯”è¾ƒç±»é—®é¢˜ï¼‰ï¼ŒHyDEå±•ç°å‡ºç‹¬ç‰¹ä¼˜åŠ¿ï¼š\n",
    "\n",
    "- è‡ªåŠ¨è¡¥å……æŸ¥è¯¢ä¸Šä¸‹æ–‡ï¼ˆå¦‚éšå«çš„ å®‰å…¨æ€§ ã€ å„¿ç«¥è®¾æ–½ ç­‰è¯„ä¼°ç»´åº¦ï¼‰\n",
    "- å¸®åŠ©ç³»ç»Ÿä»å¤šä¸ªæ–‡æ¡£ç‰‡æ®µä¸­èšåˆä¿¡æ¯ï¼ˆå¦‚æœ¬æ¬¡ç»“æœæ•´åˆäº†æ¨èæ™¯ç‚¹å’Œå…±åŒç‰¹ç‚¹ä¸¤éƒ¨åˆ†å†…å®¹ï¼‰\n",
    "- å¹³å‡ç›¸ä¼¼åº¦å¾—åˆ† 0.7249 è¡¨æ˜å›ç­”ä¸çœŸå®ç­”æ¡ˆé«˜åº¦ä¸€è‡´\n",
    "\n",
    "é€šè¿‡è¾“å‡ºç»“æœå¯è§ï¼ŒHyDEæœ‰æ•ˆè§£å†³äº†ä¼ ç»ŸRAGç³»ç»Ÿä¸­\"æŸ¥è¯¢æ„å›¾ç†è§£ä¸è¶³\"å’Œ\"æ£€ç´¢ç»“æœä¸ç¨³å®š\"ä¸¤å¤§æ ¸å¿ƒé—®é¢˜ï¼Œè™½ç„¶è¿™ä¸ªâ€œå‡æƒ³æ–‡æ¡£â€æ˜¯è™šæ„çš„ï¼Œä½†å®ƒåœ¨é£æ ¼å’Œå†…å®¹ä¸Šä¸çœŸå®æ—…æ¸¸æ”»ç•¥éå¸¸æ¥è¿‘ï¼Œä»è€Œæå‡äº†æ£€ç´¢çš„ç›¸å…³æ€§ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290772ef",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf6580c",
   "metadata": {},
   "source": [
    "## äºŒ æå–æ ‡ç­¾å¢å¼ºæ£€ç´¢\n",
    "\n",
    "åœ¨å‘é‡æ£€ç´¢çš„åŸºç¡€ä¸Šï¼Œæ·»åŠ æ ‡ç­¾è¿‡æ»¤æœºåˆ¶èƒ½å¤Ÿæ˜¾è‘—æå‡æ£€ç´¢ç²¾åº¦ã€‚è¿™ä¸€æ–¹å¼å°±å¦‚åŒå›¾ä¹¦é¦†ä¸ä»…æä¾›ä¹¦åæ£€ç´¢ï¼Œè¿˜é…å¤‡åˆ†ç±»ç¼–å·ç³»ç»Ÿï¼ŒåŒç®¡é½ä¸‹è®©æ£€ç´¢ç»“æœæ›´åŠ ç²¾å‡†ã€‚\n",
    "\n",
    "æ ‡ç­¾æå–ä¸»è¦åº”ç”¨äºä»¥ä¸‹ä¸¤ä¸ªå…³é”®åœºæ™¯ï¼š\n",
    "\n",
    "- å»ºç«‹ç´¢å¼•æ—¶ ï¼šä»æ–‡æ¡£åˆ‡ç‰‡ä¸­æå–ç»“æ„åŒ–æ ‡ç­¾ï¼Œå°†å…¶ä¸æ–‡æ¡£åˆ‡ç‰‡ä¸€åŒå­˜å‚¨ã€‚\n",
    "- æ£€ç´¢æ—¶ ï¼šä»ç”¨æˆ·é—®é¢˜ä¸­æå–å¯¹åº”çš„æ ‡ç­¾è¿›è¡Œåˆæ­¥è¿‡æ»¤ï¼Œå†ç»“åˆå‘é‡æ£€ç´¢è·å–æœ€ç»ˆç»“æœã€‚\n",
    "ä¸‹é¢é€šè¿‡ä¸¤ä¸ªç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•ä»ä¸åŒç±»å‹çš„æ–‡æœ¬ä¸­æå–æ ‡ç­¾ï¼ŒåŒæ—¶ç»™å‡ºä¼˜åŒ–åçš„ä»£ç ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a33d3f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå¸‚æ—…æ¸¸ä»‹ç»æ ‡ç­¾æå–ç»“æœï¼š\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 17:08:53,641 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"key\": \"æ—…æ¸¸ç›®çš„åœ°\", \"value\": \"ä¸Šæµ·\"}, {\"key\": \"æ™¯ç‚¹åç§°\", \"value\": \"å¤–æ»©\"}, {\"key\": \"æœ€ä½³æ—…æ¸¸æ—¶é—´\", \"value\": \"æ˜¥ç§‹ä¸¤å­£\"}, {\"key\": \"æ—…æ¸¸æ´»åŠ¨\", \"value\": \"æ¬£èµç¾ä¸½çš„å¤œæ™¯ç¯å…‰ç§€\"}]\n",
      "\n",
      "ä¸»é¢˜å…¬å›­æ¸¸ç©æŒ‡å—æ ‡ç­¾æå–ç»“æœï¼š\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 17:08:55,725 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"key\": \"æ—…æ¸¸ç›®çš„åœ°\", \"value\": \"å¹¿å·\"}, {\"key\": \"æ™¯ç‚¹åç§°\", \"value\": \"é•¿éš†æ¬¢ä¹ä¸–ç•Œ\"}, {\"key\": \"æ—…æ¸¸æ´»åŠ¨\", \"value\": \"äº²å­æ¸¸ç©\"}, {\"key\": \"æ—…æ¸¸èŠ±è´¹\", \"value\": \"çº¦300å…ƒ\"}]\n",
      "\n",
      "è‡ªç„¶é£å…‰æ—…æ¸¸æ¨èæ ‡ç­¾æå–ç»“æœï¼š\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 17:08:57,432 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"key\": \"æ—…æ¸¸ç›®çš„åœ°\", \"value\": \"ä¹å¯¨æ²Ÿ\"}, {\"key\": \"æ™¯ç‚¹åç§°\", \"value\": \"ä¹å¯¨æ²Ÿ\"}, {\"key\": \"æ—…æ¸¸æ´»åŠ¨\", \"value\": \"å¾’æ­¥æ¬£èµé£æ™¯\"}, {\"key\": \"æ—…æ¸¸èŠ±è´¹\", \"value\": \"220å…ƒ\"}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"DASHSCOPE_API_KEY\"), base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\")\n",
    "\n",
    "system_message = \"\"\"ä½ æ˜¯ä¸€ä¸ªæ ‡ç­¾æå–ä¸“å®¶ã€‚è¯·ä»æ—…æ¸¸ç›¸å…³æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼Œå¹¶æŒ‰è¦æ±‚è¾“å‡ºæ ‡ç­¾ã€‚ \n",
    "\n",
    "---\n",
    "\n",
    "ã€æ”¯æŒçš„æ ‡ç­¾ç±»å‹ã€‘\n",
    "\n",
    "- æ—…æ¸¸ç›®çš„åœ°\n",
    "- æ™¯ç‚¹åç§°\n",
    "- æ—…æ¸¸æ´»åŠ¨\n",
    "- æœ€ä½³æ—…æ¸¸æ—¶é—´\n",
    "- æ—…æ¸¸èŠ±è´¹\n",
    "\n",
    "---\n",
    "\n",
    "ã€è¾“å‡ºè¦æ±‚ã€‘\n",
    "\n",
    "1. è¯·ç”¨ JSON æ ¼å¼è¾“å‡ºï¼Œå¦‚ï¼š[{\"key\": \"æ—…æ¸¸ç›®çš„åœ°\", \"value\": \"åŒ—äº¬\"}]\n",
    "2. å¦‚æœæŸç±»æ ‡ç­¾æœªè¯†åˆ«åˆ°ï¼Œåˆ™ä¸è¾“å‡ºè¯¥ç±»\n",
    "\n",
    "---\n",
    "\n",
    "å¾…åˆ†ææ–‡æœ¬å¦‚ä¸‹ï¼š\n",
    "\"\"\"\n",
    "\n",
    "def extract_tags(text):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"qwen-turbo\",\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': system_message},\n",
    "            {'role': 'user', 'content': text}\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# ç¤ºä¾‹1ï¼šåŸå¸‚æ—…æ¸¸ä»‹ç»\n",
    "city_text = \"\"\"ä¸Šæµ·æ˜¯ä¸€ä¸ªå……æ»¡é­…åŠ›çš„æ—…æ¸¸åŸå¸‚ï¼Œå¤–æ»©æ˜¯å¿…å»çš„æ™¯ç‚¹ä¹‹ä¸€ï¼Œæ™šä¸Šå¯ä»¥æ¬£èµç¾ä¸½çš„å¤œæ™¯ç¯å…‰ç§€ã€‚æœ€ä½³æ—…æ¸¸æ—¶é—´æ˜¯æ˜¥ç§‹ä¸¤å­£ï¼Œå‚è§‚å¤–æ»©æ— éœ€é—¨ç¥¨ã€‚\"\"\"\n",
    "print(\"åŸå¸‚æ—…æ¸¸ä»‹ç»æ ‡ç­¾æå–ç»“æœï¼š\")\n",
    "print(extract_tags(city_text))\n",
    "\n",
    "# ç¤ºä¾‹2ï¼šä¸»é¢˜å…¬å›­æ¸¸ç©æŒ‡å—\n",
    "park_text = \"\"\"å¹¿å·é•¿éš†æ¬¢ä¹ä¸–ç•Œæ˜¯äº²å­æ¸¸ç©çš„å¥½å»å¤„ï¼Œè¿™é‡Œæœ‰å„ç§åˆºæ¿€çš„æ¸¸ä¹è®¾æ–½ï¼Œå¦‚å‚ç›´è¿‡å±±è½¦ã€‚å‘¨æœ«å’ŒèŠ‚å‡æ—¥æ˜¯æ¸¸ç©é«˜å³°ï¼Œå…¨ç¥¨ä»·æ ¼çº¦300å…ƒã€‚\"\"\"\n",
    "print(\"\\nä¸»é¢˜å…¬å›­æ¸¸ç©æŒ‡å—æ ‡ç­¾æå–ç»“æœï¼š\")\n",
    "print(extract_tags(park_text))\n",
    "\n",
    "# ç¤ºä¾‹3ï¼šè‡ªç„¶é£å…‰æ—…æ¸¸æ¨è\n",
    "nature_text = \"\"\"ä¹å¯¨æ²Ÿçš„ç§‹å¤©æ˜¯æœ€ç¾çš„ï¼Œäº”å½©æ–‘æ–“çš„æ¹–æ°´å’Œå±±æ—æ„æˆäº†ç»ç¾çš„ç”»å·ã€‚åœ¨è¿™é‡Œå¯ä»¥å¾’æ­¥æ¬£èµé£æ™¯ï¼Œé—¨ç¥¨ä»·æ ¼æ—ºå­£220å…ƒã€‚\"\"\"\n",
    "print(\"\\nè‡ªç„¶é£å…‰æ—…æ¸¸æ¨èæ ‡ç­¾æå–ç»“æœï¼š\")\n",
    "print(extract_tags(nature_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095baa1d",
   "metadata": {},
   "source": [
    "### ç»“è®º\n",
    "1. åŠŸèƒ½å®ç° ï¼šä»£ç æˆåŠŸä»æ—…æ¸¸ç›¸å…³æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–æ ‡ç­¾ï¼Œè¾“å‡ºç¬¦åˆ JSON æ ¼å¼è¦æ±‚ï¼Œæ¶µç›–äº†æ—…æ¸¸ç›®çš„åœ°ã€æ™¯ç‚¹åç§°ã€æ—…æ¸¸æ´»åŠ¨ã€æœ€ä½³æ—…æ¸¸æ—¶é—´å’Œæ—…æ¸¸èŠ±è´¹ç­‰ä¿¡æ¯ã€‚\n",
    "2. è¾“å‡ºç»“æœ ï¼šé’ˆå¯¹ä¸åŒç±»å‹çš„æ—…æ¸¸æ–‡æœ¬ï¼ˆåŸå¸‚æ—…æ¸¸ä»‹ç»ã€ä¸»é¢˜å…¬å›­æ¸¸ç©æŒ‡å—ã€è‡ªç„¶é£å…‰æ—…æ¸¸æ¨èï¼‰ï¼Œéƒ½èƒ½å‡†ç¡®æå–å‡ºå¯¹åº”æ ‡ç­¾ï¼ŒéªŒè¯äº†ä»£ç çš„æœ‰æ•ˆæ€§ã€‚\n",
    "3. æ‰©å±•æ€§ ï¼šç³»ç»Ÿæç¤ºå’Œæ”¯æŒçš„æ ‡ç­¾ç±»å‹å¯çµæ´»è°ƒæ•´ï¼Œæ–¹ä¾¿æ ¹æ®ä¸åŒéœ€æ±‚æ‰©å±•æ ‡ç­¾ç±»å‹å’Œå¤„ç†æ›´å¤šç§ç±»çš„æ—…æ¸¸æ–‡æœ¬ã€‚\n",
    "\n",
    "### æ ‡ç­¾çš„ä»·å€¼\n",
    "1. ç²¾å‡†æ£€ç´¢ ï¼šä¼ä¸šå¯åˆ©ç”¨æå–çš„æ ‡ç­¾å»ºç«‹æ–‡æ¡£ç´¢å¼•ï¼Œåœ¨ç”¨æˆ·æ£€ç´¢æ—¶ï¼Œå…ˆé€šè¿‡æ ‡ç­¾è¿‡æ»¤å‡ºç›¸å…³æ–‡æ¡£åˆ‡ç‰‡ï¼Œå†ç»“åˆå‘é‡ç›¸ä¼¼åº¦æ£€ç´¢ï¼Œèƒ½å¤§å¹…æå‡æ£€ç´¢çš„å‡†ç¡®æ€§å’Œæ•ˆç‡ï¼Œå¿«é€Ÿå®šä½ç”¨æˆ·æ‰€éœ€ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·æœç´¢â€œä¸Šæµ·æ™¯ç‚¹â€ï¼Œå¯å¿«é€Ÿç­›é€‰å‡ºåŒ…å«â€œä¸Šæµ·â€ç›¸å…³æ ‡ç­¾çš„æ–‡æ¡£ã€‚\n",
    "\n",
    "2. ä¸ªæ€§åŒ–æ¨è ï¼šåŸºäºæ ‡ç­¾ä¿¡æ¯ï¼Œä¼ä¸šå¯ä»¥ä¸ºç”¨æˆ·æä¾›ä¸ªæ€§åŒ–çš„æ—…æ¸¸æ¨èã€‚ä¾‹å¦‚ï¼Œæ ¹æ®ç”¨æˆ·çš„æ—…æ¸¸åå¥½å’Œå†å²è®°å½•ï¼Œç»“åˆæ™¯ç‚¹çš„æ ‡ç­¾ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æ¨èç¬¦åˆå…¶éœ€æ±‚çš„æ—…æ¸¸ç›®çš„åœ°å’Œæ´»åŠ¨ã€‚\n",
    "\n",
    "3. æ•°æ®åˆ†æ ï¼šæ ‡ç­¾æ•°æ®å¯ä»¥ç”¨äºä¼ä¸šè¿›è¡Œæ•°æ®åˆ†æï¼Œäº†è§£çƒ­é—¨æ—…æ¸¸ç›®çš„åœ°ã€æ—…æ¸¸æ´»åŠ¨å’Œç”¨æˆ·æ¶ˆè´¹ä¹ æƒ¯ç­‰ã€‚ä¼ä¸šå¯ä»¥æ ¹æ®è¿™äº›æ•°æ®ä¼˜åŒ–æ—…æ¸¸äº§å“å’ŒæœåŠ¡ï¼Œåˆ¶å®šæ›´æœ‰é’ˆå¯¹æ€§çš„è¥é”€ç­–ç•¥ã€‚\n",
    "\n",
    "4. å†…å®¹ç®¡ç† ï¼šæ ‡ç­¾èƒ½å¤Ÿå¸®åŠ©ä¼ä¸šå¯¹å¤§é‡çš„æ—…æ¸¸æ–‡æ¡£è¿›è¡Œåˆ†ç±»å’Œç®¡ç†ï¼Œæé«˜æ–‡æ¡£çš„ç»„ç»‡æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚ä¼ä¸šå¯ä»¥æ ¹æ®æ ‡ç­¾å¯¹æ–‡æ¡£è¿›è¡Œåˆ†ç»„ã€æ’åºå’Œç­›é€‰ï¼Œæ–¹ä¾¿å‘˜å·¥æŸ¥æ‰¾å’Œä½¿ç”¨ç›¸å…³ä¿¡æ¯ã€‚\n",
    "\n",
    "5. æå‡ç”¨æˆ·ä½“éªŒ ï¼šç²¾å‡†çš„æ£€ç´¢ç»“æœå’Œä¸ªæ€§åŒ–çš„æ¨èèƒ½å¤Ÿæå‡ç”¨æˆ·ä½“éªŒï¼Œå¢åŠ ç”¨æˆ·å¯¹ä¼ä¸šçš„æ»¡æ„åº¦å’Œå¿ è¯šåº¦ï¼Œä»è€Œä¿ƒè¿›ä¸šåŠ¡çš„å¢é•¿ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
