{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 政策文档检索总是漏关键点？构建高精度垂直领域检索系统\n",
    "\n",
    "政策文档检索总是遗漏关键信息？这在企业实际业务中是个普遍且令人头疼的问题。  \n",
    "面对海量复杂、层级严密的政策、法规或财务报告时，传统的检索增强生成（RAG）系统往往力不从心。\n",
    "\n",
    "本文将深入探讨如何结合 **HiRAG（Hierarchical RAG，分层知识检索增强生成）** 的理论优势与 **LlamaIndex** 的实操能力，构建一个高精度的垂直领域检索系统，彻底解决 RAG 在商业化落地中遇到的“断章取义”和“大海捞针”困境。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 传统 RAG 的局限与商业化挑战\n",
    "\n",
    "在处理复杂、结构化程度高的政策文档、法规或财务报告时，传统的 **检索增强生成（RAG）** 系统暴露出一系列关键问题，严重制约其在企业级场景中的落地应用。\n",
    "\n",
    "### 核心挑战\n",
    "\n",
    "1. **上下文碎片化（Context Fragmentation）**  \n",
    "   传统 RAG 多基于段落或句子级别的向量化检索，导致原始文档的上下文信息被割裂。这种碎片化使得模型难以理解完整语义，尤其在涉及长距离依赖或逻辑推理的场景中表现不佳。\n",
    "\n",
    "2. **语义关联缺失（Lack of Semantic Connection）**  \n",
    "   文档中的概念往往存在多层级、多维度的语义关系（如政策中的条款、子条款、附件、实施细则等）。传统 RAG 缺乏对这些语义结构的建模能力，导致检索结果“只见树木，不见森林”。\n",
    "\n",
    "3. **局部与全局知识断层（Local-Global Knowledge Disconnection）**  \n",
    "   检索过程通常只关注与查询最相似的局部文本片段，忽略了文档整体结构和上下文逻辑。这种断层容易引发“断章取义”，在政策解读、合规审查等高风险场景中可能带来严重后果。\n",
    "\n",
    "因此，构建一个能理解文档深层结构、融合多粒度知识的RAG系统至关重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HiRAG：构建“智能目录”与“专家网络”\n",
    "\n",
    "HiRAG 通过引入**层次化知识结构**，大幅提升复杂政策文档的检索精度，解决了传统 RAG 的三大痛点：上下文碎片、语义断层、局部与全局知识割裂。\n",
    "\n",
    "![HiRAG 数据集转换](./15/image.png)\n",
    "\n",
    "https://github.com/hhy-huang/HiRAG\n",
    "\n",
    "\n",
    "### 1. 层次化知识索引（HiIndex）——构建“智能目录”\n",
    "\n",
    "- 从文档中提取实体和关系，构建**多层级知识图谱**。\n",
    "- 通过语义聚类和摘要生成，打通底层实体间的语义孤岛。\n",
    "- 实现文档从“无结构文本”到“结构化目录”的转变。\n",
    "\n",
    "\n",
    "\n",
    "### 2. 层次化知识检索（HiRetrieval）——三级检索机制\n",
    "\n",
    "- **局部检索**：定位具体实体（如某政策条款）。\n",
    "- **全局检索**：识别宏观背景（如所属监管框架）。\n",
    "- **桥接检索**：建立局部与全局之间的语义路径（如“云计算 → AWS → 亚马逊”）。\n",
    "\n",
    "> 通过三级检索机制，LLM 可同时获取细节与背景，避免“断章取义”。\n",
    "\n",
    "\n",
    "\n",
    "### 3. 融合知识图谱——升级为“专家网络”\n",
    "\n",
    "- **HiRAG**：专注单文档内部的精准检索。\n",
    "- **知识图谱**：连接多文档间的逻辑关系（如替代、依赖、引用）。\n",
    "\n",
    "两者结合，打造一个既懂结构又懂逻辑的“专家级”检索系统，显著提升垂直领域的信息召回与理解能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HiRAG 实操指南：LlamaIndex 框架下的实现\n",
    "\n",
    "在主流 RAG 框架中，**LlamaIndex** 是实现 **HiRAG** 最直接、最高效的工具，得益于其内置的 **AutoMergingRetriever** 组件，天然支持分层检索结构。\n",
    "\n",
    "### 三步实现 HiRAG\n",
    "\n",
    "无论使用哪种框架，HiRAG 的核心流程都包含以下三步：\n",
    "\n",
    "### 1. 分层解析（Hierarchical Parsing）\n",
    "\n",
    "- **目标**：保留文档天然结构（如章节、条款、段落）。\n",
    "- **方法**：使用 `HierarchicalNodeParser` 将文档拆分为 **父节点（Parent Nodes）** 和 **子节点（Child Nodes）**。\n",
    "- **优势**：避免传统切块方式破坏语义层级，提升上下文完整性。\n",
    "- **最佳实践**：\n",
    "  - 根据标题、编号、缩进等结构划分层级。\n",
    "  - 设置不同层级的 `chunk_sizes`，例如章节为父节点，具体条款为子节点。\n",
    "\n",
    "### 2. 两阶段检索（Two-Step Retrieval）\n",
    "\n",
    "- **步骤一：子节点检索**  \n",
    "  在子节点中进行向量搜索，找到最相关的**细节内容**。\n",
    "  \n",
    "- **步骤二：关联父节点**  \n",
    "  自动关联相关父节点，补充上下文背景，避免“断章取义”。\n",
    "\n",
    "- **实现工具**：`AutoMergingRetriever` 自动合并子节点及其父节点，构建完整上下文。\n",
    "\n",
    "### 3. 增强生成（Augmented Generation）\n",
    "\n",
    "- **输入内容**：结合“精确的子节点” + “完整的父节点”作为上下文。\n",
    "- **输出效果**：LLM 生成内容更准确、完整、逻辑清晰。\n",
    "- **提示技巧**：设计提示词时，引导模型利用多层级信息进行回答。\n",
    "\n",
    "### LlamaIndex 实操简要步骤\n",
    "\n",
    "### 4. **安装依赖**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index==0.12.44\n",
    "%pip install pydantic\n",
    "%pip install llama-index-llms-openai \n",
    "%pip install llama-index-embeddings-huggingface \n",
    "%pip install torch transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 准备文档并进行分层解析\n",
    "这是最关键的一步。我们使用 HierarchicalNodeParser。\n",
    "为了模拟政策文档，我们创建一个包含多章节的字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parent nodes (chapters): 3\n",
      "Total child nodes (articles): 7\n",
      "\n",
      "Example parent node:\n",
      "  parent_id=chapter_1, title=Chapter 1: General Provisions and Objectives, text_len=1109\n",
      "  parent preview: # Chapter 1: General Provisions and Objectives  ## Article 1: Policy Background ...\n",
      "  #children=2\n",
      "    child 1: child_id=chapter_1_article_1, title=Article 1: Policy Background and Objectives, text_len=630\n",
      "      child preview: ## Article 1: Policy Background and Objectives  This policy aims to thoroughly i...\n",
      "    child 2: child_id=chapter_1_article_2, title=Article 2: Scope of Application, text_len=429\n",
      "      child preview: ## Article 2: Scope of Application  This policy applies to all types of enterpri...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List, Dict, Any\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Same policy text (English) wrapped as LangChain Documents\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "policy_text = \"\"\"\n",
    "# Chapter 1: General Provisions and Objectives\n",
    "\n",
    "## Article 1: Policy Background and Objectives\n",
    "\n",
    "This policy aims to thoroughly implement the national innovation-driven development strategy\n",
    "and accelerate the construction of a globally influential science and technology innovation center.\n",
    "Through a comprehensive set of fiscal, taxation, talent, and financial support measures,\n",
    "the policy seeks to stimulate enterprise innovation, promote industrial structure optimization\n",
    "and upgrading, and enhance the core competitiveness of the regional economy.\n",
    "Special emphasis is placed on supporting **high-tech enterprises**, helping them achieve\n",
    "breakthroughs in key core technologies.\n",
    "\n",
    "## Article 2: Scope of Application\n",
    "\n",
    "This policy applies to all types of enterprises that are legally registered within the administrative\n",
    "region of City XX and possess independent legal entity status, as well as qualified high-level talent\n",
    "working in City XX.\n",
    "Priority support is given to enterprises operating in strategic emerging industries such as\n",
    "new energy, artificial intelligence, biomedicine, and advanced manufacturing.\n",
    "\n",
    "# Chapter 2: High-Tech Enterprise Certification and Support\n",
    "\n",
    "## Article 3: Certification Standards and Procedures\n",
    "\n",
    "Enterprises that meet the requirements of the national high-tech enterprise certification\n",
    "management regulations may apply for certification in accordance with the prescribed procedures.\n",
    "Certification criteria include, but are not limited to: ownership of core independent intellectual\n",
    "property rights, products or services falling within the “National Key Supported High-Tech Fields,”\n",
    "the proportion of R&D expenditure, and the proportion of revenue from high-tech products or services.\n",
    "The centralized application period runs from July to September each year. Enterprises must submit\n",
    "application materials through the official platform of the Municipal Science and Technology Bureau.\n",
    "\n",
    "## Article 4: Financial Subsidies and Incentives\n",
    "\n",
    "Newly certified national high-tech enterprises will receive a one-time reward of **RMB 300,000**.\n",
    "Enterprises passing national high-tech certification for the first time may additionally receive\n",
    "an R&D investment subsidy of up to **RMB 500,000**, with the specific amount determined based on\n",
    "R&D intensity and output performance.\n",
    "    * **4.1 R&D Investment Subsidy Details**: The subsidy is calculated as 10% of the enterprise’s\n",
    "      R&D expenditure in the previous year, capped at RMB 500,000. It is primarily used for\n",
    "      purchasing R&D equipment, paying R&D personnel salaries, and outsourced R&D activities.\n",
    "    * **4.2 Disbursement of Incentive Funds**: Incentive funds will be disbursed to the enterprise’s\n",
    "      designated account within 30 working days after the certification results are announced.\n",
    "    * **4.3 Priority Loan Support**: Certified high-tech enterprises may access low-interest loans\n",
    "      provided by partner banks, with a maximum credit line of RMB 50 million.\n",
    "\n",
    "## Article 5: Talent Recruitment and Development Support\n",
    "\n",
    "High-tech enterprises are encouraged to recruit high-level talent. Eligible individuals may receive\n",
    "benefits such as housing subsidies and priority access to education for their children.\n",
    "Special funds are established to encourage enterprises to conduct technical talent training and\n",
    "enhance employees’ professional skills.\n",
    "\n",
    "# Chapter 3: Supervision, Management, and Violations\n",
    "\n",
    "## Article 6: Supervision and Inspection\n",
    "\n",
    "The Municipal Science and Technology Bureau, the Finance Bureau, and other relevant departments will\n",
    "conduct regular inspections of enterprises receiving policy support to ensure compliance and\n",
    "maximize policy effectiveness.\n",
    "Enterprises must cooperate with inspections and provide relevant materials truthfully.\n",
    "\n",
    "## Article 7: Handling of Violations\n",
    "\n",
    "Enterprises that falsely claim, fraudulently obtain, withhold, or misappropriate fiscal funds will\n",
    "be required to return all subsidies in accordance with the law and will be disqualified from applying\n",
    "for any municipal policy support for the next three years.\n",
    "Serious cases will be referred to judicial authorities.\n",
    "\"\"\"\n",
    "\n",
    "docs = [Document(page_content=policy_text, metadata={\"source\": \"policy_demo\"})]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) \"Hierarchical parsing\" in LangChain (simple, explicit, no special parser)\n",
    "#    We create:\n",
    "#      - Parent nodes: Chapters\n",
    "#      - Child nodes: Articles within each chapter\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "CHAPTER_RE = re.compile(r\"(?m)^#\\s+(.*)$\")\n",
    "ARTICLE_RE = re.compile(r\"(?m)^##\\s+(.*)$\")\n",
    "\n",
    "def split_by_pattern(text: str, header_re: re.Pattern) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Split text into sections by a markdown header regex.\n",
    "    Returns a list of {\"title\": str, \"content\": str}.\n",
    "    \"\"\"\n",
    "    matches = list(header_re.finditer(text))\n",
    "    if not matches:\n",
    "        return [{\"title\": \"ROOT\", \"content\": text}]\n",
    "\n",
    "    sections = []\n",
    "    for i, m in enumerate(matches):\n",
    "        start = m.start()\n",
    "        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
    "        title = m.group(1).strip()\n",
    "        content = text[start:end].strip()\n",
    "        sections.append({\"title\": title, \"content\": content})\n",
    "    return sections\n",
    "\n",
    "parent_docs: List[Document] = []\n",
    "child_docs: List[Document] = []\n",
    "\n",
    "# Build parent (chapter) docs\n",
    "chapters = split_by_pattern(policy_text, CHAPTER_RE)\n",
    "\n",
    "for ci, ch in enumerate(chapters, 1):\n",
    "    parent_id = f\"chapter_{ci}\"\n",
    "    parent_doc = Document(\n",
    "        page_content=ch[\"content\"],\n",
    "        metadata={\n",
    "            \"node_type\": \"parent\",\n",
    "            \"parent_id\": parent_id,\n",
    "            \"title\": ch[\"title\"],\n",
    "            \"level\": 1,\n",
    "        },\n",
    "    )\n",
    "    parent_docs.append(parent_doc)\n",
    "\n",
    "    # Build child (article) docs within each chapter\n",
    "    articles = split_by_pattern(ch[\"content\"], ARTICLE_RE)\n",
    "\n",
    "    # If no \"##\" headers, treat the whole chapter as one child\n",
    "    if len(articles) == 1 and articles[0][\"title\"] == \"ROOT\":\n",
    "        articles = [{\"title\": \"Chapter body\", \"content\": ch[\"content\"]}]\n",
    "\n",
    "    for ai, art in enumerate(articles, 1):\n",
    "        child_id = f\"{parent_id}_article_{ai}\"\n",
    "        child_doc = Document(\n",
    "            page_content=art[\"content\"],\n",
    "            metadata={\n",
    "                \"node_type\": \"child\",\n",
    "                \"child_id\": child_id,\n",
    "                \"parent_id\": parent_id,   # link child -> parent\n",
    "                \"title\": art[\"title\"],\n",
    "                \"level\": 2,\n",
    "            },\n",
    "        )\n",
    "        child_docs.append(child_doc)\n",
    "\n",
    "print(f\"Total parent nodes (chapters): {len(parent_docs)}\")\n",
    "print(f\"Total child nodes (articles): {len(child_docs)}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Print an example parent + its child nodes\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Build parent_id -> list(child_docs)\n",
    "children_by_parent: Dict[str, List[Document]] = {}\n",
    "for cd in child_docs:\n",
    "    children_by_parent.setdefault(cd.metadata[\"parent_id\"], []).append(cd)\n",
    "\n",
    "# Print the first parent with children\n",
    "# Print the first parent with children\n",
    "for pd in parent_docs:\n",
    "    pid = pd.metadata[\"parent_id\"]\n",
    "    kids = children_by_parent.get(pid, [])\n",
    "    if kids:\n",
    "        parent_preview = pd.page_content[:80].replace(\"\\n\", \" \")\n",
    "\n",
    "        print(\"\\nExample parent node:\")\n",
    "        print(f\"  parent_id={pid}, title={pd.metadata.get('title')}, text_len={len(pd.page_content)}\")\n",
    "        print(f\"  parent preview: {parent_preview}...\")\n",
    "\n",
    "        print(f\"  #children={len(kids)}\")\n",
    "        for i, cd in enumerate(kids[:5], 1):\n",
    "            child_preview = cd.page_content[:80].replace(\"\\n\", \" \")\n",
    "            print(f\"    child {i}: child_id={cd.metadata['child_id']}, title={cd.metadata.get('title')}, text_len={len(cd.page_content)}\")\n",
    "            print(f\"      child preview: {child_preview}...\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码解释：\n",
    "- HierarchicalNodeParser 是 LlamaIndex 提供的核心组件，用于实现分层文档解析。\n",
    "- chunk_sizes 参数是其关键，它定义了从上到下不同层级的块大小。例如，[2048, 512, 128] 意味着它会先尝试将文档切分成最大2048个Token的块（作为“父节点”），然后将这些父节点再细分成最大512个Token的块（“中间节点”），最后将中间节点细分成最大128个Token的块（“叶子节点”）。这种层级拆分保留了文档的语义上下文。\n",
    "- get_nodes_from_documents(docs) 执行实际的解析，生成一个包含所有层级节点（及其父子关系）的列表。\n",
    "- get_leaf_nodes(nodes) 从所有节点中筛选出最底层的叶子节点。在后续的向量索引构建中，我们通常只对这些最细粒度的叶子节点进行嵌入和索引，因为它们包含了最直接的答案片段。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 构建索引和存储\n",
    "我们需要将所有节点（父和子）的信息都存储起来。AutoMergingRetriever 会利用这个存储上下文中的父子关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vector index built. Leaf/child nodes have been embedded and stored in Weaviate.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Cell 1: Vector index ONLY on leaf/child nodes (LangChain + Weaviate)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_weaviate import WeaviateVectorStore\n",
    "import weaviate\n",
    "from weaviate.connect import ConnectionParams\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "client = weaviate.WeaviateClient(\n",
    "    connection_params=ConnectionParams.from_params(\n",
    "        http_host=\"127.0.0.1\",\n",
    "        http_port=8080,\n",
    "        http_secure=False,\n",
    "        grpc_host=\"127.0.0.1\",\n",
    "        grpc_port=50051,\n",
    "        grpc_secure=False,\n",
    "    )\n",
    ")\n",
    "client.connect()\n",
    "\n",
    "# IMPORTANT: index only child_docs (leaf nodes)\n",
    "vectorstore = WeaviateVectorStore.from_documents(\n",
    "    documents=child_docs,          # <-- leaf nodes only\n",
    "    embedding=embeddings,\n",
    "    client=client,\n",
    "    index_name=\"PolicyLeafNodes\",\n",
    "    text_key=\"text\",\n",
    ")\n",
    "\n",
    "print(\"✅ Vector index built. Leaf/child nodes have been embedded and stored in Weaviate.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码解释：\n",
    "- ServiceContext 是 LlamaIndex 的一个核心概念，它封装了 RAG 管道中的核心组件，包括 LLM、Embedding 模型和节点解析器。\n",
    "- StorageContext 负责管理数据的存储。storage_context.docstore.add_documents(nodes) 这一步至关重要，它将所有层级的节点（包括父节点、中间节点和叶子节点）都存储在 LlamaIndex 的文档存储中。AutoMergingRetriever 后续就是通过这个 docstore 来查询节点的父子关系。\n",
    "- VectorStoreIndex(leaf_nodes, ...)：这里我们只用 leaf_nodes 来初始化 VectorStoreIndex。这意味着只有最细粒度的叶子节点会被转换为向量并存储在向量数据库中。这是因为我们通常认为用户查询的语义与最具体的文本片段（叶子节点）最匹配。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 配置并使用 AutoMergingRetriever\n",
    "现在，我们用 AutoMergingRetriever 替换掉普通的检索器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def auto_merge_retrieve(\n",
    "    query: str,\n",
    "    vectorstore,\n",
    "    parent_map: Dict[str, Document],\n",
    "    top_k: int = 5,\n",
    "    verbose: bool = True,\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    LangChain-style equivalent of LlamaIndex AutoMergingRetriever.\n",
    "\n",
    "    1) Retrieve top_k leaf nodes from the vectorstore\n",
    "    2) Group by parent_id\n",
    "    3) If multiple children share a parent, return merged parent context\n",
    "       (parent + concatenated child snippets)\n",
    "    4) Otherwise return (parent + single child) merged context\n",
    "    \"\"\"\n",
    "    base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n",
    "    leaf_hits: List[Document] = base_retriever.invoke(query)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[auto_merge] query='{query}'\")\n",
    "        print(f\"[auto_merge] retrieved leaf_hits={len(leaf_hits)}\")\n",
    "\n",
    "    # Group retrieved leaf nodes by parent_id\n",
    "    grouped: Dict[str, List[Document]] = {}\n",
    "    for d in leaf_hits:\n",
    "        pid = (d.metadata or {}).get(\"parent_id\")\n",
    "        grouped.setdefault(pid, []).append(d)\n",
    "\n",
    "    merged_results: List[Document] = []\n",
    "\n",
    "    for pid, children in grouped.items():\n",
    "        parent = parent_map.get(pid)\n",
    "\n",
    "        # Build merged text\n",
    "        child_block = \"\\n\\n\".join(\n",
    "            f\"- {c.page_content.strip()}\" for c in children if (c.page_content or \"\").strip()\n",
    "        ).strip()\n",
    "\n",
    "        if parent:\n",
    "            merged_text = (\n",
    "                f\"[PARENT CONTEXT]\\n{parent.page_content.strip()}\\n\\n\"\n",
    "                f\"[CHILD EVIDENCE - {len(children)} hit(s)]\\n{child_block}\"\n",
    "            )\n",
    "        else:\n",
    "            # If parent not found, fall back to only child evidence\n",
    "            merged_text = f\"[CHILD EVIDENCE - {len(children)} hit(s)]\\n{child_block}\"\n",
    "\n",
    "        if verbose:\n",
    "            title = parent.metadata.get(\"title\") if parent and parent.metadata else None\n",
    "            print(f\"[auto_merge] parent_id={pid} title={title} children={len(children)}\")\n",
    "\n",
    "        merged_results.append(\n",
    "            Document(\n",
    "                page_content=merged_text,\n",
    "                metadata={\n",
    "                    \"parent_id\": pid,\n",
    "                    \"num_children_merged\": len(children),\n",
    "                    \"merged_parent_found\": bool(parent),\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return merged_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auto_merge] query='What is the one-time reward for newly certified high-tech enterprises?'\n",
      "[auto_merge] retrieved leaf_hits=5\n",
      "[auto_merge] parent_id=chapter_2 title=Chapter 2: High-Tech Enterprise Certification and Support children=3\n",
      "[auto_merge] parent_id=chapter_1 title=Chapter 1: General Provisions and Objectives children=2\n",
      "\n",
      "--- Merged results ---\n",
      "--------------------------------------------------------------------------------\n",
      "[1] parent_id=chapter_2 children=3\n",
      "[PARENT CONTEXT]\n",
      "# Chapter 2: High-Tech Enterprise Certification and Support\n",
      "\n",
      "## Article 3: Certification Standards and Procedures\n",
      "\n",
      "Enterprises that meet the requirements of the national high-tech enterprise certification\n",
      "management regulations may apply for certification in accordance with the prescribed procedures.\n",
      "Certification criteria include, but are not limited to: ownership of core independent intellectual\n",
      "property rights, products or services falling within the “National Key Supported High-Tech Fields,”\n",
      "the proportion of R&D expenditure, and the proportion of revenue from high-tech pr...\n",
      "--------------------------------------------------------------------------------\n",
      "[2] parent_id=chapter_1 children=2\n",
      "[PARENT CONTEXT]\n",
      "# Chapter 1: General Provisions and Objectives\n",
      "\n",
      "## Article 1: Policy Background and Objectives\n",
      "\n",
      "This policy aims to thoroughly implement the national innovation-driven development strategy\n",
      "and accelerate the construction of a globally influential science and technology innovation center.\n",
      "Through a comprehensive set of fiscal, taxation, talent, and financial support measures,\n",
      "the policy seeks to stimulate enterprise innovation, promote industrial structure optimization\n",
      "and upgrading, and enhance the core competitiveness of the regional economy.\n",
      "Special emphasis is placed on sup...\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Build parent_map (LangChain replacement for LlamaIndex docstore)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# parent_docs must already exist from your hierarchical parsing step\n",
    "# Each parent Document must have metadata[\"parent_id\"]\n",
    "\n",
    "parent_map = {\n",
    "    p.metadata[\"parent_id\"]: p\n",
    "    for p in parent_docs\n",
    "}\n",
    "\n",
    "query = \"What is the one-time reward for newly certified high-tech enterprises?\"\n",
    "\n",
    "merged_docs = auto_merge_retrieve(\n",
    "    query=query,\n",
    "    vectorstore=vectorstore,\n",
    "    parent_map=parent_map,\n",
    "    top_k=5,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\n--- Merged results ---\")\n",
    "for i, d in enumerate(merged_docs, 1):\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"[{i}] parent_id={d.metadata.get('parent_id')} children={d.metadata.get('num_children_merged')}\")\n",
    "    print(d.page_content[:600] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CrossEncoder reranker + LLM ready\n"
     ]
    }
   ],
   "source": [
    "# 5. 构建查询引擎# RetrieverQueryEngine 将检索器与可选的后处理器结合起来，然后将检索结果传递给LLM进行答案生成。\n",
    "# 最佳实践：添加重排器 (Re-ranker)。\n",
    "# 即使向量检索找到了相似的节点，这些节点的相关性可能还需要进一步排序。\n",
    "# SentenceTransformerRerank 使用一个独立的模型（如 BAAI/bge-reranker-base）对检索到的节点进行重新打分，\n",
    "# 从而选择出最相关的 top_n 个节点。这能显著提升检索结果的精确性。\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Cell 1: Reranker + LLM (LangChain)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from typing import List, Tuple\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Cross-encoder reranker (same model you wanted)\n",
    "rerank_model = CrossEncoder(\"BAAI/bge-reranker-base\")\n",
    "TOP_N = 3\n",
    "\n",
    "# LLM for answer generation\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "print(\"✅ CrossEncoder reranker + LLM ready\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ rerank_docs() and query_engine() defined\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Cell 2: rerank + query_engine (LangChain equivalent of RetrieverQueryEngine)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def rerank_docs(query: str, docs: List[Document], top_n: int = TOP_N) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Re-score (query, doc) pairs with a cross-encoder and keep top_n docs.\n",
    "    Stores score in doc.metadata[\"rerank_score\"].\n",
    "    \"\"\"\n",
    "    if not docs:\n",
    "        return []\n",
    "\n",
    "    pairs: List[Tuple[str, str]] = [(query, d.page_content or \"\") for d in docs]\n",
    "    scores = rerank_model.predict(pairs)  # higher = more relevant for bge-reranker\n",
    "\n",
    "    scored: List[Tuple[Document, float]] = []\n",
    "    for d, s in zip(docs, scores):\n",
    "        d.metadata = d.metadata or {}\n",
    "        d.metadata[\"rerank_score\"] = float(s)\n",
    "        scored.append((d, float(s)))\n",
    "\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [d for d, _ in scored[:top_n]]\n",
    "\n",
    "\n",
    "def build_context(docs: List[Document], max_chars: int = 6000) -> str:\n",
    "    \"\"\"\n",
    "    Concatenate top docs into a single context string.\n",
    "    \"\"\"\n",
    "    blocks = []\n",
    "    total = 0\n",
    "    for i, d in enumerate(docs, 1):\n",
    "        text = (d.page_content or \"\").strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        block = f\"[Doc {i}]\\n{text}\\n\"\n",
    "        if total + len(block) > max_chars:\n",
    "            break\n",
    "        blocks.append(block)\n",
    "        total += len(block)\n",
    "    return \"\\n\".join(blocks)\n",
    "\n",
    "\n",
    "def query_engine(query: str, retriever, retrieve_top_k: int = 5, rerank_top_n: int = TOP_N) -> str:\n",
    "    \"\"\"\n",
    "    LangChain-style query engine:\n",
    "      1) retrieve documents\n",
    "      2) rerank with CrossEncoder\n",
    "      3) generate answer with ChatOpenAI\n",
    "    \"\"\"\n",
    "    # 1) retrieve\n",
    "    docs = retriever.invoke(query)\n",
    "\n",
    "    # 2) rerank\n",
    "    top_docs = rerank_docs(query, docs, top_n=rerank_top_n)\n",
    "\n",
    "    # 3) generate\n",
    "    context = build_context(top_docs)\n",
    "    prompt = (\n",
    "        \"You are a precise assistant. Answer using ONLY the provided context.\\n\"\n",
    "        \"If the answer is not in the context, say you don't know.\\n\\n\"\n",
    "        f\"Question:\\n{query}\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "    return llm.invoke(prompt).content\n",
    "\n",
    "\n",
    "print(\"✅ rerank_docs() and query_engine() defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "def query_engine_with_sources(\n",
    "    query: str,\n",
    "    retriever,\n",
    "    retrieve_top_k: int = 5,\n",
    "    rerank_top_n: int = TOP_N,\n",
    ") -> Tuple[str, List[Document]]:\n",
    "    # 1) retrieve\n",
    "    docs = retriever.invoke(query)\n",
    "\n",
    "    # 2) rerank\n",
    "    top_docs = rerank_docs(query, docs, top_n=rerank_top_n)\n",
    "\n",
    "    # 3) generate\n",
    "    context = build_context(top_docs)\n",
    "    prompt = (\n",
    "        \"You are a precise assistant. Answer using ONLY the provided context.\\n\"\n",
    "        \"If the answer is not in the context, say you don't know.\\n\\n\"\n",
    "        f\"Question:\\n{query}\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    answer = llm.invoke(prompt).content\n",
    "    return answer, top_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ merged_retriever is ready\n"
     ]
    }
   ],
   "source": [
    "class MergedRetriever:\n",
    "    def __init__(self, vectorstore, parent_map, top_k: int = 5, verbose: bool = True):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.parent_map = parent_map\n",
    "        self.top_k = top_k\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def invoke(self, query: str):\n",
    "        return auto_merge_retrieve(\n",
    "            query=query,\n",
    "            vectorstore=self.vectorstore,\n",
    "            parent_map=self.parent_map,\n",
    "            top_k=self.top_k,\n",
    "            verbose=self.verbose,\n",
    "        )\n",
    "\n",
    "merged_retriever = MergedRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    parent_map=parent_map,\n",
    "    top_k=5,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"✅ merged_retriever is ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[auto_merge] query='What is the exact subsidy amount and what conditions must be met?'\n",
      "[auto_merge] retrieved leaf_hits=5\n",
      "[auto_merge] parent_id=chapter_2 title=Chapter 2: High-Tech Enterprise Certification and Support children=2\n",
      "[auto_merge] parent_id=chapter_3 title=Chapter 3: Supervision, Management, and Violations children=2\n",
      "[auto_merge] parent_id=chapter_1 title=Chapter 1: General Provisions and Objectives children=1\n",
      "The exact subsidy amount is **RMB 300,000** for newly certified national high-tech enterprises, and an additional R&D investment subsidy of up to **RMB 500,000** may be available, determined by R&D intensity and output performance. The conditions that must be met include passing the national high-tech enterprise certification and having R&D expenditure in the previous year, as the R&D investment subsidy is calculated as 10% of that expenditure, capped at RMB 500,000.\n",
      "\n",
      "--- Sources ---\n",
      "parent_id: chapter_2\n",
      "rerank_score: 0.04834342002868652\n",
      "[PARENT CONTEXT]\n",
      "# Chapter 2: High-Tech Enterprise Certification and Support\n",
      "\n",
      "## Article 3: Certification Standards and Procedures\n",
      "\n",
      "Enterprises that meet the requirements of the national high-tech enterprise certification\n",
      "management regulations may apply for certification in accordance with the pres ...\n",
      "\n",
      "parent_id: chapter_3\n",
      "rerank_score: 0.003408474149182439\n",
      "[PARENT CONTEXT]\n",
      "# Chapter 3: Supervision, Management, and Violations\n",
      "\n",
      "## Article 6: Supervision and Inspection\n",
      "\n",
      "The Municipal Science and Technology Bureau, the Finance Bureau, and other relevant departments will\n",
      "conduct regular inspections of enterprises receiving policy support to ensure complian ...\n",
      "\n",
      "parent_id: chapter_1\n",
      "rerank_score: 3.739110616152175e-05\n",
      "[PARENT CONTEXT]\n",
      "# Chapter 1: General Provisions and Objectives\n",
      "\n",
      "## Article 1: Policy Background and Objectives\n",
      "\n",
      "This policy aims to thoroughly implement the national innovation-driven development strategy\n",
      "and accelerate the construction of a globally influential science and technology innovation ce ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the exact subsidy amount and what conditions must be met?\"\n",
    "answer, source_docs = query_engine_with_sources(query, merged_retriever)\n",
    "\n",
    "print(answer)\n",
    "print(\"\\n--- Sources ---\")\n",
    "for d in source_docs:\n",
    "    print(\"parent_id:\", d.metadata.get(\"parent_id\"))\n",
    "    print(\"rerank_score:\", d.metadata.get(\"rerank_score\"))\n",
    "    print(d.page_content[:300], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码解释：\n",
    "- AutoMergingRetriever 是 LlamaIndex 实现 HiRAG 核心功能的组件。它接收一个基础的 retriever（这里是我们基于叶子节点构建的向量索引的检索器），以及一个 storage_context（包含了所有节点的父子关系）。\n",
    "- 当 AutoMergingRetriever 执行检索时，它首先通过底层检索器找到相关的叶子节点。然后，它会检查这些叶子节点的父节点，如果发现多个相关的叶子节点都属于同一个父节点，或者该父节点能提供更完整的上下文，它就会“合并”并返回该父节点的内容，而不是零散的叶子节点。这个过程是自动且智能的。\n",
    "- verbose=True 在开发调试时非常有用，它会打印出 AutoMergingRetriever 的内部操作日志，让你看到它是如何进行节点合并的。\n",
    "- SentenceTransformerRerank：这是一个后处理器，用于对检索到的文档进行重新排序。虽然向量相似度已经筛选出了一批相关文档，但它们可能并非最优。重排器使用一个更强大的语义模型对这些文档进行二次评估，选择出真正与查询最相关的文档，进一步提升召回精度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "效果对比：\n",
    "- 普通检索器：针对“申请高新技术企业补贴的具体金额是多少？”的查询，可能只返回一个非常孤立的句子，如 “...补贴金额为50万元...“。提问者会困惑：这是针对哪个城市？哪一年的政策？有什么前提条件？\n",
    "- HiRAG (AutoMergingRetriever)：会返回一个更大的内容块，如 “第二章 高新技术企业认定与支持 ... 第四条 财政补贴与奖励 ... 对首次通过认定的国家高新技术企业，额外提供最高50万元的研发投入补贴... 4.1 研发投入补贴细则：研发投入补贴按照企业上一年度研发费用的10%计算，最高不超过50万元。本政策适用于在XX市行政区域内依法注册...”。上下文一目了然，完美解决了“漏掉关键点”的问题，提供了更全面的信息，包括金额、计算方式和适用范围。\n",
    "\n",
    "## 总结\n",
    "通过上述实践，我们可以清晰地看到 LlamaIndex 的 AutoMergingRetriever 如何利用层次化节点解析和智能合并策略，有效地将底层细节与上层上下文结合，从而在垂直领域文档检索中实现更高的精度和上下文连贯性，真正解决商业化RAG的业务痛点。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
