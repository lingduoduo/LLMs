{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25c722b6",
   "metadata": {},
   "source": [
    "## 14 多路召回融合难？动态阈值机制确保高质量结果优先排序"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab7a37e",
   "metadata": {},
   "source": [
    "## 一、多路召回与加权融合：从单一到多维的检索优化\n",
    "\n",
    "### 1.1 传统做法的局限性\n",
    "\n",
    "在传统的检索流程中，通常遵循“单一查询 → 向量检索 → 排序 → 生成答案”的模式。这种方法虽然直接，但在面对复杂或多意图查询时，其召回能力显得捉襟见肘，极易遗漏高相关但直接匹配度不高的文档。\n",
    "\n",
    "### 1.2 优化做法的显著优势\n",
    "\n",
    "为了大幅提升检索效果，优化的多路召回策略引入了多维度的检索范式：多个扩展查询 → 多次检索 → 聚合结果 → 重排序 → 生成答案\n",
    "\n",
    "这种方法的核心在于：它不再局限于原始查询本身，而是通过查询扩展（Query Expansion）生成多个相关查询，并并行地在不同召回路径中进行搜索（例如，基于关键词的检索、基于向量相似度的检索、基于知识图谱的检索等）。这样做带来的显著优势包括：\n",
    "\n",
    "- **提高召回覆盖率（Recall）**：通过从多角度、多数据源进行检索，能捕获更多与用户意图相关的文档，从而大大减少漏召的可能性。\n",
    "- **利用 LLM 进行更精细的 Reranking**：将多路召回到的候选文档集合起来，再利用强大的**大型语言模型（LLM）**对这些候选段落进行更精细的相关性打分和排序。LLM凭借其强大的语义理解能力，能识别文档与查询之间更深层次的关联，显著提升最终排序的准确性。\n",
    "\n",
    "---\n",
    "\n",
    "## 二、伪代码示例：多路召回与 LLM 重排序\n",
    "\n",
    "下面是一个简化的伪代码示例，展示了多路召回与 LLM 重排序的基本流程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471545aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = []\n",
    "for q in expanded_queries: # 遍历每个扩展查询\n",
    "    docs = vector_db.search(q, top_k=5) # 从向量数据库中检索前 K 个文档\n",
    "    retrieved_docs.extend(docs) # 将检索到的文档添加到集合中\n",
    "\n",
    "# 使用 LLM 对所有召回的文档进行相关性评分并排序\n",
    "reranked_docs = rerank_with_llm(retrieved_docs, original_query) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3383283",
   "metadata": {},
   "source": [
    "## 二、引入基于置信度的动态融合阈值机制\n",
    "\n",
    "尽管多路召回结合 LLM 重排序显著提升了检索效果，但在实际应用中，如何有效融合来自不同召回路径的结果，并确保高质量结果的优先排序，仍是一个挑战。尤其当不同召回路径返回结果质量不一或存在重复文档时，简单的加权融合可能无法达到最优效果。\n",
    "\n",
    "### 2.1 核心思想\n",
    "\n",
    "为解决这一难题，我们可以引入一种**基于置信度的动态融合阈值机制**。这种机制的核心思想是：\n",
    "\n",
    "1. **评估召回路径的置信度**：  \n",
    "   为每个召回路径的结果赋予一个置信度分数。这个分数可以基于多种因素确定，例如：\n",
    "   - **匹配度得分**：召回模型返回的相似度分数或匹配度。\n",
    "   - **路径的历史稳定性与准确性**：通过离线评估或在线 A/B 测试，统计不同召回路径在历史数据中的表现。\n",
    "   - **召回文档的质量特征**：例如文档的来源权威性、内容的完整性等。\n",
    "\n",
    "2. **动态调整融合权重**：  \n",
    "   根据召回路径的置信度分数，自动调整不同召回路径的权重。置信度高的路径，其返回的结果在融合时将获得更高的权重。这意味着系统会更倾向于采信那些被认为更可靠、更高质量的召回源。\n",
    "\n",
    "3. **设置动态阈值**：  \n",
    "   引入一个动态阈值，只有达到一定置信度或相关性分数门槛的文档才会被纳入最终的重排序列表。这个阈值可以根据整体召回结果分布、查询复杂程度，甚至是用户历史行为动态调整。例如，当召回的整体质量普遍较高时，可适当提高阈值以筛选更优质结果；反之，在召回结果较少或质量不佳时，可适当降低阈值以确保一定的召回数量。\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 引入机制的目标\n",
    "\n",
    "通过引入基于置信度的动态融合阈值机制，我们能实现以下关键目标：\n",
    "\n",
    "- ✅ **确保高质量结果优先排序**：  \n",
    "  高置信度召回路径中的高质量文档将获得更高的融合权重，并在动态阈值的筛选下优先进入 LLM 重排序阶段。这意味着系统能够更有效地将真正相关的、有价值的信息推到用户面前。\n",
    "\n",
    "- ✅ **提升排序稳定性**：  \n",
    "  动态调整权重和阈值，使得系统能更好地适应不同查询类型和数据分布。即使某些召回路径在特定情况下表现不佳，整体融合机制也能通过降低其权重，避免其对最终排序结果产生负面影响，从而提升整个检索系统的稳定性。\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 演示代码：多路召回与 LLM 重排序\n",
    "\n",
    "为了更好地理解上述概念，这里提供一个可运行的 Python 演示代码。该代码使用 [LlamaIndex](https://www.llamaindex.ai/ ) 框架与 OpenAI 大模型进行交互，展示了多路召回、文档去重以及重排序过程。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51308fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linghuang/miniconda3/envs/llm_clean/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "from pydantic import ConfigDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1397ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d11eab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 1) Initialization\n",
    "# -----------------------------------------------------------------------------\n",
    "# Make sure OPENAI_API_KEY is set:\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "# Sample documents (same meaning as your LlamaIndex demo)\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"Artificial Intelligence (AI) is a new technical science that aims to simulate, extend, and augment human intelligence through theories, methods, technologies, and application systems.\",\n",
    "        metadata={\"source\": \"wiki\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Deep learning is a new field within machine learning research, motivated by the construction of neural networks that simulate the human brain for analytical learning.\",\n",
    "        metadata={\"source\": \"wiki\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"LLM stands for Large Language Model. It is an artificial intelligence model trained on large volumes of text data and is widely used in natural language processing tasks.\",\n",
    "        metadata={\"source\": \"tech_blog\"},\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Vector index (FAISS) + embeddings\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Keyword/BM25 retriever\n",
    "bm25 = BM25Retriever.from_documents(docs)\n",
    "bm25.k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7460ddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Any\n",
    "import math\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from pydantic import ConfigDict\n",
    "\n",
    "\n",
    "class MultiPathRetriever(BaseRetriever):\n",
    "    # ✅ Declare fields so Pydantic allows assignment\n",
    "    bm25_retriever: Any\n",
    "    vectorstore: Any\n",
    "\n",
    "    vector_top_k: int = 2\n",
    "    bm25_top_k: int = 2\n",
    "    vector_weight: float = 0.8\n",
    "    bm25_weight: float = 0.6\n",
    "\n",
    "    # ✅ Allow non-Pydantic types (FAISS, BM25Retriever)\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # --- BM25 path ---\n",
    "        try:\n",
    "            self.bm25_retriever.k = self.bm25_top_k\n",
    "        except Exception:\n",
    "            pass\n",
    "        bm25_docs = self.bm25_retriever.invoke(query)\n",
    "\n",
    "        # assign a simple rank-based score (no native BM25 scores exposed)\n",
    "        bm25_scored = [\n",
    "            (d, (1.0 / (i + 1)) * self.bm25_weight, \"bm25\")\n",
    "            for i, d in enumerate(bm25_docs)\n",
    "        ]\n",
    "\n",
    "        # --- Vector path (FAISS) ---\n",
    "        # similarity_search_with_score returns (doc, distance) where lower is better\n",
    "        faiss_with_scores: List[Tuple[Document, float]] = self.vectorstore.similarity_search_with_score(\n",
    "            query, k=self.vector_top_k\n",
    "        )\n",
    "        vec_scored = [\n",
    "            (d, (1.0 / (1.0 + dist)) * self.vector_weight, \"vector\")\n",
    "            for d, dist in faiss_with_scores\n",
    "        ]\n",
    "\n",
    "        # --- Merge + dedup by content ---\n",
    "        best = {}\n",
    "        for d, s, src in (vec_scored + bm25_scored):\n",
    "            key = d.page_content\n",
    "            if key not in best or s > best[key][1]:\n",
    "                best[key] = (d, s, src)\n",
    "\n",
    "        merged = list(best.values())\n",
    "        if not merged:\n",
    "            return []\n",
    "\n",
    "        # --- Dynamic threshold: mean + std (keep at least 1) ---\n",
    "        scores = [s for _, s, _ in merged]\n",
    "        mean = sum(scores) / len(scores)\n",
    "        std = math.sqrt(sum((s - mean) ** 2 for s in scores) / len(scores))\n",
    "        thr = mean + std\n",
    "\n",
    "        merged.sort(key=lambda x: x[1], reverse=True)\n",
    "        kept = [x for x in merged if x[1] >= thr] or merged[:1]\n",
    "\n",
    "        # add scores to metadata\n",
    "        out = []\n",
    "        for d, s, src in kept:\n",
    "            md = dict(d.metadata or {})\n",
    "            md[\"fusion_score\"] = float(s)\n",
    "            md[\"retrieval_path\"] = src\n",
    "            out.append(Document(page_content=d.page_content, metadata=md))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3055b9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "import Stemmer\n",
    "\n",
    "stemmer = Stemmer.Stemmer(\"english\")\n",
    "bm25_retriever = BM25Retriever.from_documents(docs, k=2, stemmer=stemmer)\n",
    "\n",
    "def preview(doc: Document, n: int = 500) -> str:\n",
    "    text = (doc.page_content or \"\").replace(\"\\n\", \" \").strip()\n",
    "    return text[:n] + (\"...\" if len(text) > n else \"\")\n",
    "\n",
    "def print_results(title: str, docs: List[Document], n_preview: int = 500) -> None:\n",
    "    print(f\"\\n{title} (count={len(docs)})\")\n",
    "    for i, d in enumerate(docs, 1):\n",
    "        src = (d.metadata or {}).get(\"source\", \"\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"[{i}] source={src}\")\n",
    "        print(preview(d, n=n_preview))\n",
    "\n",
    "retriever = MultiPathRetriever(\n",
    "    vectorstore=vectorstore,          # NOTE: pass the FAISS *vectorstore*, not as_retriever()\n",
    "    bm25_retriever=bm25_retriever,\n",
    "    vector_top_k=2,\n",
    "    bm25_top_k=2,\n",
    "    vector_weight=0.8,\n",
    "    bm25_weight=0.6,\n",
    ")\n",
    "\n",
    "query = \"What happened at Viaweb and Interleaf?\"\n",
    "hybrid_docs = retriever.invoke(query)   # ✅ LangChain 1.x way\n",
    "print_results(\"Hybrid Results\", hybrid_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3e6d0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 3) LLM reranking (OpenAI via LangChain)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def llm_rerank(query: str, docs: List[Document], llm: ChatOpenAI) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Ask the LLM for a 0-10 relevance score for each doc, then sort descending.\n",
    "    Stores LLM score in doc.metadata[\"llm_rerank_score\"].\n",
    "    \"\"\"\n",
    "    if not docs:\n",
    "        return []\n",
    "\n",
    "    scored_docs: List[Tuple[Document, float]] = []\n",
    "\n",
    "    for d in docs:\n",
    "        prompt = (\n",
    "            \"You are a strict relevance grader.\\n\"\n",
    "            f\"Question: {query}\\n\\n\"\n",
    "            \"Passage:\\n\"\n",
    "            f\"{d.page_content}\\n\\n\"\n",
    "            \"Return ONLY a single number between 0 and 10.\"\n",
    "        )\n",
    "\n",
    "        resp = llm.invoke(prompt).content.strip()\n",
    "\n",
    "        try:\n",
    "            score = float(resp)\n",
    "        except ValueError:\n",
    "            # fallback if model returns something unexpected\n",
    "            score = 0.0\n",
    "\n",
    "        d.metadata[\"llm_rerank_score\"] = score\n",
    "        scored_docs.append((d, score))\n",
    "\n",
    "    scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [d for d, _ in scored_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c0f9d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x13b44fed0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f914c915",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 4) Run demo\n",
    "# -----------------------------------------------------------------------------\n",
    "retriever = MultiPathRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    bm25_retriever=bm25,\n",
    "    vector_top_k=2,\n",
    "    bm25_top_k=2,\n",
    "    vector_weight=0.8,\n",
    "    bm25_weight=0.6,\n",
    ")\n",
    "\n",
    "query = \"What is a large language model?\"\n",
    "candidate_docs = retriever.invoke(query)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)  # pick any OpenAI chat model you have access to\n",
    "reranked_docs = llm_rerank(query, candidate_docs, llm)\n",
    "\n",
    "print(\"--- Final Retrieved and Re-Ranked Documents ---\")\n",
    "if not reranked_docs:\n",
    "    print(\"No relevant documents were found.\")\n",
    "else:\n",
    "    for i, d in enumerate(reranked_docs, start=1):\n",
    "        rs = d.metadata.get(\"retrieval_score\", None)\n",
    "        path = d.metadata.get(\"retrieval_path\", None)\n",
    "        llm_s = d.metadata.get(\"llm_rerank_score\", None)\n",
    "        print(f\"{i}. {d.page_content}\")\n",
    "        print(f\"   retrieval_score={rs:.3f} path={path} | llm_rerank_score={llm_s:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecc7ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123c1504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 最终召回并重排序的文档 ---\n",
      "1. LLM 是大型语言模型的缩写，它是一种经过大量文本数据训练的人工智能模型，广泛用于自然语言处理任务。 (召回分数: 0.69)\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleKeywordTableIndex, Document\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from typing import List\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. 初始化设置\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# 初始化 OpenAI 客户端\n",
    "# 请确保您的 OPENAI_API_KEY 已经设置为环境变量\n",
    "# 例如: os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "client = OpenAI()\n",
    "\n",
    "# 准备用于演示的原始文档\n",
    "# 这里的文档内容很简单，仅用于展示流程\n",
    "documents = [\n",
    "    Document(text=\"人工智能（AI）是一种旨在模拟、延伸和扩展人类智能的理论、方法、技术及应用系统的一门新的技术科学。\", metadata={\"source\": \"wiki\"}),\n",
    "    Document(text=\"深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络。\", metadata={\"source\": \"wiki\"}),\n",
    "    Document(text=\"LLM 是大型语言模型的缩写，它是一种经过大量文本数据训练的人工智能模型，广泛用于自然语言处理任务。\", metadata={\"source\": \"tech_blog\"}),\n",
    "]\n",
    "\n",
    "# 基于文档，分别构建两种不同类型的索引：向量索引和关键词索引\n",
    "# 向量索引擅长理解语义相关性（意思相近）\n",
    "vector_index = VectorStoreIndex.from_documents(documents)\n",
    "# 关键词索引擅长精确匹配文本中的词语\n",
    "keyword_index = SimpleKeywordTableIndex.from_documents(documents)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. 自定义多路召回的Retriever\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class MultiPathRetriever(BaseRetriever):\n",
    "    \"\"\"\n",
    "    自定义一个融合了向量搜索和关键词搜索的混合检索器。\n",
    "    这种方法结合了两种搜索方式的优点，可以提高召回文档的全面性和准确性。\n",
    "    \"\"\"\n",
    "    def __init__(self, vector_retriever, keyword_retriever):\n",
    "        self.vector_retriever = vector_retriever\n",
    "        self.keyword_retriever = keyword_retriever\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query: str) -> List[NodeWithScore]:\n",
    "        # 第一步：并行从两个检索器中获取结果\n",
    "        vector_nodes = self.vector_retriever.retrieve(query)\n",
    "        keyword_nodes = self.keyword_retriever.retrieve(query)\n",
    "\n",
    "        # 第二步：为不同来源的结果分配不同的权重，并合并\n",
    "        # 这里的权重是经验值，可以根据实际效果调整。我们假设向量检索的结果更可靠一些。\n",
    "        for node in vector_nodes:\n",
    "            node.score = (node.score or 1.0) * 0.8  # 为向量检索结果分配0.8的权重\n",
    "        for node in keyword_nodes:\n",
    "            node.score = (node.score or 1.0) * 0.6  # 为关键词检索结果分配0.6的权重\n",
    "\n",
    "        combined_nodes = vector_nodes + keyword_nodes\n",
    "\n",
    "        # 第三步：去重\n",
    "        # 将两个路径返回的节点合并后，可能会有重复的文档，需要基于文本内容进行去重。\n",
    "        seen_texts = set()\n",
    "        unique_nodes = []\n",
    "        for node in combined_nodes:\n",
    "            if node.text not in seen_texts:\n",
    "                seen_texts.add(node.text)\n",
    "                unique_nodes.append(node)\n",
    "        \n",
    "        # 第四步：动态阈值过滤\n",
    "        # 为了只保留最相关的文档，我们计算一个动态阈值（平均分+标准差），过滤掉得分较低的文档。\n",
    "        scores = [n.score for n in unique_nodes if n.score is not None]\n",
    "        if not scores:\n",
    "            return []  # 如果没有任何带分数的节点，则返回空\n",
    "\n",
    "        mean_score = sum(scores) / len(scores)\n",
    "        std_dev = (sum((s - mean_score)**2 for s in scores) / len(scores))**0.5\n",
    "        threshold = mean_score + std_dev\n",
    "\n",
    "        # 只返回得分高于阈值的文档\n",
    "        filtered_nodes = [n for n in unique_nodes if n.score is not None and n.score >= threshold]\n",
    "\n",
    "        return filtered_nodes\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. 执行检索与重排序\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# 实例化我们的多路召回检索器\n",
    "# similarity_top_k=2 表示每个检索器最多返回2个最相关的结果\n",
    "retriever = MultiPathRetriever(\n",
    "    vector_retriever=vector_index.as_retriever(similarity_top_k=2),\n",
    "    keyword_retriever=keyword_index.as_retriever(similarity_top_k=2)\n",
    ")\n",
    "\n",
    "# 定义一个查询问题\n",
    "query = \"什么是大型语言模型？\"\n",
    "# 执行检索，获取经过初步筛选和过滤的文档节点\n",
    "nodes = retriever.retrieve(query)\n",
    "\n",
    "# LLM 重排序 (Reranking)\n",
    "# 这是提升最终结果质量的关键一步。\n",
    "# 我们让一个强大的语言模型（如GPT）来评估每个召回的文档与原始问题的相关性，并给出一个分数。\n",
    "# 然后根据这个分数对文档进行重新排序。\n",
    "reranked_nodes = []\n",
    "if nodes:\n",
    "    reranked_nodes = sorted(nodes, key=lambda x: float(client.completions.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",  # 使用一个适合做指令跟随任务的模型\n",
    "        prompt=f\"请评估以下段落与问题 '{query}' 的相关性，仅需给出一个0到10之间的分数。\\n\\n段落：{x.text}\\n\\n相关性评分：\",\n",
    "        max_tokens=5,  # 只需要输出一个分数，所以限制token数量\n",
    "        temperature=0  # 为了让评分更稳定，使用确定性输出\n",
    "    ).choices[0].text.strip() or \"0\"), reverse=True)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. 输出最终结果\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "print(\"--- 最终召回并重排序的文档 ---\")\n",
    "if not reranked_nodes:\n",
    "    print(\"未能找到相关的文档。\")\n",
    "else:\n",
    "    for i, node in enumerate(reranked_nodes):\n",
    "        # 打印出重排序后的文档内容和它在召回阶段的原始分数\n",
    "        print(f\"{i+1}. {node.text} (召回分数: {node.score:.2f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d46d0b",
   "metadata": {},
   "source": [
    "基于 LLM 的重排序过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e035de71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始查询: '关于LlamaIndex和大型语言模型的信息'\n",
      "扩展查询: ['LlamaIndex 如何工作?', '大型语言模型应用', 'LlamaIndex 和 LLM', 'OpenAI API 使用']\n",
      "\n",
      "--- 正在执行召回, 查询: 'LlamaIndex 如何工作?' ---\n",
      " -> 召回了 1 篇文档。\n",
      "--- 正在执行召回, 查询: '大型语言模型应用' ---\n",
      " -> 召回了 0 篇文档。\n",
      "--- 正在执行召回, 查询: 'LlamaIndex 和 LLM' ---\n",
      " -> 召回了 5 篇文档。\n",
      "--- 正在执行召回, 查询: 'OpenAI API 使用' ---\n",
      " -> 召回了 4 篇文档。\n",
      "\n",
      "多路召回共聚合了 7 篇不重复的文档。\n",
      "\n",
      "--- 开始使用 LLM 对召回的文档进行重排序 ---\n",
      "  - 正在评分: 文档ID 'doc3' ... 分数: 85\n",
      "  - 正在评分: 文档ID 'doc1' ... 分数: 0\n",
      "  - 正在评分: 文档ID 'doc4' ... 分数: 20\n",
      "  - 正在评分: 文档ID 'doc5' ... 分数: 0\n",
      "  - 正在评分: 文档ID 'doc6' ... 分数: 80\n",
      "  - 正在评分: 文档ID 'doc7' ... 分数: 0\n",
      "  - 正在评分: 文档ID 'doc8' ... 分数: 60\n",
      "\n",
      "--- 最终重排序结果 (按LLM评分降序) ---\n",
      "1. [分数: 85] (ID: doc3) 内容: '如何使用LlamaIndex进行文档索引和检索？LlamaIndex是一个用于将LLM与外部数据连接的数据框架。...'\n",
      "2. [分数: 80] (ID: doc6) 内容: '大型语言模型(LLM)是近年来人工智能领域的重要突破，它们能够生成连贯且相关的文本。...'\n",
      "3. [分数: 60] (ID: doc8) 内容: '使用OpenAI API可以访问各种强大的LLM模型，进行文本生成、摘要和问答等任务。...'\n",
      "4. [分数: 20] (ID: doc4) 内容: '自然语言处理(NLP)是人工智能的另一个领域，涉及计算机理解和生成人类语言。...'\n",
      "5. [分数: 0] (ID: doc1) 内容: 'Python是一种广泛使用的编程语言，常用于数据科学和机器学习。...'\n",
      "6. [分数: 0] (ID: doc5) 内容: 'Python的语法非常简洁和易读，适合初学者入门。...'\n",
      "7. [分数: 0] (ID: doc7) 内容: '深度学习是机器学习的一个子集，使用神经网络进行数据建模。...'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from llama_index.core import Document, QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.schema import NodeWithScore # 导入 NodeWithScore\n",
    "from llama_index.llms.openai import OpenAI as LlamaOpenAI # 导入LlamaIndex的OpenAI LLM封装，并重命名以示区分\n",
    "\n",
    "# --- 1. 环境与配置 ---\n",
    "\n",
    "# 确保OPENAI_API_KEY环境变量已设置\n",
    "# 在实际使用中，请将 \"YOUR_OPENAI_API_KEY\" 替换为您的真实密钥\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    raise ValueError(\"请设置 OPENAI_API_KEY 环境变量。\")\n",
    "\n",
    "# --- 2. 模拟召回器 (用于演示) ---\n",
    "\n",
    "class MockRetriever(BaseRetriever):\n",
    "    \"\"\"\n",
    "    一个模拟的检索器，用于演示目的。\n",
    "    在真实场景中，这里应该是来自不同来源的多个真实检索器，\n",
    "    例如：向量数据库检索器、关键词检索器、图数据库检索器等。\n",
    "    \"\"\"\n",
    "    def __init__(self, all_docs: List[Document]):\n",
    "        self._all_docs = all_docs\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"\n",
    "        模拟检索过程。\n",
    "        为了简化，这里仅通过简单的关键词匹配来返回文档。\n",
    "        \"\"\"\n",
    "        query_text = query_bundle.query_str.lower()\n",
    "        retrieved_nodes = []\n",
    "        for doc in self._all_docs:\n",
    "            # 如果查询中的任何一个词出现在文档中，就认为召回成功\n",
    "            if any(keyword in doc.text.lower() for keyword in query_text.split()):\n",
    "                # --- 修复 ---\n",
    "                # LlamaIndex的Retriever标准输出格式是NodeWithScore对象列表。\n",
    "                # 此处将召回的Document包装成NodeWithScore，以符合框架要求。\n",
    "                retrieved_nodes.append(NodeWithScore(node=doc, score=1.0))\n",
    "        # 模拟真实检索器返回 top_k 个结果\n",
    "        return retrieved_nodes[:5]\n",
    "\n",
    "# --- 3. 核心功能：多路召回与LLM重排序 ---\n",
    "\n",
    "def retrieve_and_rerank(\n",
    "    original_query: str,\n",
    "    expanded_queries: List[str],\n",
    "    all_docs: List[Document]\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    执行多路召回，聚合结果，然后使用 LLM 进行重排序。\n",
    "\n",
    "    Args:\n",
    "        original_query (str): 用户的原始查询。\n",
    "        expanded_queries (List[str]): 由原始查询扩展出的一组子查询。\n",
    "        all_docs (List[Document]): 知识库中的所有文档。\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: 经过重排序并带有分数的文档列表。\n",
    "    \"\"\"\n",
    "    print(f\"原始查询: '{original_query}'\")\n",
    "    print(f\"扩展查询: {expanded_queries}\\n\")\n",
    "\n",
    "    # --- 步骤 1: 多路召回与结果聚合 ---\n",
    "    # 初始化模拟检索器\n",
    "    retriever = MockRetriever(all_docs)\n",
    "    \n",
    "    retrieved_docs = {}  # 使用字典去重，键为文档ID，值为文档对象\n",
    "    for q in expanded_queries:\n",
    "        print(f\"--- 正在执行召回, 查询: '{q}' ---\")\n",
    "        # 从模拟检索器获取文档\n",
    "        nodes_from_path = retriever.retrieve(QueryBundle(query_str=q))\n",
    "        print(f\" -> 召回了 {len(nodes_from_path)} 篇文档。\")\n",
    "        for node_with_score in nodes_from_path:\n",
    "            doc = node_with_score.node\n",
    "            retrieved_docs[doc.id_] = doc # 利用字典特性自动去重\n",
    "\n",
    "    unique_docs = list(retrieved_docs.values())\n",
    "    print(f\"\\n多路召回共聚合了 {len(unique_docs)} 篇不重复的文档。\\n\")\n",
    "\n",
    "    if not unique_docs:\n",
    "        return []\n",
    "\n",
    "    # --- 步骤 2: LLM 重排序 ---\n",
    "    print(\"--- 开始使用 LLM 对召回的文档进行重排序 ---\")\n",
    "    \n",
    "    # 初始化 LlamaIndex 的 LLM 封装\n",
    "    llm = LlamaOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "    \n",
    "    reranked_results = []\n",
    "    for doc in unique_docs:\n",
    "        # 为每个文档构建一个用于评分的提示 (Prompt)\n",
    "        prompt = (\n",
    "            f\"评估以下文档内容与用户查询的相关性。请只输出一个0到100之间的整数分数，100表示最相关。\\n\\n\"\n",
    "            f\"用户查询: '{original_query}'\\n\\n\"\n",
    "            f\"文档内容: '{doc.text}'\\n\\n\"\n",
    "            f\"相关性分数:\"\n",
    "        )\n",
    "        try:\n",
    "            # 调用 LLM 获取评分\n",
    "            response = llm.complete(prompt)\n",
    "            score = int(response.text.strip())\n",
    "            print(f\"  - 正在评分: 文档ID '{doc.id_}' ... 分数: {score}\")\n",
    "            reranked_results.append({\"doc\": doc, \"score\": score})\n",
    "        except (ValueError, AttributeError) as e:\n",
    "            print(f\"  - 评分失败: 文档ID '{doc.id_}', 错误: {e}。记为0分。\")\n",
    "            reranked_results.append({\"doc\": doc, \"score\": 0})\n",
    "\n",
    "    # 根据 LLM 给出的分数进行降序排序\n",
    "    reranked_results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "    return reranked_results\n",
    "\n",
    "# --- 4. 演示执行 ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 模拟知识库中的文档\n",
    "    mock_docs = [\n",
    "        Document(id_=\"doc1\", text=\"Python是一种广泛使用的编程语言，常用于数据科学和机器学习。\"),\n",
    "        Document(id_=\"doc2\", text=\"机器学习是人工智能的一个分支，专注于让计算机从数据中学习。\"),\n",
    "        Document(id_=\"doc3\", text=\"如何使用LlamaIndex进行文档索引和检索？LlamaIndex是一个用于将LLM与外部数据连接的数据框架。\"),\n",
    "        Document(id_=\"doc4\", text=\"自然语言处理(NLP)是人工智能的另一个领域，涉及计算机理解和生成人类语言。\"),\n",
    "        Document(id_=\"doc5\", text=\"Python的语法非常简洁和易读，适合初学者入门。\"),\n",
    "        Document(id_=\"doc6\", text=\"大型语言模型(LLM)是近年来人工智能领域的重要突破，它们能够生成连贯且相关的文本。\"),\n",
    "        Document(id_=\"doc7\", text=\"深度学习是机器学习的一个子集，使用神经网络进行数据建模。\"),\n",
    "        Document(id_=\"doc8\", text=\"使用OpenAI API可以访问各种强大的LLM模型，进行文本生成、摘要和问答等任务。\"),\n",
    "    ]\n",
    "\n",
    "    # 原始查询 和 手动扩展的子查询\n",
    "    original_query = \"关于LlamaIndex和大型语言模型的信息\"\n",
    "    expanded_queries = [\n",
    "        \"LlamaIndex 如何工作?\",\n",
    "        \"大型语言模型应用\",\n",
    "        \"LlamaIndex 和 LLM\",\n",
    "        \"OpenAI API 使用\",\n",
    "    ]\n",
    "\n",
    "    # 执行召回和重排序流程\n",
    "    final_results = retrieve_and_rerank(original_query, expanded_queries, mock_docs)\n",
    "\n",
    "    # 打印最终排序结果\n",
    "    print(\"\\n--- 最终重排序结果 (按LLM评分降序) ---\")\n",
    "    if not final_results:\n",
    "        print(\"未能找到任何相关文档。\")\n",
    "    else:\n",
    "        for i, item in enumerate(final_results):\n",
    "            doc = item['doc']\n",
    "            score = item['score']\n",
    "            print(f\"{i+1}. [分数: {score}] (ID: {doc.id_}) 内容: '{doc.text[:100]}...'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d73d187",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "多路召回结合 LLM 重排序是提升信息检索系统性能的强大策略。\n",
    "\n",
    "而基于置信度的动态融合阈值机制则能够有效解决多路召回结果融合的难题，更能确保高质量结果的优先排序，并显著提升整个系统的稳定性和鲁棒性。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
